=~=
Philos. Technol. @® CrossMark
https://dot.org/10.1007/s13347-018-0303-9

EDITOR LETTER

 

Soft Ethics and the Governance of the Digital

Luciano Floridi!”

© Springer Science+Business Media B.V., part of Springer Nature 2018

Today, in any mature information society (Floridi 2016), we no longer live online or
offline but on/ife, that is, we increasingly live in that special space, or infosphere, that is
seamlessly analogue and digital, offline and online. If this seems confusing, perhaps an
analogy may help to convey the point. Imagine someone asks whether the water is
sweet or salty in the estuary where the river meets the sea. Clearly, that someone has not
understood the special nature of the place. Our mature information societies are
growing in such a new, liminal place, like mangroves flourishing in brackish water.
And in these ‘mangrove societies’, machine-readable data, new forms of smart agency
and onlife interactions are constantly evolving, because our technologies are perfectly
fit to take advantage of such a new environment, often as the only real natives. As a
result, the pace of their evolution can be mind-blowing. And this in turn justifies some
apprehension. However, we should not be distracted by the scope, depth and pace of
technological innovation. True, it does disrupt some deeply ingrained assumptions of
the old, exclusively analogue society, e.g. about production, logistics, customization,
competition, education, work, health, entertainment, politics and security, Just to
mention some crucial topics. Yet that is not the most consequential challenge we are
facing. It is rather how we are going to design the infosphere and the mature informa-
tion societies developing within it that matters most. Because the digital revolution
transforms our views about values and their priorities, good behaviour, and what sort of
innovation is socially preferable—and this is the fundamental issue, let me explain.
To many, what digital innovation will throw up next may seem the real challenge.
The question itself is recurrent: what comes next? What is the next disruption? What is
the new killer app? Will this be the year of the final battle between Virtual Reality vs.
Augmented Reality? Or is it the Internet of Things that will represent the new frontier,
perhaps in some combination with Smart Cities? Is it the end of the TV as we know it
coming soon? Will healthcare be made unrecognisable by machine learning, or should

 

DX] Luciano Floridi
luciano floridi @ oii.ox.ac.uk

1 Oxford Intemet Institute, University of Oxford, 1 St Giles, Oxford OX1 3JS, UK
2 The Alan Turing Institute, 96 Euston Road, London NW1 2DB, UK

Published online: 17 February 2018 2 Springer
L. Floridi

 

our attention rather be focused on the automation of logistics and transport? What will
the new smart assistants in the home do, apart from telling us what the weather is like,
and allowing us to choose the next song? Behind similar questions lies the unspoken
assumption that technological innovation leads, and everything else follows, or lags
behind: business models, working conditions, standards of living, legislation, social
norms, habits, and expectations. Yet this is precisely the distracting narrative we should
resist. Not because it is wrong, but because it is only superficially right. The deeper
truth is that the revolution has already occurred: the transition from an entirely analogue
and offline world to one that is increasingly also digital and online will never happen
again in the history of humanity. One day, a quantum computing phone running
artificial intelligence apps may be in the pocket of your average teenager, but our
generation is the last one that will have seen a non-digital world. And this is the really
extraordinary turning point, because that landing in the infosphere happens only once.
What this new world will be like, as we create it, is both fascinating, in terms of
opportunities, and worrisome, in terms of risks. But the ‘exploration’ of the infosphere,
to indulge in the geographical metaphor a bit longer, no matter how challenging,
prompts a much more fundamental question, which is socio-political and truly crucial:
what kind of mature information societies do we want to build? What is our human
project for the digital age? Looking at our present backwards—that is, from the future
to our present—this is the time in history when we shall be seen to have laid down the
foundation for our mature information societies. We shall be judged by the quality of
our work. So, clearly, the real challenge in no longer digital innovation, but the
governance of the digital.

The proof that this is the case is all around us, in the many initiatives addressing the
impact of the digital on everyday life and how to regulate it. It is also implicit in the
current narrative on the unstoppable and unreachable nature of technological innova-
tion, if one looks just a bit more closely. In the same context where people complain
about the speed of innovation and the impossible task of chasing it with some
normative framework, one finds that there is equal certainty about the serious risk that
the wrong legislation may kill innovation entirely or destroy whole technological
sectors and developments. You do not have to be Nietzsche (“Was mich nicht umbringt
macht mich starker’—‘what does not kill me makes me stronger’ (Nietzsche 2008)) to
realise that the inference to be drawn is that updating the rules of the game is perfectly
possible—it can have immense consequences—but that reacting to technological
innovation is not the best approach. We need to shift from chasing to leading. If then
we like the direction in which we move, or where we want to go, then the speed at
which we are moving or getting there can actually be something very positive. The
more we like where we are going, the feaster we will want to get there. But for this to
happen, society needs to stop playing defence and start playing attack. The question is
not whether, but how. And to start addressing the how, some clarifications are helpful.

On the governance of the digital, there is much to be said, and even more still to be
understood and theorised, but one point is clear: the governance of the digital (hence-
forth digital governance), the ethics of the digital (henceforth digital ethics, also known
as computer, information or data ethics (Floridi and Taddeo 2016)) and the regulation
of the digital (henceforth digital regulation) are different normative approaches, com-
plementary, but not to be confused with each other, in the following sense (see Fig. 1
for a visual representation).

a Springer
Soft Ethics and the Governance of the Digital

 

   
 

  
 

   

DE shapes DG and DR
through moral evaluation

DR shapes DG
through legal compliance

 

Digital Governance

Fig. 1 The relationship between digital ethics, digital regulation and digital governance

Digital governance is the practice of establishing and implementing policies, proce-
dures, and standards for the proper development, use and management of the
infosphere. It is also a matter of convention and good coordination, sometimes neither
moral nor immoral, neither legal nor illegal. For example, through digital governance, a
government agency or a company may (a) determine and control processes and
methods used by data stewards and data custodians in order to improve the data quality,
reliability, access, security and availability of its services and (b) devise effective
procedures for decision-making and for the identification of accountabilities with
respect to data-related processes. A typical application of digital governance was the
work I co-chaired for the British Cabinet Office in 2016 on a ‘Data Science Ethical
Framework’ (Cabinet Office 2016), which was ‘[...] intended to give civil servants
guidance on conducting data science projects, and the confidence to innovate with
data.”!

Digital governance may comprise guidelines and recommendations that overlap
with, but are not identical to, digital regulation. This is just another way of speaking
about the relevant legislation, a system of rules elaborated and enforced through social
or governmental institutions to regulate the behaviour of the relevant agents in the
infosphere. Not every aspect of digital regulation is a matter of digital governance and
not every aspect of digital governance is a matter of digital regulation. In this case, a
good example is provided by the General Data Protection Regulation (GDPR).”
Compliance is the crucial relation through which digital regulation shapes digital
governance.

All this holds true of digital ethics, understood as the branch of ethics that studies
and evaluates moral problems relating to data and information (including generation,
recording, curation, processing, dissemination, sharing and use), algorithms (including
AI, artificial agents, machine learning and robots) and corresponding practices and
infrastructures (including responsible innovation, programming, hacking, professional
codes and standards), in order to formulate and support morally good solutions (e.g.

' Available from https://www.gov.uk/government/publications/data-science-ethical-framework
2 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection
of natural persons with regard to the processing of personal data and on the free movement of such data, and
repealing Directive 95/46/EC (General Data Protection Regulation), OJEU L119, 04/05/2016.

2 Springer
L. Floridi

 

good conduct or good values). Digital ethics shapes digital regulation and digital
governance through the relation of moral evaluation.

Once the map is understood, several consequences become clear.

First, there is the synecdoche use. Digital governance in Fig. | is just one of the three
normative forces that can shape and guide the development of the digital. But it is not
uncommon to use that part for the whole (a bit like using ‘coke’ for any variety of cola)
and to speak of digital governance as referring to the whole set. It is what I did at the
beginning of this article, when I stated that the real challenge today is the governance of
the digital. By that I meant to refer not just to digital governance but also to digital
ethics and digital regulation, i.e. to the whole normative map. And this is also how I
interpret the report ‘Data Management and Use: Governance in the 21st Century’ that
we published in 2017 as a joint British Academy and Royal Society working group
(British Academy 2017). As long as the synecdoche is clear, there is no problem.

Second, when policy-makers, both in political and in business contexts, wonder why
we should engage in moral evaluation when legal compliance is already available (this
is a recurring topic in the discussion of the GDPR), the answer should be clear:
compliance is necessary but insufficient to steer society in the right direction. Because
digital regulation indicates what the legal and illegal moves in the game are, so to
speak, but it says nothing about what the good and best moves could be to win the
game—that is, to have a better society. This is the task of both digital ethics, on the side
of moral values and preferences, and of digital governance, on the side of best
management. This is why, for example, the European Data Protection Supervisor (the
EU’s independent data protection authority) established the Ethics Advisory Group in
2015, in order to analyse the new ethical challenges posed by digital developments and
current legislation, especially in relation to the GDPR. The report we published (EDPS
Ethics Advisory Group 2018) should be read as a contribution to, and a stepping stone
towards, a normative governance of the infosphere in the EU.

Third, digital ethics may be understood now in two ways, as hard and soft ethics.
Hard ethics is what we usually have in mind when discussing values, rights, duties and
responsibilities—or, more broadly, what is morally right or wrong, and what ought or
ought not to be done—in the course of formulating new regulations or challenging
existing ones. In short, hard ethics is what makes or shapes the law. Thus, in the best
scenario, lobbying in favour of some good legislation or to improve that which already
exists can be a case of hard ethics. For example, hard ethics helped to dismantle
apartheid in South Africa and supported the approval of legislation in Iceland that
requires public and private businesses to prove that they offer equal pay to employees,
irrespective of their gender (the gender pay gap continues to be a scandal in most
countries).

Soft ethics covers the same normative ground as hard ethics, but it does so by
considering what ought and ought not to be done over and above the existing
regulation, not against it, or despite its scope, or to change it, or to by-pass it (e.g. in
terms of self-regulation). In other words, soft ethics is post-compliance ethics: in this
case, ‘ought implies may’. Now, both hard and soft ethics usually presuppose feasibility
or, in more Kantian terms, assume that ‘ought implies can’, given that an agent has a
moral obligation to perform an action only if this is possible in the first place. It follows
that soft ethics also assumes a post-feasibility approach. Add that any ethical approach,
at least in the EU, accepts, as its minimal starting point, the implementation of the

a Springer
Soft Ethics and the Governance of the Digital

 

Universal Declaration of Human Rights (UDHR) and The Charter of Fundamental
Rights of the European Union (within the UDHR, there are rights that may assume
greater relevance than others, such as article 3 on security, article 7 on non-discrimi-
nation, article 12 on privacy, article 19 on freedom of opinion and expression and
article 20 on freedom of association). And the result is that the space of soft ethics is
both partially bounded, and yet unlimited. To see why, it is easy to visualise it in the
shape of a trapezoid (see Fig. 2), with the lower side representing a feasibility base that
is ever-expanding through time—we can do more and more things—the two
constraining sides representing legal compliance and human rights, and the open upper
side representing the space where what is morally good may happen in general and, in
the context of this article, may happen in terms of shaping and guiding the development
of our mature information societies.

Soft digital ethics can be rightly exercised in places of the world where digital
regulation is already on the good side of the moral vs. immoral divide. But it would be a
mistake to argue for a soft ethics approach to establish a normative framework when
agents (especially governments and companies) are operating in contexts where human
rights are disregarded, e.g. in China, North Korea or Russia, or in contexts where hard
ethics is precisely what is needed to change the current regulation, e.g. in the USA and,
a fortiori, in the three countries already mentioned. It is really within the European
Union (EU) that soft ethics can rightly be exercised, to help companies, governments
and other organisations to take more and better advantage, morally speaking, of the
opportunities offered by digital innovation, because even in the EU, legislation is
necessary but insufficient. It does not cover everything (nor should it), and agents
should leverage digital ethics in order to assess and decide what role they wish to play
in the infosphere, when regulations provide no simple or straightforward answer, when
competing values and interests need to be balanced (or indeed when regulations
provide no guidance) and when there is more that can be done over and above what
the law strictly requires. This is why it is in the EU that a good use of soft ethics could
lead to ‘good corporate citizenship’ within a mature information society.

Fourth, given the open future addressed by digital ethics, it is obvious that ethics
foresight analysis—based on data analytics applied strategically to the ethical impact
assessment of digital technologies, goods, services and practices (see Fig. 3}—must
become a priority (Floridi 2014). For the task of digital ethics is not simply to ‘look into
the [digital] seeds of time / And say which grain will grow and which will not’
(Macbeth, 13, 159-162), it also to determine which ones should grow and which
should not.

Human Rights Compliance

  
   
 

  

The space

IME of soft ethics

Feasibility

Fig. 2. The space of soft ethics

2 Springer
L. Floridi

 

 

Fig. 3 Ethics foresight analysis cycle

Or to use a metaphor already introduced above, the best way to catch the technology
train is not to chase it, but to be at the next station. In other words, we need to anticipate
and steer the ethical development of technological innovation. And we can do this by
looking at what is actually feasible, privileging, within this, what is environmentally
sustainable, then what is socially acceptable and then, ideally, choose what is socially
preferable (see Fig. 4).

We do not yet have an infosphere equivalent for the concept of sustainability of the
biosphere, so our current equation is incomplete (see Fig. 5).

In Fig. 4, I suggested that we interpret the x as social ‘preferability’ but I am aware it
is just a place holder for a better idea to come. This may take a while, given that ‘the
tragedy of the commons’ was published in 1968 but the expression ‘sustainable
development’ was only coined by the Brundtland Report almost 20 years later, in
1987 (Brundtland 1987). Yet the lack of conceptual terminology does not make the
governance of the digital a mere utopian effort. In particular, digital ethics, with its
values, principles, choices, recommendations and constrains already influences the
world of technology much more than any other force. This is because the evaluation
of what is morally good, right or necessary shapes public opinion—hence the socially

STi la 10)

Acceptable

Preferable

 

Fig. 4 Digital ethics impact assessment

a Springer
Soft Ethics and the Governance of the Digital

 

 

biosphere : sustainability = infosphere : x

 

 

 

Fig. 5 A difficult equation to balance

acceptable or preferable and the politically feasible, and so, ultimately, the legally
enforceable, and what agents may or may not do. In the long run, people (as users,
consumers, citizens, patients, etc.) are constrained in what they can or cannot do by
organisations, e.g. businesses, which are constrained by law, but the latter is shaped and
constrained by ethics, which is where people decide in what kind of society they want
to live (see Fig. 6). Unfortunately, such a normative cascade becomes obvious mainly
when backlash happens, 1.e., mostly in negative contexts, when the public rejects some
solutions, even when they may be good solutions. A normative cascade should instead
be used constructively, to pursue the design of a mature information society of which
we can be proud.

From all this, it follows that ethics in general and digital ethics in particular cannot
be a mere add-on, an afterthought, a late-comer, an owl of Minerva that takes its flight
only when the shades of night are gathering, only once digital innovation has taken
place, and possibly bad solutions have been implemented, less good alternatives have
been chosen, or mistakes have been made. Nor can it be a mere exercise of questioning.
The building of critical awareness is important but it is also only one of the four tasks of
a proper ethical approach to the design and governance of the digital. The other three
are signalling that ethical problems matter, engaging with stakeholders affected by such
ethical problems, and, above all, providing sharable solutions. Any ethical exercise that
in the end fails to provide some acceptable recommendations is only a preamble. So
digital ethics must inform strategies for the development and use of digital technologies
from the very beginning, when changing the course of action is easier and less costly, in
terms of resources and impact. It must sit at the table of policy-making and decision-
taking procedures from day one. For we must not only think twice but, most impor-
tantly, we must think before taking important steps. This is particularly relevant in the
EU, where [ have argued that soft ethics can be properly exercised and where SETI
(science, engineering, technology and innovation) developments are crucial. If soft
digital ethics can be a priority anywhere, this is certainly in Europe. We should adopt it
as soon as possible.

 

Fig. 6 Example of a normative cascade, with business as agent and people as customers. Business could be
replaced by government and people by citizens

2 Springer
L. Floridi

 

References

British Academy, The Royal Society. (2017). Data management and use: governance in the 21st century—a
joint report by the British Academy and the Royal Society.

Brundtland, G. H. (1987). The Brundtland Report, World Commission on Environment and Development.
Oxford: Oxford University Press.

Cabinet Office, Government Digital Service. (2016). Data Science Ethical Framework. Available online at
https://www.gov.uk/government/publications/data-science-ethical-framework.

EDPS Ethics Advisory Group. (2018). Towards a digital ethics. Available online at https://edps.europa.
ewites/edp/files/publication/18-01-25_eag_report_enpdf.

Floridi, L. (2014). Technoscience and ethics foresight. Philosophy & Technology, 27(4), 499-501.

Floridi, L. (2016). Mature information societies—a matter of expectations. Philosophy & Technology, 29(1),
14.

Floridi, L., & Taddeo, M. (2016). What is data ethics? Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences, 374(2083).

Nietzsche, F. W. (2008). Twilight of the idols, or, how to philosophize with a hammer. Oxford: Oxford
University Press.

a Springer
International Journal of Communication 11(2017), 824-842 1932-8036/20170005

Surveillance Culture:
Engagement, Exposure, and Ethics in Digital Modernity

DAVID LYON
Queen’s University, Canada

This article argues that to make sense of surveillance today, the concept of surveillance
culture should be added to the conceptual tool kit. This goes beyond the important
concerns of the surveillance state and surveillance society to examine how today’s
subjects make sense of, respond to, and—in some cases—initiate surveillance activities.
Building conceptually on Charles Taylor’s work, the concepts of surveillance imaginaries
and surveillance practices are proposed as a means of analysis of how surveillance is
engaged today. Previous studies have hinted at surveillance culture both explicitly and
implicitly, but more is needed. This article explores further one illustrative dimension—
that of online practices of sharing. These practices are seen, in turn, in relation to
visibility and exposure. Finally, the concept of surveillance culture is shown to be
relevant to current discussions ethics and of digital citizenship.

Keywords: surveillance, surveillance culture, online information, digital citizenship

An unprecedented surveillance culture is emerging. Its key feature is that people actively
Participate in an attempt to regulate their own surveillance and the surveillance of others. There is
growing evidence of patterns of perspectives, outlooks, or menta/lités on surveillance, along with some
closely related modes of initiating, negotiating, or resisting surveillance. These I call surveillance
imaginaries and surveillance practices, respectively. They are analytically distinguishable, but not
separable. They shade into each other. This article discusses the reasons for focusing on the growth of
surveillance culture as engagement; some of its key features, including, specifically, exposure; and how
the surveillance culture concept enlarges previous debates about the surveillance state and surveillance
society, and it facilitates the discussion of ethics and citizenship.

The term surveillance culture has appeared before, but it has yet to be treated as a broad
phenomenon in its own right and theorized as a development distinct from others, such as surveillance
state and surveillance society. William Staples (1998), for instance, used surveillance culture in a book
title, exploring what he appropriately terms “postmodern” developments in our everyday interactions with
surveillance. Surveillance culture also appears in John McGrath's subtitle to Loving Big Brother (2004), a
study that helpfully indicates and discusses some of the performative dimensions of surveillance. Or think
of Jonathan Finn’‘sinsights, with regard to camera surveillance in particular, about how, with the
proliferation of public space cameras, surveillance has become a “way of seeing, a way of being” (2012, p.
78). Each provides a good springboard into surveillance culture.

 

David Lyon: lyond@queensu.ca
Date submitted: 2016-03-29

Copyright © 2017 (David Lyon). Licensed under the Creative Commons Attribution Non-commercial No
Derivatives (by-nc-nd). Available at http://ijoc.org.
International Journal of Communication 11(2017) Surveillance Culture 825

Take a current example of how surveillance culture relates to some pressing issues concerning
surveillance in general. The kind of “suspicionless surveillance” carried out by intelligence agencies, such
as the U.S. National Security Agency (NSA), to which Edward Snowden’‘s disclosure of documents drew
attention, cannot be understood simply in terms of older concepts such as surveillance state or
surveillance society. They now have to be supplemented with a concept that focuses more on the active
roles played by surveillance subjects, not least because those roles make a difference to the surveillance
outcomes. I propose that surveillance culture is just such a concept and that focusing on what occurs
within various aspects of surveillance culture helps to explain why the responses to Snowden—and to
surveillance in general—have been so diverse: from outrage and political mobilization to buoyant
reassurance or even complacency.

The culture of surveillance was becoming visible at the turn of the 21st century, especially after
the 9/11 attacks on America and the advent of social media, and became even clearer after Snowden
copied and released documents from the NSA in 2013. Historians may discern the first signs of
surveillance culture in the later 20th century, but now it is present on a broad scale and its contours are
becoming clear. What is meant by this term? It is the sense—as Raymond Williams (1958) might have
said— that surveillance is becoming part of a whole way of life. Hence, my use of the word cu/ture. It is no
longer merely something external that impinges on our lives. It is something that everyday citizens
comply with—willingly and wittingly, or not—negotiate, resist, engage with, and, in novel ways, even
initiate and desire. From being an institutional aspect of modernity or a technologically enhanced mode of
social discipline or control, it is now internalized and forms part of everyday reflections on how things are
and of the repertoire of everyday practices.

The document disclosures of Snowden certainly brought some important debates into the
foreground—questions of digital rights in relation to both corporations and government departments and
agencies, and of who is responsible for flows of data across borders, flows that have clear consequences
for life chances and freedoms (Kuner, 2014; Mosco, 2014). The disclosures also served to revitalize
controversies over the role of online political activity that had surfaced widely a few years ago after the
so-called Arab Spring. To what extent were the new media the means of fomenting popular and radical
change, and to what extent were they the tools of repression and the denial of democratic aspirations?

These matters cannot properly be considered without first thinking more broadly about
surveillance culture. That culture, in turn, must be seen in relation to the astonishing growth of what may
fairly be called digital modernity in the 20th, but especially in the 21st century. Exploring the origins,
carriers, and consequences of surveillance culture is a way of contextualizing more effectively the post-
Snowden world. In what follows, I show that the presence of a surveillance culture raises fresh questions
for everyday involvement with digital media, questions with ethical and political aspects that point to
possibilities and challenges for digital citizenship. Both surveillance and citizenship are now mediated by
the digital. What is the setting for this?
826 David Lyon International Journal of Communication 11(2017)

Surveillance Culture: The Context

Surveillance culture is a product of contemporary late-modern conditions or, simply, of digital
modernity. From the later 20th century especially, corporate and state modes of surveillance, mediated by
increasingly fast and powerful new technologies, tilted toward the incorporation of everyday life through
information infrastructures and our increasing dependence on the digital in mundane relationships. Just as
all cultural shifts relate in significant ways to social, economic, and political conditions, today’s surveillance
culture is formed through organizational dependence, political-economic power, security linkages, and
social media engagement.

Let me contrast surveillance culture with previous terms and show why, on their own, they are
now inadequate. Surveillance state worked well in the postwar Orwellian period and can still capture
significant aspects of surveillance, such as the activities of intelligence agencies. But even there, the
surveillance state is heavily dependent on commercial entities—Internet and telephone companies—to
provide the desired data (Ball et al., 2015). Although such data have been used, via warrants, by police
and security agencies for decades, the mass scale on which this now happens alters the dynamic. Today,
no one is unaffected by this very post-Orwellian collusion of governmental and corporate forces. A second
factor is that many of those data are themselves generated in the first place by the everyday online
activities of millions of ordinary citizens. We collude as never before in our own surveillance by sharing—
whether willingly or wittingly, or not—our personal information in the online public domain. Surveillance
culture helps situate this. If this is state surveillance, it has a deeply different character from that which in
popular terms is “Orwellian.”

If the surveillance state is an inadequate concept, the equally commonplace idea of a surveillance
society is also insufficient for the task. Although surveillance society helps to indicate the broader context
within which the unsettling discoveries about the mass surveillance engaged by the NSA and the “Five
Eyes” occur, it also falls short of explaining today’s situation. Surveillance society is a concept originally
used to indicate ways in which surveillance was spilling over the rims of its previous containers—
government departments, policing agencies, workplaces—to affect many aspects of daily life. But the
emphasis was still on how surveillance was carried out by certain agencies in ways that increasingly
touched the routines of social life—from outside, as it were. This concept was often used in ways that paid
scant attention to citizens’, consumers’, travelers’, or employees’ experience of and engagement with
surveillance.

From the later 20th century onward, surveillance became a central organizing feature of societies
that had developed information infrastructures, in which complexity was managed using categories
(Bennett, Haggerty, Lyon, & Steeves, 2014; Ericson & Haggerty, 2000; Lyon, 2007). By the early 21st
century, evidence was emerging of a “third phase” of computing, after the mainframe and personal
computer phases, where computing machinery is embedded, more-or-less invisibly, in the environments
of everyday life. Many refer to this as evidence for the “Internet of things,” where the focus is on “smart”
devices and objects capable of communicating with users and other devices. As we shall see, this extends
in specific ways the reliance on surveillance as a mode of organization. Today’s surveillance culture is
informed by these developments.
International Journal of Communication 11(2017) Surveillance Culture 827

Second, it is almost a truism to say that surveillance is also a major industry. Global corporations
are involved, often with close links to government. The Snowden disclosures made this abundantly clear if
there was any doubt previously. From the very start, in June 2013, the Snowden documents showed that
the NSA has access to telephone company (Verizon) metadata and also mines the customer databases of
Internet corporations such as Apple, Google, Microsoft, Amazon, and Facebook (often referred to as the
“Big Five”). On the one hand, then, these corporations engage in large-scale surveillance of their
customers. And on the other, they share these data with government agencies.

Moreover, the character of the corporation is also important for the relation between the political
economy and the culture of surveillance. The Big Five corporations now dominate not only the Internet but
also the economic mode of operation, which has moved beyond the managerial and financial modes of
accumulation that characterized the later 20th and early 21st century. As understood by Shoshana Zuboff,
the emerging phase is surveillance capitalism, now intimately involved with big data practices (Zuboff,
2015, 2016, Lyon 2014a). Its aim is to “predict and modify human behaviour to produce revenue and
market control” (Zuboff, 2015, p. 75). Her analysis is based on Google's strategies that evidence a “formal
indifference” toward its user base. For her, this has implications for what she calls “information
civilization.” These remarks are limited to the related idea of surveillance culture that seeks to grasp how
users’ responses to such attempted prediction and modification affects their success.

This means, in turn, that links with securitization are strong and pervasive. As David Garland
observed, surveying the late 20th-century world of policing and governance, neoliberal governance flows
naturally from this; what he called the “culture of control” is its leading motif (Garland, 2001). Risk
management and security was already an important surveillance motif. But a formative opportunity for its
expansion was offered by 9/11 and its aftermath, which, significantly, relied heavily on recently ailing
technology companies to create a new industry of “homeland security” (Lyon, 2003). Thus, one of the key
trends of recent surveillance has been securitization, applied in numerous and ever-growing and
intensifying areas (Bennett et al., 2014). Such securitization demands greater amounts of information
about risk and how to handle it, which both weakens traditional privacy requirements and increases
surveillance of what are deemed risky behaviors. In terms of surveillance culture, this reinforces the sense
that surveillance is warranted, “for our own good.” In practice, of course, this is also understood
ambivalently.

This sense of risk, and the need to take steps to reduce it, is not only evident on the grand scale
of (inter)national policy but also penetrates daily life at home, where self-tracking for health, income, and
time management is an increasing phenomenon. Only a few years ago, The New York Times still thought
of this as something for geeks and keep-fit addicts (Wolf, 2010). Today, such self-monitoring is less
unusual and often taken for granted. Wearable devices have become increasingly popular since the first
decade of this century, and now, talk of the “quantified self” is much more commonplace (Crawford,
Lingel, & Karppi, 2015). In this world, people seek a form of “self-knowledge” so that they can lead
“better lives,” even though only a small fragment of the data is seen by them, the vast majority of the
data ending up in the databases of the wearable device corporations.

Finally, and perhaps best known, is the relation between social media and the surveillance
828 David Lyon International Journal of Communication 11(2017)

culture. I regard José van Dijck’s work (2013, 2014) as exemplary here. Her book examines social media
cultures, and the related article expands the argument to include questions of surveillance and privacy. It
would seem that among the Snowden revelations, the realization by broad swaths of the public that what
happens on social media is open to both corporation and government was one of the most striking. Van
Dijck points out how this connects with “dataism,” the secular belief—part of the surveillance imaginary, in
my terms—that users can safely entrust their data to large corporations. Snowden put serious dents into
dataism. Indeed, in the U.S. a recent study of Americans’ main fears shows that being tracked by
corporations or government is close to the top of the list (Bader, 2016). It would hardly be surprising if
such findings do not have an impact in everyday uses of social media.

According to Pew Internet and American Life researchers, Snowden’s disclosures have indeed had
an impact on social media use (Rainie & Madden, 2015). For example, 34% (or 30% of all adults) of those
aware of the government surveillance programs have taken at least one step to hide or shield their
information from the government—changing privacy settings, using other communication media than
social media, or avoiding certain applications. A slightly smaller proportion (25%) has changed their use of
phones, e-mail, or search engines following Snowden. Knowing more about government surveillance
produces more evidence of changed behavior.

Let me say one more thing about the contexts of the surveillance culture. Having noted its
relation to organizational dependence, political-economic power, security linkages, and social media
engagement, I should also observe that surveillance culture has many facets and varies according to
region. The point of using the concept of surveillance culture is to distinguish it from notions such as
surveillance state or surveillance society by focusing on participation and engagement of surveilled and
surveilling subjects. But surveillance culture will, like any culture, develop differently and often morph
unpredictably, especially in contexts of increasing social liquidity (Bauman & Lyon, 2013). It will,
moreover, bud and blossom differently depending on historical and political circumstance. Most of what is
said here refers primarily to North America and Western Europe, although readers in Asia, Latin America,
Africa, or the Middle East will recognize many features of surveillance culture, necessarily inflected by local
circumstances. With that said, let us consider the main features of surveillance culture and ask how these
may be best analyzed.

Engagement: Imaginaries and Practices

Surveillance culture exhibits forms that are varied and constantly mutating, but they have some
common features that we begin to explore. I refer to those common features in the singular as
surveillance culture, which, despite the singular-sounding concept, is nonetheless multifaceted and
complex. As an increasing proportion of our social relationships is digitally mediated, subjects are
involved, not merely as the targets or bearers of surveillance, but as more-and-more knowledgeable and
active participants. This occurs most obviously through social media and Internet use in general and has
arguably intensified an everyday adoption of varied surveillance mentalities and practices.

There are two main aspects of this. One has to do with widespread compliance with surveillance.
Although attempts to resist surveillance in certain settings are relatively commonplace, in most settings
International Journal of Communication 11(2017) Surveillance Culture 829

and for most of the time, surveillance has become so pervasive that the majority comply without
questioning it (Zureik, Stalker, & Smith, 2010). This general collusion with contemporary surveillance is
something that puzzles those who have lived through the surveillance regimes of authoritarian
governments (e.g., Bauman & Lyon, 2013). But as Lyon (2014) argues, such compliance may be
explained by reference to three rather commonplace factors—familiarity, fear, and fun.

On the first, familiarity, surveillance has become a taken-for-granted aspect of life, from loyalty
cards in the supermarket, to ubiquitous public and private space cameras, and to security routines in
airports, sports arenas, and many other sites. This normalization and domestication of surveillance
appears to account, in part, for the general level of compliance (Murakami Wood & Webster, 2009). As for
fear, this has become more marked since 9/11, and it is apparent that the reported desire for surveillance
measures relates to the ratcheting-up of uncertainty in a media-amplified exploitation of fear (Lyon,
2003). And at the opposite end of the emotional spectrum, fun also accounts for compliance, above all in
the realm of social media and digital devices. Although they are integrated into “serious” life in many self-
evident ways, for many users there are many leisure-time and “entertainment” aspects of the same
systems. Anders Albrechtslund suggests that in these areas, surveillance may be “potentially empowering,
subjectivity building and even playful” (Albrechtslund, 2008, p. 1). This underscores what Snowden said in
a speech: “I five on the Internet” (2015, video).

The question of why certain populations would comply so readily with surveillance is important
and has been rehearsed, but it does not tell the whole story by any means. The second and larger issue is
why such populations might also participate in, actively engage with, and initiate surveillance themselves.
The fact that the tools for such activities are increasingly available is part of the answer, but that can
hardly be the whole story. After all, some tools are adopted and used while others are ignored and
neglected. Additionally, the markets are volatile, especially in social media platforms, with some erstwhile
leaders such as Facebook now losing customers to Instagram or Snapchat. As in other spheres, social
engagement with new technologies cannot somehow be read off technological capacities or availability.
These are sociotechnical phenomena.

Turning more specifically to the components of surveillance culture, I suggest that together the
concepts of imaginaries and practices serve well to frame the discussion. Building on Charles Taylor's
analysis of “social imaginaries” (Taylor, 2004, 2007) surveillance social imaginaries (or simply
“surveillance imaginaries”) have to do with shared understandings about certain aspects of visibility in
daily life, and in social relationships, expectations, and normative commitments. They provide a capacity
to act, to engage in, and to legitimate surveillance practices. In turn, surveillance practices help to carry
surveillance imaginaries and to contribute to their reproduction.

Surveillance imaginaries are constructed through everyday involvement with surveillance as well
as from news reports and popular media such as film and the Internet. They include the growing
awareness that modern life is lived under surveillance, that this affects social relationships in many ways—
for instance, Will my employer look at my antics on this Facebook page?—that the very idea of an
expectation of privacy may be moot, and that everything from complacency to confrontation may be
appropriate modes of responding to surveillance. Surveillance imaginaries offer not only a sense of what
830 David Lyon International Journal of Communication 11(2017)

goes on—the dynamics of surveillance—but also a sense of how to evaluate and engage with it—the duties
of surveillance. Such imaginaries, in turn, inform and animate surveillance practices; the two belong
together.

Surveillance practices may be both activities that relate to being surveilled (responsive) and also
modes of engagement with surveillance (initiatory). Examples of the former, responsive practices, might
include installing some form of encrypted protection from unwanted attention from national security
agencies or marketing corporations, or wearing clothing that limits camera recognition in public places, or
eschewing the use of loyalty cards. Examples of the latter, initiatory practices, on the other hand, might
include installing a dash-cam to record the activities of other road users while one is driving, using social
media to check up on personal details of others, including complete strangers, or indulging in self-
surveillance through monitoring heart rates or calculating activity duration and intensity with devices such
as Fitbits (often referred to as the “quantified self”; see above, p.4). As noted, these are analytical
distinctions and some kinds of practices may include elements of each.

Exploring today’s surveillance culture through the lenses of imaginaries and practices offers fresh
ways of thinking about surveillance in general. It opens up a much more complex cultural landscape than
can be captured with the concepts of surveillance state or surveillance society (though it does not
supersede them) and simultaneously takes us beyond simple conceptual binaries such as power-
Participation, in/visibility, and privacy-publicness. As noted, for example, for many users of social media,
despite popular perceptions to the contrary, privacy is still a valued condition, but so also is publicness
(boyd, 2010).

It is worth reemphasizing that the term surveillance culture does not for a moment signify any
unified or all-embracing situation. It is merely an umbrella term for many different kinds of phenomena
that points to the reality of a “whole way of life” that relates, positively and negatively, to surveillance.
The emphasis on imaginaries and practices already indicates the variety of phenomena that exists in this
context. On the other hand, one can discern patterns, just as Michel de Certeau (1984) shows in The
Practice of Everyday Life, where the major strategies of, say, consumption, are reappropriated in everyday
situations. Within surveillance culture, people both negotiate surveillance strategies—for instance, often
seeing the giving of personal data as a trade-off for personal benefit (Rainie & Anderson, 2014)—and also
adopt them as their own, modifying them for their circumstances and initiating forms of surveillance on
themselves and others (e.g., Trottier, 2012).

Sharing as Exposure

A key aspect of today’s nascent surveillance culture is the imperative to share. Social media is in
some ways synonymous with such sharing, and the theme has also been picked up in films and novels
such as, classically, Dave Eggers’s The Circle (2013), where it takes the form of a corporate—but also a
coded, post-Orwellian slogan—“Caring is sharing.” The connection with the corporate sector is crucial
because this links it with the user-generated-content of Web 2.0 and the more general phenomenon of
prosumption. From a corporate perspective, such prosumption and sharing is the fons et origio of the
flowing floods of data on preference, habits, opinions, and commitments of digital technology users that
International Journal of Communication 11(2017) Surveillance Culture 831

can be used for advertising or, perhaps more properly, the construction of consuming subjects (Turow,
2011). In this section we discuss some recent analyses of “sharing” that comport with the surveillance
culture argument.

As Deborah Lupton (2015) observes, this may be theorized, critically, as a means of perpetuating
neoliberal principles and as a central way in which corporations monetize content sharing and circulation.
Moreover, it may mask ways in which classic consequences of capitalist practice continue in contemporary
forms, creating discrimination and disadvantage for certain populations. At the same time, Lupton points
to the ways that—in a similar vein to Albrechtslund—social media users enjoy creating content of all kinds
and benefit from the feedback of other users, a process that helps to keep the whole system in motion, or
in current usage, “spreadable.” As she says, “The sharing subject seeks to recirculate content as part of
their identity and participation in social networks and communities,” believing that it will “have an impact
on their networks” (Lupton, 2015, p. 30).

Sharing may also be thought of as an aspect of exposure (Ball, 2009), in which persons are made
more visible by others or—and this is the relevant sense—deliberately make themselves more visible.
Kirstie Ball explores “exposure” in terms of the “political economy of interiority’—in which institutions
associated with technology, media, employment, and consumption create a demand or mobilize resources
to focus on psychological states, or intimate behaviors. This is discussed further by Bernard Harcourt in
his book Exposed (2015), which explores the ways in which the willingness to self-expose online has
become a defining feature of our times. The issue of exposure has become far more widely discussed in
the intervening years since 2009, as social media in particular became so central to social life.

Ball's leading concern is that subjectivity tends to be underplayed in the surveillance literature, in
which subjectivity is often seen primarily in terms of oppression, coercion, ambivalence, or ignorance.
Against this, she proposes that—at least—reflexivity, performativity, embodiment, and the psychoanalytic
be brought more clearly into the picture. The fact that people may not actively resist or even question
surveillance does not necessarily mean that they do not care about it, suggests Ball. You may hate the
biometric camera at airport security, but you have to hide your feelings if you want to travel. There are
many reasons why surveillance may be tolerated or even sought after, or why surveillance, negatively
construed, may be seen as less significant in some situations than what are taken to be its positive
benefits. The obvious example is engaging with social media or using loyalty cards even though users are
aware of the ways that both corporate and government bodies may be tracking their activities.

In particular, Ball draws on John McGrath’s (2004) work on performativity to explore how certain
psychoanalytic dimensions of surveillance may be illuminated. McGrath insists that subjects of surveillance
still make choices, however fleetingly, when “hailed” by the system in question. Experiences relating to,
for example, reality TV, talent shows, or pornography place “a generic high value on the capture of
authentic embodied experiences in a range of settings” (Ball, 2009, p. 645). Thus, through employment
situations, new media, and biometrics, the political economy of interiority helps to connect “inner” and
“outer” lives in various interconnected ways. Considering what exposure means in such contexts is the
burden of Ball's work. While it may have negative connotations, such as vulnerability or abandonment,
exposure may also be actively sought for pleasure or satisfaction.
832 David Lyon International Journal of Communication 11(2017)

Exposure, in surveillance contexts, occurs for various possible reasons. The institutions involved,
whether in call centers, reality shows, or media portrayals of international “crises,” wish to shape the
responses of those employed or depicted without resorting to tactics that might thwart the authenticity of
the subjects in question. Clearly, the emphasis on “authenticity” plays a crucial role. Subjects may in
some sense be required to comply, but their active involvement means that their knowledge, desires, and
expectations will form part of the outcome. So how is personal exposure legitimated in such contexts?

Various responses to this challenge exist, some of them noted by Ball. Gary Marx, for example,
proposes that in relatively unintrusive “soft surveillance” situations subjects may be more willing to give
up body data, whereas Frank Furedi adds to this by noting that the “confessional mode” of today’s
“therapy cultures” encourages public displays of vulnerability. Jodi Dean (2002) goes beyond this, pointing
out that simply in order to be informed one must reveal more about oneself in the public domain—for
example, in seeking online answers to questions of personal health. The public’s “right to know” places
high value on such publicity. Indeed, this can be generalized further, according to Dean, to the point of
arguing that “uncovering secrets” is the key to a healthy democracy.

To consider the mushrooming phenomenon of exposure as a deliberate tactic is to acknowledge
that there is more to “data subjects” than the reductionistic and passive position in which they are often
found. Blind or bland user compliance should not be assumed by commentators or analysts. The realities
of the lived body, rather than merely thinking of bodies as “reduced to information,” should be kept in
sight. Travelers going through airport security, for instance, may feel demeaned by the sense that the
data-on-the-screen substitutes for their own narratives, but their lack of complaint may relate not to the
unreality or unimportance of the negative experience, but rather to the fact that they are at the airport to
fly—not speaking out at that point may just reflect their existential priorities at that moment (Saulnier,
2016).

It may also be that, in certain circumstances, the desire to be exposed could be seen as a mode
of resistance. There are extreme cases of this, of course, one being the activities of Hasan Elahi, who
informs the NSA—and anyone who goes to his website—of his movements, eating habits, and so on, 24/7.
In the everyday world of exposure, however, surveilled subjects experience surveillance through a series
of complex layers, each of which can be uncovered by surveillance. How institutions prompt different kinds
of responses to surveillance is crucial—and complicated by the multifaceted character of the situations in
which such surveillance is experienced. Not reducing the experience of surveillance to a one-dimensional
or binary—“compliance or resistance”’—format and acknowledging the variety and subtlety of responses
helps us understand the lived realities of surveillance subjects.

“Desire” is also an important element inspiring exposure. As seen by Gilles Deleuze and Felix
Guattari in Anti-Oedipus (1972, 2004), desire is not merely a response to lack but as a productive force.
For Harcourt (2015), a social-media-saturated era—what he dubs a “digital frenzy”’—encourages our
knowing self-exposure or self-exhibition. He sees this as particularly true for younger people; the teens
interviewed by danah boyd, for example, who believe that unless you‘re on social media “you don't exist”
(boyd, 2014, p. 5). Today's situation, says Harcourt, is better described as the “expository society” than
any of Debord’s “society of the spectacle,” Foucault’s “disciplinary society” or Deleuze’s “society of
International Journal of Communication 11(2017) Surveillance Culture 833

control.” However, he sees this as the outcome of our having become “numb to the risk of digital
transparence” (Harcourt, 2015, p. 19). He emphasizes the ways that subjects have been “dulled” by
things like self-centeredness, the illusion of free markets, militaristic homeland security, and
overincarceration. Indeed, he writes that the “see-throughness of our digital live mirrors the all-
seeingness of the penal sphere” (Harcourt, 2015, p. 21). Pleasure and punishment suffuse each other and
work together.

Corporate surveillance, now working through social media, may be thought of as shaping current
subjectivities. Harcourt also argues that this is a crucial new development, “replenished by our own
curiosity and pleasure—retweeted, friended, shared and reposted (p. 50),” thus inserting surveillance
capability into our everyday pleasures. Consumerism liberates the flows of desire, now seen in the digital.
Whereas for Orwell, surveillance power was yoked to destroying desire and passion—“desire was
thoughtcrime”—today these are the very enablers of digital exposure, the means of surveillance. Harcourt
leans on Deleuze and Guatarri’s dream of desire as a “machine” in which the unconscious is a productive
factory. As assemblages of desire-producing machines, we are now linked with other machines—digital
devices like iPhones, Facebook, and the Internet.

Such soft surveillance has today become commonplace and was at first seen as separate from
harder, more coercive forms. However, during the first decade of the 21st century, the U.S. Department
of Homeland Security (DHS) began to use social media for “harder” surveillance purposes, and by the
second, “SOCMINT”—for “Social Media Intelligence’—had become a recognized part of security agencies’
arsenal. Indeed, proposals were made by the DHS to demand social media information of travelers at
border points (Gibbs, 2016). For Harcourt, desire is now in tandem with another mode of power, this time
from Foucault’s writing: not that of surveillance, but of security.

As in the theme park or the shopping mall, so now also with social media, sites of consumption
are often the product of a private-public mix. They optimize the movement of consumers while minimizing
labor and other costs (Andrejevic, 2007). Thus, for Harcourt, digital exposure becomes the “wired space of
secure consumption” (Harcourt, 2015, p. 97). While biopower may be visible in some surveillance
contexts, this postsecuritarian “expository power” focuses on all our little wants, desires, preferences,
beliefs, ambitions, our individuality and differences, says Harcourt, to shape our digital selves.

Despite the focus on desire in Harcourt’s work, one might be forgiven for thinking that in the end
the chances for the development of active agency in the digital realm are downplayed if not remote.
Exposure becomes once again something that appears to be primarily done to rather than by social media
users. True, desire is in many ways directed powerfully by corporate-governmental alliances in the virtual
realm. Nonetheless, in Harcourt’s account, there exist forms of “leaderless resistance” that open up spaces
for otherwise silenced voices within the digital realm. He is thinking of networked movements that gain
strength from their communicating members rather than from strong central leadership. This, he asserts,
requires of all a certain courage and conviction, an “ethics of the self” (Harcourt, 2015, p. 283).

The use of Foucault’s work on the ethics of the self (in the History of Sexuality) is common to
other writers in the field. Lupton points out that the ways in which people configure and represent
834 David Lyon International Journal of Communication 11(2017)

themselves on social media may be construed as ethical self-formation. As aspects of life are shared, so
others express their approval or disapproval through “liking” or sharing the content more widely. This,
arguably, is a self-reflective process in which many users participate and that may contribute not only to
individual self-formation but also to the development of social norms and expectations (Lupton, 2015). To
this we turn next.

Ethics: How to “Go On”

One aspect of a culture of surveillance is that it has an unavoidably evaluative dimension.
Particularly the notion of surveillance imaginaries points toward the normative. Readers of Raymond
Williams will recall that this, too, was a feature of his work; he lamented, for example, the reduction of
ethical to technical concerns. The very idea of culture implies that questions of how to think, to behave, to
act, to intervene are raised, within any given social imaginary. So if the particular slant of the imaginary is
surveillant, then at least some hint of the ethics of surveillance will be present in the practices. Ordinary
subjects need to know how to “go on” in the digital realm as awareness grows of the consequences of the
widespread and multifarious uses of personal data within today’s digital modernity. Everything from
everyday rules of thumb, to more sophisticated shared responses, to surveillance tactics is emerging.

One way of considering this is to return to the notion of transparency, so central to novelist
Eggers’s plot in The Circle. The corporate campus of the Circle—the megacorporation that has swallowed
up the Big Five into one gargantuan organization is itself dominated by glass buildings, all of which may
be seen through. A new device has recently been issued to all employees; a “see-change” camera that
hangs as a pendant from every neck. One of the corporate slogans is that “Everything that happens must
be known,” and the openness, the transparency of all that is going on is the means to that end. The novel
prods and pokes at the transparency that has become a byword of the digital modernity’s surveillance
capitalism.

To discuss transparency, however, is to raise a deeper question of visibility. While transparency
does provoke discussions of its limits—as The Circle makes plain—the more properly ethical questions
arise over how we are made visible and how we make ourselves visible, or cloak our visibility. This
becomes clear in Andrea Brighenti’s pioneering work on visibility as a social process (Brighenti, 2007,
2010). Brighenti rightly argues that visibility is always relational: seeing and being seen are connected;
asymmetries and distortions are common. Western thought privileges vision, but Brighenti observes that
there is no “visible” without ways of seeing that are socially and even internationally crafted (Brighenti,
2007). Visibility, in this view, is associated with recognition, its struggles, and politics. Some disappear,
are excluded; others become supervisible. Most of the time, our experiences come somewhere between
the two. Visibility makes identification possible and breeds a culture of identification. But nothing can be
taken for granted. Visibility does not correlate automatically with recognition or oppression.

Following Brighenti, Eric Stoddart explores visibility as a more illuminating way of considering
surveillance than conventional “privacy’-based critique. His concerns with the latter are that privacy tends
to emphasize individualistic aspects of visibility and rest on an inadequate notion of information. Instead,
he proposes that “in/visibility” captures the dynamic of managing and negotiating visibility in social space.
International Journal of Communication 11(2017) Surveillance Culture 835

As he discusses it, in/visibility is the attempt to control one’s relative position within social space.
In/visibility is active and is not predicated on withdrawal. It is engagement driven rather than defensive
(Stoddart, 2011). In/visibility is cognizant of the social conditions on which it depends and in which we
exercise skills that include evaluating those conditions. It is also aware of the resources at our disposal for
making ourselves more and less visible for strategic purposes. At a small-scale level, this approach
enables people to deal with multiple or fluid identities, while on a larger canvas, it challenges monopolistic
power bases, foregrounding the question of which data should be available to whom, in which contexts
and for how long.

Stoddart’s aim is to find ways forward for an adequate ethics of surveillance, but it depends on a
sophisticated analysis of what I am calling surveillance culture. The practices of in/visibility are a crucial
part of what Stoddart calls a critical ethics of care and of self-transcendence. In this view, surveillance
ought not merely to be of people (technologized risk; isolating privacy) so much as for people—and thus
should be practiced carefully and held to account. This conclusion emerges from a critical account of
rights-based privacy orientations and an embrace of a more discursive-“disclosive” approach that aims
analytically to show what surveillance does or how it is practiced and offers possibilities for alternative
actions. Privacy and rights are not so much abandoned, in this view, as seen as one—limited—way of
considering the possibility of ethics for surveillance. His complementary approach “has the potential to
disrupt fatalistic or protected models of surveillance that foreclose possibilities for critical response”
(Stoddart, 2012, p. 376).

One readily acknowledges that there are many technological, political, and legal responses to
surveillance, and the debates cannot be summarized easily. But it is safe to say that one thing largely—
and regrettably—missing from many mainstream surveillance studies is any serious attention to ethics, or,
it must be said, to the analysis of the implicit ethics of different strands of surveillance culture. Starting
there, the pregnant possibilities of ethics—normative, contextual, disclosive, and relational—may be
probed for fresh approaches that go beyond the technological determinist, the privacy preoccupied, or the
complacent. In an era of apparently unbounded surveillance, in which the appetite for more data seems
insatiable and the types of linked data seem unending, there are vital questions awaiting imaginative and
contextually relevant ethical responses.

Such ethics, like morals, should not be seen as something abstract or disengaged, but rather
something that prompts political agendas and action. The ethics of surveillance flow naturally into the
politics of surveillance, in tune with today’s technosocial and globalized conditions, informing and
challenging current developments. The ethics and politics of complex surveillance situations present new
challenges. Important though they are, regulation and law—even when based on some sound “rights”
criteria—struggle in vain to keep up with the pace of change. Today, the need for both disclosive and
normative ethics is greater than ever. Like Stoddart, I urge a kind of ethics that explores the actual
consequences of surveillance cultures in everyday life, not just one that worries about specific harms,
important though the latter still are from a legal point of view. Our sense of how institutional surveillance
might be confronted, technologically, politically, legally and above all ethically is due for overhaul.
Cultures of surveillance, whether critical or complacent, are socially constructed and can thus be
challenged and reconstructed. But how?
836 David Lyon International Journal of Communication 11(2017)

Surveillance Culture and Beyond

It cannot be stressed enough that the issues discussed are not minor, transient, or contingent.
Surveillance culture is one dimension of a highly significant social, technological, and political-economic
transformation that is unavoidably imbricated with digital modernity. If surveillance culture can be
understood as a matter of surveillance imaginaries and practices, then inevitably it prompts normative and
ethical questions. As I argued earlier, these relate not just to matters of law and limits but also to what is
appropriate in each context and what might enhance human life or enable human flourishing. The
discussion is limited to one area of consideration of what is “beyond” surveillance culture—digital
citizenship. It is indicative rather than systematic or comprehensive.

This article argues for a careful, critical, and cu/tura/ analysis of surveillance situations. It
suggests that we go beyond common designations such as surveillance state and surveillance society. But
it is also worth picking up a strand from the start of the article, that the Snowden disclosures are better
understood if considerations concerning surveillance culture are drawn into the mix. More than one
response to Snowden has insisted that we try to understand better the actual practices of the users of
smart phones and the Internet to seek a practical ethics appropriate to today’s situation (e.g., Bauman et
al., 2014). Such are more likely to find traction in the complex liquid world of surveillance today.

The point of analyzing surveillance culture is to uncover not only the various kinds of imaginaries
and practices of surveillance but also to understand how those lived Ies connect with ethical challenges—
how to go on in daily, digital life. This is not to say that the analysis of surveillance culture is not
worthwhile in its own right. It is—for a number of reasons, mentioned earlier. But such analysis may also
contribute to other kinds of debates, especially those relating to privacy and data protection, and to those
about social responsibility and citizenship in the time of digital modernity. Such debates are often marred
by a failure to acknowledge how the very terms of debate have altered in the 21st century.

The practices that have emerged, for instance in the world of social media, are socia/ practices,
and the imaginaries that they inform and that shape them are equally socia/. For Taylor, social imaginaries
incorporate “a sense of normal expectations we have of each other, the kind of common understanding
that enables us to carry out the collective practices that make up our social life” (2004, p. 24). They are
thus simultaneously factual and normative—people know how things go, but it is never disconnected from
a sense of how things ought to go. That this is also true of what I call surveillance imaginaries is clear
from the contexts within which, for instance, camera surveillance may be thought of as acceptable, or
otherwise. Bathrooms are off limits, while cameras at traffic signals are often seen as legitimate. Helen
Nissenbaum’s work on contextual integrity, which stresses the malleability of privacy according to its
setting, underscores this importance of context. Julie Cohen insists that academic and legal treatments
often reduce privacy to technical and abstract matters whereas in fact our handling of information is
always an embodied experience. Such lived experiences are vital, today, for the ways in which the self—
also a key concern of Taylor’s—is “configured” within digital networks (Cohen, 2012; Nissenbaum, 2009).

By the same token, they also have some unavoidably political aspects that implies some notion of
citizenship. Debates over citizenship classically refer to membership and responsibilities within the nation-
International Journal of Communication 11(2017) Surveillance Culture 837

state. But they may equally be thought of in terms of rights claims, or in terms of the kinds of
responsibilities that are incumbent on those who are connected, in this case, digitally. Now, debates over
digital citizenship have sometimes been rather limited—not to say idealistic—but as much of life is
increasingly lived online, there is an urgent need to consider digital citizenship more broadly.

One recent study of digital citizenship that resonates well with the discussion of surveillance
culture is Engin Isin and Evelyn Ruppert’s Being Digital Citizens (2015), and this can be commended as a
study that opens up emerging issues relating to participants’ actual online behavior. The book is a unique
collaboration that intermingles citizenship studies and digital studies. Its starting point is Foucault’s insight
that becoming citizens means being constituted as subjects of power—only now they enact themselves
through the Internet both submissively and subversively. In the end, Isin and Ruppert ask the same kinds
of questions that I have asked; in their words, “How do we conduct ourselves through the Internet?”
(2015, p. 13).

This question cuts to the chase—or chases—of what digital citizenship might look like. On the one
hand, there may be some urgent questions that have to do with teens interacting online: They often have
clear ideas about what is and is not appropriate in their own communications with peers and in their
relationships with parents and teachers (Steeves, 2006). On the other hand, other equally urgent issues
concern how our online interactions should be handled in a post-Snowden environment. Bauman and
others ask whether Internet users “will continue to participate in their own surveillance through self-
exposure or develop new forms of subjectivity that are more reflexive about the consequences of their
own actions?” (Bauman et al., 2014, p. 124). Much hangs, at very different levels, on how these issues
are addressed.

Isin and Ruppert conclude that digital citizenship connects especially with what they call digital
acts—legal, performative, and imaginary—and with rights to expression, access, and privacy—plus now,
openness and innovation. They write of the many individuals and groups who are exemplars of digital
rights activists and campaigners. But they also point to a much wider swath of persons who “live on the
Internet” who engage in “dissensus” as well as promoting positive values online. They drive no wedge
between those who make digital rights claims as they act on the Internet and those who work toward
“bills, charters, declarations and manifestos” (Isin & Ruppert, 2015, p. 179), seeing the two in
complementary fashion, much as was argued earlier, do practices and imaginaries. And while there may
be important regional variations, the emerging figures who are digital citizens not only represent tradition
but also a politics of a citizen to come.

Conclusion

This article argues that the concept of surveillance culture should be developed to understand
more clearly the relations between contemporary surveillance and the everyday lives of those who might
be described as its subjects. Using the work of Charles Taylor on “modern social imaginaries” as a
springboard, it is suggested that two interrelated terms, surveillance imaginary and surveillance practice
may be used to marshal analyses of how those immersed in the digital world (and beyond, of course; the
focus here is on the communicational dimension of surveillance) conceive of and act in surveillance
838 David Lyon International Journal of Communication 11(2017)

contexts. Different experiences, for example, of fear, familiarity, and fun, may produce different outcomes
in terms of compliance with institutional surveillance, whether governmental or corporate.

Taking this further, it is suggested that to grasp the ethical and political challenges of digital
modernity, a concept such as surveillance culture is vital. Why? Because the dominant public and
academic discourses about surveillance are couched in terms of the surveillance state or surveillance
society. Neither of these is adequate today, not least because they tend to accent the viewpoint of the
surveillor, the agent of surveillance, and often fail to give place to the ways that (what are called here)
surveillance imaginaries and practices produce complacency, compliance, negotiation, or resistance.

This strategy of deliberately analyzing the surveillance imaginaries and practices of surveillance
subjects is intellectually appropriate in that it attempts to grapple more realistically with contemporary
digital realities as well as other, residual surveillance situations not directly mediated by the digital. But it
also helps to connect with other significant debates about how to respond, for example, to the disclosures
made by Edward Snowden about global, intensive, suspicionless surveillance. Legal and_ political
approaches, seen for some time as sorely inadequate to the task of confronting contemporary
surveillance, will benefit tremendously from trying to get to grips with the diverse lived experiences of
those often lumped together as “users,” as, for instance, the work of Julie Cohen, on the legal side, or
Engin Isin and Evelyn Ruppert, on the political, clearly show.

Considering the significance of surveillance culture is a fruitful way of going beyond earlier
analyses of the surveillance state or the surveillance society. Those concepts retain their salience for many
situations, but are of limited value in providing a full-orbed understanding of today’s surveillance—
especially in the mainly online contexts discussed. Much more work will be needed to fill out this work
satisfactorily, but I hope that this brief article will stimulate such endeavors.

References

Albrechtslund, A. (2008). Online networking as participatory surveillance. First Monday, 13(3). Retrieved
from http://firstmonday.org/article/view/2142/1949/

Andrejevic, M. (2007). iSpy: Surveillance and power in the interactive era. Lawrence: University Press of
Kansas.

Bader, C. (2016). America’s top fears 2015. The Chapham University Survey on American Fears. Retrieved
from https://blogs.chapman.edu/wilkinson/2015/10/13/americas-top-fears-2015/

Ball, K. (2009). Exposure: Exploring the subject of surveillance. Information, Communication and Society,
12(5), 639--657

Ball, K., Canhoto, A., Daniel, E., Dibb, S., Meadows, M., & Spiller, K. (2015). The privacy security state:
Surveillance, consumer data and the war on terror. Copenhagen, Denmark: Copenhagen
International Journal of Communication 11(2017) Surveillance Culture 839

Business School Press.

Bauman, Z., Bigo, D., Esteves, P., Guild, E., Jabri, V., Lyon, D., & Walker, R. B. J. (2014). After Snowden:
Rethinking the impact of surveillance. International Political Sociology, 8(2), 121-144.

Bauman, Z., & Lyon, D. 2013. Liquid surveillance: A conversation. Cambridge, UK: Polity Press.

Bennett, C., Haggerty, K., Lyon, D., & Steeves, V. (Eds.). (2014). Transparent lives: Surveillance in
Canada. Edmonton, Canada: Athabasca University Press.

boyd, d. (2010, March 31). Making sense of privacy and publicity. Paper presented at SXSW conference,
Austin, Texas. Retrieved from http://www. danah.org/papers/talks/2010/SXSW2010.html

boyd, d. (2014). It’s complicated: The social lives of networked teens. New Haven, CT: Yale University
Press.

Brighenti, A. (2007). Visibility: A category for the social sciences. Current Sociology, 55(3), 323-342.
Brighenti, A. (2010). Visibility in social theory and social research. London, UK: Palgrave Macmillan.

Cohen, J. (2012). Configuring the networked self: Code law and the play of everyday practice. New
Haven, CT: Yale University Press.

Crawford, K., Lingel, J., & Karppi, T. (2015). Our metrics, ourselves: One hundred years of self-tracking
from the weight scale to the wrist wearable device. European Journal of Cultural Studies,
18(4/5), 479-496.

De Certeau, M. (1984). The practice of everyday life. Berkeley: University of California Press.

Dean, J. (2002). Publicity’s secret: How technoculture capitalizes on democracy. Ithaca, NY: Cornell
University Press.

Deleuze, G., & Guattari, F. (1972, ET 2004). Anti-Oedipus: Capitalism and schizophrenia. New York, NY:
Continuum.

Dijck, J. van. (2013). The culture of connectivity. New York, NY: Oxford University Press.

Dijck, J. van. (2014). Datafictaion, dataism and dataveillance: Big data between scientific paradigm and
ideology. Surveillance & Society, 12(2), 197-208.

Eggers, D. 2013. The circle. San Francisco, CA: McSweeney’s.

Epstein, C. (2016). Surveillance, privacy and the making of the modern subject. Body & Society, 22(2),
840 David Lyon International Journal of Communication 11(2017)

28-57.

Ericson, R., & Haggerty, K. (2000). The surveillant assemblage. British Journal of Sociology, 51(4), 605-
622.

Finn, J. (2012). Seeing surveillantly: Surveillance as social practice. In A. Doyle, R. Lippert, & D. Lyon
(Eds.}, Eyes everywhere: The global growth of camera surveillance (pp. 67-80). London, UK:

Routledge.

Garland, D. (2001). The culture of control: Crime and social order in conteporary society. Chicago, IL:
University of Chicago Press.

Gibbs, S. (2016, June 28). U.S. border control could start asking for your social media accounts. The
Guardian. Retrieved from https://www.theguardian.com/technology/2016/jun/28/us-customs-

border-protection-social-media-accounts-facebook-twitter

Harcourt, B. (2015). Exposed: Desire and disobedience in the digital age. Cambridge, MA: Harvard
University Press.

Isin, E., & Ruppert, E. (2015). Being digital citizens. London, UK: Rowman and Littlefield.

Kuner, C. (2014). Transborder data flow regulation and data privacy law. Oxford, UK: Oxford University
Press.

Lupton, D. (2015). Digital sociology. London, UK: Routledge.
Lyon, D. (2003). Surveillance after September 11. Cambridge, UK: Polity Press.
Lyon, D. (2007). Surveillance studies: An overview. Cambridge, UK: Polity Press.

Lyon, D. (2014). The emerging culture of surveillance. In A. Janssen & M. Christensen (Eds.), Media,
surveillance and identity (pp. 71-90). New York, NY: Peter Lang.

Lyon, D. (2014a). Surveillance, Snowden and Big Data: Capacities, Consequences, Critique, Big Data &
Society 1(1), 1-13.

McGrath, J. (2004). Loving big brother: Surveillance culture and performance space. London, UK:
Routledge.

Mosco, V. (2014). To the cloud: Big data in a turbulent world. London, UK: Routledge.

Murakami Wood, D., & Webster, W. (2009). Living in surveillance societies: The normalisation of
surveillance in Europe and the threat of Britain’s bad example. Journa/ of Contemporary European
International Journal of Communication 11(2017) Surveillance Culture 841

Research, 5(2), 259-273.

Nissenbaum, H. (2009). Privacy in context: Technology, policy and the integrity of social life. Stanford,
CA: Stanford University Press.

Rainie, L., & Anderson, J. (2014). The future of privacy. Pew Research Center. Retrieved from
http://www. pewinternet.org/2014/12/18/future-of-privacy/

Rainie, L., & Madden, M. (2015). America’s privacy strategies post Snowden. Pew Research Center.
Retrieved from http://www. pewinternet. org/2015/03/16/americans-privacy-strategies-post-

snowden/

Saulnier, A. (2016). Surveillance studies and the surveilled subject (Doctoral thesis). Queen’s University,
Kingston, Canada.

Staples, W. G. (1998). The culture of surveillance: Discipline and social contro! in the United States. New
York, NY: St Martin‘’s Press.

Steeves, V. (2006). It’s not child’s play: The online invasion of children’s privacy. University of Ottawa
Law and Technology Journal, 3(1), 171-187.

Stoddart, E. (2011). Theological perspectives on a surveillance society: Watching and being watched.
London, UK: Routledge.

Stoddart, E. (2012). A surveillance of care: Evaluating surveillance ethically. In K. Ball, K. Haggerty, & D.
Lyon (Eds.), The Routledge handbook of surveillance studies (pp. 669-676). London, UK:
Routledge.

Taylor, C. (2004). Modern social imaginaries. Durham, NC: Duke University Press.

Taylor, C. (2007). A secular age. Cambridge, MA: Harvard University Press.

Trottier, D. (2012). Social media as surveillance. London, UK: Ashgate.

Turow, J. (2011). The daily you: How the new advertising industry is defining your identity and your
worth. New Haven, CT: Yale University Press.

williams, R. (1958). Culture and society: 1780-1950. London, UK: Chatto and Windus.

Wolf, G. (2010, April 27). The data-driven life. The New York Times. Retrieved from
http://www. nytimes.com/2010/05/02/magazine/02self-measurement-t.html

Zuboff, S. (2015). Big other: Surveillance capitalism and the prospects of an information civilization.
842 David Lyon International Journal of Communication 11(2017)

Journal of Information Technology, 30, 75-89.

Zuboff, S. (2016, March 5). The secrets of surveillance capitalism. FAZ Feuilleton. Retrieved from
http://www. faz.net/aktuell/feuilleton/debatten/the-digital-debate/shoshana-zuboff-secrets-of-
surveillance-capitalism-14103616.html/

Zureik, E., Stalker, L. H., & Smith, E. (2010). Surveillance, privacy and globalization of personal
information. Montreal, Canada: McGill-Queen’s University Press.
Digital Ethics Now!

ALLAN LUKE

The history of language and literacy education offers an important lesson—that the
teaching and learning of communication, by definition, entails ethical and ideological
constraints and conventions, however explicit or implicit these may be to learners. To
rethink current policy and curriculum strategies, consider this alternative proposition: The
educational challenge raised by digital culture is not one of skill or technological
competence, but one of participation and ethics. Accordingly, digital education would
move far beyond the current attempts to expand curriculum definitions of competences and
capacities. As a matter of social justice, it requires, as the articles in this issue call for,
nothing less than (1) equitable access; (2) ongoing dialogue over the personal and collective
consequences of everyday actions and exchanges with digital resources and social media;
(3) critical examination of the semantic contents of the digital archive and how these
represent the world; and (4) the use of digital media for the exchange of ideas, viewpoints,
and resources as part of a renewed civics and civility across communities old and new,
residual and emergent.

The everyday issues faced by digital youth are ethical matters. How do today’s
young people and children deal with right and wrong, truth and falsehood, representation
and misrepresentation in their everyday lives online? How do they anticipate and live with
and around the real consequences of their online actions and interactions with others? How
do they navigate the complexities of their public exchanges and their private lives, and how
do they engage with parental, corporate, and government surveillance? Finally, how can
they engage and participate as citizens, consumers and workers, friends, colleagues, and
kin in the public and political, cultural and economic spheres of the internet? These
questions are examined in current empirical studies of young peoples’ virtual and real
everyday lives in educational institutions and homes (e.g., Livingstone & Sefton-Green,
2016; Quan-Haase, 2016).

The Problem

There are now almost continuous public calls for heightened child protection and
surveillance in response to widespread moral panic around digital childhood (e.g., Havey
& Puccio, 2016). These range from concerns about the displacement of embodied activity,
physical play, and face-to-face verbal exchange by compulsive online messaging and
gaming to online harassment, bullying, real and symbolic violence, and corporate and state
surveillance and data mining; from sexual and commercial exploitation of young people
and children to exposure to violence, pornography, ideological indoctrination and outright
criminal behavior. Their power to generate fascinating new expressive forms and

 

This paper is adapted from Luke, A., Sefton-Green, J., Graham, P., Kellner, D., & Ladwig, J. (2017)
Digital Ethics, Political Economy And The Curriculum: This Changes Everything. In K. Mills, A.
Stornaiuolo, & J. Zacher Pandya, (Eds.) Handbook of Writing, Literacies, and Education in Digital
Cultures (pp. 251-263). New York: Routledge.

 

Language and Literacy Volume 20, Issue 3, 2018 Page 185
relationships and reshape the arts and sciences notwithstanding, digital media are
amplifiers of the best and the worst, the sublime and the mundane, the significant and the
most trivial elements of human behaviour, knowledge, and interaction. How could it be
any other way? It is all here online—statements, images, sounds; acts of hatred and love,
war and peace, bullying and courtship, truth and lies, violence and care, and oppression
and liberation—and in every possible third or fourth space, in ever proliferating
redundancy, cut through with noise and clutter.

How we can enlist and harness these media to learn to live together in diversity,
mutual respect, and difference—addressing complex social, economic, and environmental
problems while building convivial and welcoming, just and life-sustaining communities
and societies—is the key educational problem facing this generation of young people and
their teachers. This is an ethical vision and an ethical challenge.

Many school systems are in shock and denial over this turn of events, especially
given the historic use of print textbooks as a practical and effective means for defining and
controlling what might count as official knowledge for children and youth. Schools have
responded with a patchwork of rules governing what kids can and cannot do in their online
exchanges and communications. These emerge in a reactionary and agglomerative way,
often in response to incidents of abusive, illegal, or symbolically violent online acts, or to
events whose origins are attributed to online actions—from suicides to gun violence to
pedophilia. Schools work from a mix of regional and district-level policies that include
constraints on hardware access, proprietary lockout and surveillance systems, privacy and
intellectual property regulations, and school-level codes and class rules on everything from
texting and screen time to plagiarism and copying from internet sources. These sit
alongside home-based restrictions (or freedoms) on time, access, and use in those families
that can afford mobile and online devices. This is complicated by increasing law
enforcement efforts to prevent the online recruitment, exploitation, and indoctrination of
youth by terrorist groups, financial scammers, and criminal organisations. In this thicket of
overlapping systems of surveillance, unmediated exchange by youth and children would
appear to be the exception rather than the rule (Boyd, 2015). Taken together, the digital
strategies of large public education systems in North America, the Asia Pacific, and Europe
appear to be at best post hoc and piecemeal—motivated by genuine concern and real
problems, but typically lacking stated ethical foundations and working within prevailing
neoliberal policy frameworks (the latter of which have eschewed engagement with
educational philosophy and ethics more generally). This underlines what has become a
significant (meta) ethical dilemma in itself—that the policy push for teaching through and
about educational technology presents itself as ethically and politically neutral.

Ethics refers to the codes, norms, and procedures that govern everyday life and
interaction, civility, and exchange in institutions, societies, and cultures (Dewey, 2008).
Digital ethics—the normative principles for action and interaction in digital
environments—cannot be addressed through a listing of prohibitions for what kids can and
cannot do online. For those young people whose families and communities have affordable
everyday access to the internet?—and, in fact, many rural and remote, Indigenous and

 

? The common claim that the internet is now universal is unfounded. While composite estimates are that 89%
of North Americans and 73% of Europeans and Australians have Internet access, global access continues to
be below 50%. Quan-Haas (2016) further describes the persistent stratification of Canadian and American
access by social class, age and social geography. See: http://www.internetworldstats.com/stats.htm

 

Language and Literacy Volume 20, Issue 3, 2018 Page 186
economically marginal communities do not have such access—knowledge and learning,
civic participation, work, leisure, and everyday social interaction with their peers and
others occurs online. Digital actions—whether clicking or tweeting, posting, sharing or
liking—are by definition social actions; as such, they are used for goal-seeking purposes
with real pragmatic effects and consequences (Wilden, 1972).

Digital actions—even those of children and youth, students, and “average”
citizens—may carry higher stakes and have amplified consequences that exceed the scope
of their actions through speech, writing, and other modalities in everyday life. In real
human experience and real geo/spatial and temporal contexts, digital actions can be used
to launch drone strikes; they can pass on complex technical information for making
weapons; they can draw the attention and approbation of millions to shame and humiliate.
They can be used for play, to build community, to solve complex problems, to mobilise
constructive and destructive social action. As is axiomatic in critical discourse theory,
while much of what we know and experience in the world is represented through discourse,
some discourse actions don’t matter much; others may kill, wound, maim and desecrate;
and, indeed, some may enlighten and heal (Luke, 2004). Digital action is discourse—
semiotic and social action through a “cognitive amplifier’ (Bruner & Olson, 1977) that
may have expansive and reflexive, durable and exponential effects across space and time.

In consequence, my case here is that digital ethics—an ethics of what it is to be
human and how to live just and sustainable lives in these technologically saturated societies
and economies—is ‘we core curriculum issue for schooling. It is not an adequate
educational, philosophic, or political response to current cultural, geopolitical, and
economic conditions and events for this generation of teachers and scholars, parents,
caregivers, and community Elders to simply document or celebrate the emergence of new
digital youth cultures without attempting to call out ethical parameters and concrete
historical consequences for communities, cultures, and, indeed, human existence in this
planetary ecosystem. This is a generational and pedagogic responsibility as we stand at a
juncture where residual and emergent cultures meet; where Indigenous and non-
Indigenous, historically colonized and colonizing, settler and migrant communities attempt
to reconcile and negotiate new settlements; where traditional, modernist, and postmodern
forms of life and technologies sit alongside each other, uneasily, often with increasing
inequity and violence.* All, it turns out, under the watchful eye of multiple layers of
surveillance and analysis—by the state, policing and military authorities, corporations and,
it would seem, any and every species of subcontractors, consultants, and “researchers”
seeking commercial, political, and economic advantage. This is a moment that requires
more from researchers, scholars, and educators than descriptions of instances of local
assemblage, creativity, or voice.

 

3 Dewey (1934) defines art as human endeavor meant to make the world coherent (“cohate”) and to address
and resolve problems resulting from “organism-environment disequilibria”. This is comparable to Freire’s
(1970) call for education to “‘problematicise” the world.

4 The Yonglu Aboriginal peoples of Northwest Arnhem use the term ganma to describe cultural contact,
blending, and, potentially, conflict. This refers to the point in river estuaries where fresh and salt water meets
and blends. Its application to Aboriginal “two-way education” is attributed to Mandaway Yunipinnu of
Northwest Arnhem Land (see: http://livingknowledge.anu.edu.au/html/educators/07_bothways.htm). See
also: Canadian economist Harold A. Innis’ (1951) history of the river as a medium for intercultural exchange,
communications, and transportation.

 

Language and Literacy Volume 20, Issue 3, 2018 Page 187
The alternative is to outline a definition of human ethics in relation to
communications media. The case here is for a critical literacy based upon common
principles of social justice in relation to all forms of human communication. Classroom
practice—the everyday curriculum enacted through speaking and listening, print and
digital reading and writing, signing, and imaging—can be refocused to include rigorous
debate, study, and analysis of digital communications in terms of their real consequences
as human actions; their ideological, scientific, and cultural codes, truth claims, and
meanings; and their everyday possibilities for community-based cultural and social
action—for art and science, human conviviality, and sustainable forms of life.

What is needed, I would argue, is to reconceive the central aim of schooling as the
interrogation of the forms and contents, practices, and consequences of digital
communications. The curriculum should engage developmentally and systematically with
the current issues regarding everyday actions and their consequences; corporate and state
surveillance, privacy, and transparency; and political and economic control and ownership.

Reframing Communicative Ethics

To speak about ethics is to speak about the moral codes and norms of everyday life.
The nominal foundations of Western ethics are attributed to Plato and Aristotle. Yet a//
cultures—Indigenous, African, and Asian, historical and contemporary, and Eurocentric—
depend upon normative rules, stated and unstated, regarding the rightness and
appropriateness of actions and interactions, actions, and transactions. That is, the conduct
of daily practices, the coherence and cohesion of everyday communications, and the
functional survival of communities depend upon shared (and, indeed, contested and
dynamic) codes of conduct, epistemic standpoints, and worldviews. Without normative
“cultural scripts” (Cole, 1996), everyday problem solving and learning are impossible.
Ethical norms are presupposed in every instance of communication and exchange in social
fields. Communicative ethics, then, comprise a kind of master cultural script that sets the
interactional grounds and meditational means for building, critiquing, and using other
scripts. Given the contentious political and cultural issues that schools and communities,
teachers and children now face—even where we cannot presume ideological agreement or
moral consensus, especially where we are not idealized, rational (white, male,
heterosexual, urbane) speakers with equitable access to cultural codes, discourses, and
knowledge (Benhabib, 1992)—how could this not be the centre of any curriculum?

New communications technologies have the effect of destablising and reframing
social and economic relations, living cultures, and planetary ecosystems. Such changes
raise and renew ethical dilemmas. At the macroeconomic and geopolitical levels, the
reorganization and compression of space and time enabled by communications (and
transportation) technologies have enabled new forms of monopoly, of profit, debt and,
indeed, of cultural and economic empire (Innis, 1949). The transitions from oral to literate
culture, from manuscript to print culture, and, currently, from print and oral to digital
exchange have destabilized and altered relations of power, authority, and control. This
occurs on several levels—both in terms of the actual everyday mediation of what will count
as knowledge, action, and utterance, and in terms of whose collective cultural, economic,
and political interests are actually served through these interactions. With the coming of
the book (and newspaper, broadsheet, treatise, contract and legal brief, domestic manual,
and romantic novel) and the emergence of nationalism and “print capitalism” (Anderson,

 

Language and Literacy Volume 20, Issue 3, 2018 Page 188
1983), the question of who owns, regulates, and controls—and profits and dominates—
from control and use of the dominant modes of information comes centre stage, shifting
from religious authorities to the state and, ultimately, to the industrial and postindustrial,
national, and transnational corporation. Some regimes burn books; others write, print, and
mandate them. Some governments censor the internet; all use it and monitor it. Disputes
over hate speech, libel, and what can and cannot be said in the media-based civic sphere
are now daily news—alongside revelations of the profit structures, labor practices,
environmental consequences, and taxation schemes of those media and technology
corporations that have become arguably the most profitable and dominant businesses in
human history. Note that this political economy of communications typically is not studied
in schools—even as this corporate order competes for the edubusiness of what counts as
knowledge, and how it is framed and assessed within these same schools (Picciano &
Spring, 2012).

A first task facing institutions, then, is to reframe and renew dialogue over ethics
in relation to both changed human interaction, contexts for thought and action, and changed
societal, cultural, and environmental ecologies. As is painfully clear in the current
geopolitical and national debates over borders, terrorism, security, trade, and globalization,
establishing criterial grounds for adjudicating right and wrong, true and untrue, scientific
and unscientific, civil and uncivil, humane and inhumane, and private and public
knowledge and behaviour is increasingly difficult for adults—let alone for young adults
and children—as citizens, workers, consumers, voters, and audiences. We live in an era of
post-truth, truthiness, factoids, and simulacrum—where freedom of speech and expression
is construed by many as meaning that all spoken or expressed statements or images are
equally true or right, or that statements, claims, and expressive actions have coequal effects
and consequences. That everything is, technically, known via discourse and representation
doesn’t exempt that discourse and representation from corporeal, material, and
bioecological effects. Some discourses and images kill people; some don’t matter much.

As this article goes to press, questions about the use of Facebook metadata and
ongoing debates over the proliferation and control of what has come to be known as “fake
news” are test cases for digital citizenship and communicative ethics, with interweaving
questions about what might count as truth, how to ascertain the truth, what is real and what
is imagined; and about control, privacy, and transparency of the information archive (an
archive—packed with trivia, state and corporate secrets, personal actions and images, and
official and unofficial communications; metadata on human behaviors, wants, needs, and
actions; as well as communications of all orders—proliferating at a breathtaking rate, even
as it is being hacked and mined).

Almost all elements of conventional electoral politics and public discourse in
democratic states have been put up for grabs. Even the longstanding conduct and
procedures for running autocratic and fascist states have had to accommodate and adapt to
the capacity of social media. These include the shift from television/broadcast and print-
based campaigns to the use of social media for instant commentary and mobilization of
constituencies. New social movements and coalitions, across political and cultural spectra
and across social strata and regional location, have been enabled through social media.

As the 20" century newspaper business and broadcast media struggle to survive,
the procedural conventions of the fourth estate have been supplanted by online commentary
reliant upon pastiche, forwarded tweets and images, tautological hotlinks, and internet

 

Language and Literacy Volume 20, Issue 3, 2018 Page 189
cross-reference for validation. News cycles are continuous; information proliferation,
redundancy, and appropriating unceasing; the accumulation and analysis of metadata by
the state and the corporation omnipresent (Davies, 2009). Furthermore, the making
“public” of what were considered governments’, political parties’, and individuals’
proprietary face-to-face and online communications on putative grounds of transparency
has confused matters even further. Literally nothing goes unreported, and verification,
validation, and analytic refutation of claims are, at best, difficult without recourse to other
online representation. Signs have been cut loose from the signified—from originary
context and place—and the placement, attribution, and location of signs, signifiers, and
signified is increasingly difficult. The cognate means for countering deliberate
misinformation and untruth have become more difficult to disentangle in a fully mediatized
world.

There are, of course, longstanding criteria, standards, and conventions for the
conduct of face-to-face verbal and embodied interactions—from how we read and interpret
deictic to gesture, bodily disposition, and eye contact. These are by definition vernacular,
local, and place-based. They are language and culture-specific, and vary by spatial locality
and community, time of day, and by the age/color/gender/sexuality/kin of the interlocutors.
Nonetheless, there have been attempts—from Plato to, notably, Austin (1962) and
Habermas (1976)—to establish forms of “universal pragmatics;” that is, ethical procedures
and criteria for judging both the truth docutionary) of particular speakers and utterances,
and the interactional, intended and actual (illocutionary and perlocutionary) consequences
of utterances. These models have been forcefully criticized for their presupposition of an
idealized (male, rational, White, Eurocentric) speaker with common and equitable access
to discourse resources (Benhabib & Dallymar, 1990). Yet speech still matters, and we
proceed each day to navigate through an array of speech acts and exchanges according to
procedural norms—both dejure and defacto, stated and tacit, conscious and unconscious.
Each vernacular community proceeds under assumptions about the maintenance of “face”
in communications (Scollon & Scollon, 1981). Without shared assumptions about the
intent of speakers and the consequences of speech acts in place, even the simplest verbal
exchange between a parent and a child, or a student and teacher on the playground, is
problematic.

Over the course of several hundred years, interpretive communities have developed
criteria and procedures for adjudicating, judging, and making sense of the printed word.
These range from the (written) laws governing what can be said and written, to intellectual
property conventions, to fine-grained, unremitting debates over how to interpret and value
literature and the corpus of written laws. The point here (hardly original) is that while the
rules of exchange for speakers and interlocutors, writers, and readers are far from static—
always contested and dynamic, culture and community-specific—they are (for better and
worse) established and institutionalized via schooling and universities, courts, and
legislatures.”

 

> These historical dynamics between rule systems and the eccentricities of local practice are, ultimately, the
tension between langue et parole—between paradigm and syntagm, between system and practice, between
form and function—that has driven linguistic science and semiotics since Saussure (Wilden, 1972).

 

Language and Literacy Volume 20, Issue 3, 2018 Page 190
Three Foundational Claims

To begin to set a curriculum agenda for teaching and learning digital ethics, I
outline three key foundational claims. These set the curriculum contents for digital ethics
as a field or area for teaching and learning.

The first claim is that digital ethics must operate at two analytically distinct but
practically interwoven levels. It must engage at once with now classical questions about
ideology and with questions about social actions and relations. As we have argued, the core
concems of educators about students’ digital lives pertain to the ideational and semantic
“stuff’—the ideologies, beliefs, and values that learners must navigate online. This raises
key questions about the truth, veracity, verification, and belief, as well as the consequences
of the information represented online. A recent article by a senior editor of the Guardian
put it this way:

For 500 years after Gutenberg, the dominant form of information was the printed
page: knowledge was primarily delivered in a fixed format, one that encouraged
readers to believe in stable and settled truths. Now, we are caught in a series of
confusing battles between opposing forces: between truth and falsehood, fact and
rumour, kindness and cruelty; between the few and the many, the connected and
the alienated; between the open platform of the web as its architects envisioned it
and the gated enclosures of Facebook and other social networks; between an
informed public and a misguided mob. What is common to thes struggles—and
what makes their resolution an urgent matter—is that they all involve the
diminishing status of truth. (Viner, 2016)

At the same time, truth claims and representations are themselves social actions—
consequential assertions about what is. Thus, the simultaneous and equivalent ethical
concer is with the interactional pragmatics of life online. In response to the
aforementioned concerns of educators and the public, digital ethics must focus on the use
of online social media as a primary site for everyday social relationships with peers and
others. To speak of ethics, then, refers simultaneously to both the ideational contents—the
semantic stuff—of online representations, and the social and interactional relations of
exchange between human subjects. Hence, a first foundational claim:

1) On ideology and social relations. That digital ethics must address questions
about ideological contents—the values, beliefs, ideas, images, narratives, and
truths that one produces and accesses online—and questions about social relations
that are lived and experienced online, specifically the interactional and material
consequences of individual and collective actions.

The ideational contents (M.A.K. Halliday’s 1978 “field”) and the interactional
relational protocols and consequences (Halliday’s “tenor’) may appear analytically
distinct, but are always interwoven in practice. What we say, write, speak, signify, and how
we speak, write, gesture, sign, and to whom, are ethical actions—no matter how conscious,
unconscious or self-conscious, explicit, tacit or implicit the intentions and decisions of the
human subject may be. In educational terms, then, digital ethics by definition engages both
the “classification” of knowledge qua ideational content (whether construed as

 

Language and Literacy Volume 20, Issue 3, 2018 Page 191
disciplinary, thematic, artistic, or scientific) and the “framing” of knowledge via social
relationships and actions (Bernstein, 1990).
Accordingly, schooling needs to introduce two interwoven strands of digital ethics:

e The teaching and learning of a performative ethics that enables the evaluation
and anticipation of real and potential human and cultural, social and economic,
bodily and environmental outcomes and consequences of digital actions and
exchanges, including their real and potential participants and communities; and,

e The teaching and learning of a critical literacy that enables the weighing,
judging, and critical analysis of truth claims vis a vis their forms, genres,
themes, sources, interests, and silences.

The second claim focuses on the political economy of communications (Graham &
Luke, 2013); that is, the relationships between state regulation and control, corporate
ownership of the modes of information, and their ideological and economic effects.
Following the prototypical work of Stuart Hall (1974) on broadcast media, the field of
cultural studies has focused variously on audience positioning and responses to media texts
(“decoding”), on the actual economic ownership and control of dominant modes of
information (political economy), and how these are manifest in ideological message
systems (“encoding”). Of course, digital exchanges operate on radically different
dimensions of scope and scale, speed, and interactivity than the broadcast media studied
by Hall and colleagues. Digital tools have the revolutionary effect of altering the monologic
and linear relationships of production/consumption and encoding/decoding established
through broadcast radio, television, and cinema, leading to claims that social media enables
new community, agency, and democratisation in ways that were intrinsically more difficult
in an era of network and studio-based broadcast media (Jenkins et al., 2016).

What remains powerful and relevant from Hall’s groundbreaking work is the
acknowledgement of the ideological interests at work in the production and reception of
screen and image. Even what might appear to be idiosyncratic local assemblage is
undertaken within political, economic, and cultural constraints (conditions of production)
and mediated by disposition and affiliated ideological resources. Affordances, further, are
historical and cultural products—not intrinsic technical features. Where it takes up the
challenge of digital content, the tendency in schooling has been to focus principally on
student and teacher responses and uses of media texts (through models of viewer and reader
response) and the semantic content (through models of comprehension, literary and, to an
extent, ideology critique); yet, far less explicitly if ever, has it focused on the relationships
between ideological content, relationships of institutional control and power, and the
corporate ownership of the modes of information.

Consider this analogy. This would be very much like if we were to teach—trecalling
Canadian economist Harold Innis’ prototypical analysis of the “bias of communications”
(1951) in preindustrial mercantilism and industrial capitalism—how to read newspapers or
how to use the railroad, without raising questions about who owns the press and
transportation infrastructure; whose interests these structures of ownership and control
serve; and who benefits and who is exploited by these configurations of political economy.°

 

® This is, ironically, exactly how traditional Canadian and American social studies and history textbooks
have taught about the railroads—as a celebration of the domination of nature by monopoly capitalists. Until

 

Language and Literacy Volume 20, Issue 3, 2018 Page 192
As Innis’ (1949) discussion of the relationships between “empire and communications”
argues, all emergent communications media and transportation systems effectively
reshaped human/machine and political economic and geographic ecosystemic relations as
well.

The basis of economic rule (and plutocracy) has shifted from those of colonial trade
documented by Innis (e.g., the Dutch East India Company, Hudson’s Bay Company)—to
the owners of elements of the dominant transportation infrastructure (e.g., the railways,
steel, oil, and auto industries); to the emergence of media empires (e.g., telephone, wireless,
newspaper, and television networks); to the current situation, where the world’s economy
is dominated by digital hardware/software /information corporations (e.g., Apple,
Facebook, Google/Alphabet, Oracle, Tesla, and Samsung) and producers of military and
advanced technological hardware (e.g., Boeing, Airbus, and arms manufacturers).

Hence, a second foundational claim:

2) On the political economy of communications. /n digital culture, the political
and economic are always personal—with every personal digital action being an
interlinked part of complex and often invisible economic exchanges that, by
definition, support particular corporate and class interests and have material and
ecosystemic consequences.

The educational lesson here is simple: The media that we use are not “neutral” or
benign, but are owned, shaped, and enabled—and controlled, capitalized upon, and
managed—in their own corporate interests (Pasquale, 2015). These interests, as social
scientists, ecological scientists, and community activists are increasingly realizing, have
reshaped the transnational and domestic divisions of wealth, labor, and power, and have
broad, heretofore unexamined effects on the use and sustainability of finite planetary
resources and ecosystems (cf. Klein, 2015).

My point is that while the curriculum should entail both the study of the sources of
information and their apparent distortions and ideological biases, such study can also be
extended to understanding the relationships between knowledges and global, planetary
interests—including the corporate ownership, capitalization, and profit from dominant
modes of information. There are, furthermore, persistent questions about the complex
relationships between digital work and culture and its relationship to carbon-based
economy and resource utilisation (e.g., Bowers, 2014).

The third claim is core to the establishment of any set of ethics. As argued, for many
schools, digital policy and practice tends to be both prohibitive—in reaction to “risks”
posed by digital technologies—and silent about the reconstructive institutional uses of
digital technology. Ethics is by definition a normative field; like all education and
schooling, ethical systems and claims are predicated upon a vision of what should be—of
how human beings can and should live together. The central message of Aristotle’s
Nicomachean Ethics (1999) is that everyday judgments about right and wrong are
grounded in visions of what might count as the “good life.” Ethical judgments are the
prerequisite philosophic and practical grounds for civility and justice. Habermas (1996)

 

recently, there has been negligible reference to their impacts on Indigenous peoples and their utilization of
Chinese labor.

 

Language and Literacy Volume 20, Issue 3, 2018 Page 193
refers to this as a “counterfactual ideal” that is presupposed in each speech exchange. The
third foundational claim is:

3) On a normative model of digital culture. That ethics cannot exist as a set of
norms or procedures for everyday life in digital cultures without a shared
normative vision of the good life.

In terms of digital ethics, this means that any set of ethical injunctions taught to
youth and children by definition presupposes a vision of what should be—a lifeworld
where digital communications are used for ethical purposes, for the good. Further, this
version of the good, following Behabib (2002), must acknowledge the moral imperatives
and challenges raised by diverse communities in pluralistic democratic societies, whether
online or face-to-face. My view, then, is that any school-based approach to digital ethics
must move beyond silences, prohibitions, and negative injunctions (which, in-and-of
themselves, rarely have salience with youth) to the reconstructive project of modeling and
enacting digital citizenship, convivial social relations, and action for social justice in
education, economy, and culture. The aim is to reframe digital ethics as part of a larger
inclusive and decolonizing educational project that refuses to relegate diversity and
difference (including childhood and adolescence) to “second class moral status” (2002, p.
2)—and pursues a vision of sustainable forms of Ife for all.

What is to be Done?

We have been here before. Dewey (1907/2012) surveyed the situation wrought by
industrial technologies, new communications media, economic globalization, large-scale
migration, and geopolitical conflict:

The social change...that overshadows and controls all others is the industrial one—
the application of science resulting in the great inventions that have utilized the
forces of nature on a vast and inexpensive scale: the growth of a world-wide market
as the object of production, of vast manufacturing centers to supply this market, of
cheap and rapid means of communication. ... One can hardly believe there has been
a revolution in all history so rapid, so extensive, so complete. Through it the face
of the earth is making over, even as to its physical forms; political boundaries are
wiped out and moved about...; population is hurriedly gathered into cities from the
ends of the earth; habits of living are altered with startling abruptness and
thoroughness; the search for the truths of nature is infinitely stimulated..., and their
application to life made not only practicable but commercially necessary. Even our
moral and religious ideas and interests...are profoundly affected. That this
revolution should not affect education in some way is...inconceivable. (pp. 6-7)

In response to our current, comparable situation, education systems in the
“hypercapitalist” (Graham, 2005) economies of North America, Europe, and the Asia-
Pacific have attempted to respond to fundamental and profound changes in society,
economy, and culture. Over the past three decades, they first viewed educational
technology as a logical extension of school science and mathematics education; that is, as
a matter of scientific technology and technique. This evolved into the current emphasis on

 

Language and Literacy Volume 20, Issue 3, 2018 Page 194
finding a place for the naming of the digital in the formal curriculum, with the enumeration
of lists of digital skills and behaviours, competences, and capacities, to be taught and
learned, as a preparation for work, consumption, and citizenship in technocratic society.
More recently, it has begun moving towards a begrudging embrace of gaming cultures and
creative industries more generally, recognizing that the new pathways to employment and
technological competitiveness in the current multinational corporate economy may lie in
the exploitation and development of media and genres, including the popular cultures that
previously were deemed counter-educational. All of these are, in part, attempts to
“curricularise” the new—to domesticate it into the institution that, as noted, developed to
ensure the intergenerational transmission of orality and literacy. These are, furthermore,
predictable strategies for the incorporation and appropriation of digital culture into a now
teetering neoliberal project of social-class-stratified, free-market schooling designed to
serve (digital) transnational corporate capital.

There remains a persistent refusal by educational institutions to take on board larger
ethical challenges. Finding a strategy that can cut through this refusal has not been proven
easy. Whilst current versions of media literacy or media education are, 50 years after the
era of the mass media, just about finding disciplinary respectabilitty—as evidenced by the
growth of various handbooks, courses, and accreditation’—there are very few examples of
national or regional school systems making digital ethics central to their vision of
education.

The three foundational claims here are neither original nor that different from
earlier notions of critical self-consciousness that have been proposed by Dewey or Freire.
One productive first step is to revisit and reinvent the longstanding work in critical
literacies and media literacy (e.g., Share, 2009; Buckingham & Sefton-Green, 1994). In
other words, the new kinds of social actions, political concerns, and participatory dynamics
made possible by the internet have not erased but rather reframed and negated classical
debates around the relationship of truth to untruth, right and wrong, and what it means to
be a citizen in democratic societies. These things still count—and how they count in a
digital culture should be at the core of the curriculum.

This is a very different view of where digital cultures, capacities, and technologies
might “fit” in schooling and in the curriculum. Simply put, the great unresolved issues of
our time should be at the heart of an engaged and relevant curriculum. What better way to
educate youth about the powers and problems of digital communications than to make these
same forces and problems (and indeed their digital representations) the object of study
across the curriculum? I therefore return to the proposition that I began with: The
educational challenge raised by digital technology is not one of skill and technique or
technology, but one of participation and ethics. What might this approach look like in
everyday school curriculum and instruction?

This territory is already being explored by teachers and students in the spaces left
by what has become an increasingly narrow, test-oriented, and instrumental curriculum.
Fortunately, this work is already underway in community-based projects. Many of these
are contemporary versions of Deweyian “projects” (1907/2012)—using digital tools for
community engagement and activism (e.g., Rogers, Winters, Perry, & LaMonde, 2015;
Sanford, Rogers, & Kendrick, 2014), through the use of digital resources for
intergenerational and intercultural exchange (e.g., Poitras-Pratt, in press/2018) and through

 

*http://eprints.lse.ac.uk/57103/1/Livingstone_ media information literacy 2014 author.pdf)

 

Language and Literacy Volume 20, Issue 3, 2018 Page 195
larger scale curriculum reform that focuses on the use of digital resources in purposive,
real world, “rich tasks” for students.* In these studies, teachers and students are using
digital technologies for (1) solving and addressing local political, social, and environmental
problems; (2) mobilizing cultural resources to connect and engage with their communities
and their histories, their Elders and younger generations, their peers, and with distant
cultures that they might otherwise not have contact with; and (3) the practice of active and
engaged citizenship, participation in community projects, and social movements and
action.

As part of the mainstream curriculum, then, digital resources are being used as a
means for engaging with, debating, critiquing, and navigating many of the difficult social,
scientific, and cultural issues faced by students and communities—in the face of what are,
for many, difficult conditions of economic hardship and divisive community and
intercultural relations, in a world dominated by new corporate/governmental orders whose
formations, mechanisms, and institutions sit well beyond the reach and comprehension of
many. The current digital corporate order—this political economy of transnational
information and technology—is at risk of a re-colonisation of everyday forms of life (both
those of adulthood and childhood, work and play) without the deliberative democratic
dialogue and informed debate about what might constitute a just, ethical, and life sustaining
world. This is an overdue dialogue with teachers, children, young people, and students—
and with their parents, Elders, and communities. Digital literacies, multiliteracies, and
digital and creative arts are necessarily ethical, political, and cultural practices—not job
skills or technical capacities. They are nothing less than new basics for all in these
challenging and difficult times.

References

Anderson, B. (1992) Imagined Communities: Reflections on the origins and spread of
nationalism. London, UK: Verso.

Aristotle. (1999) Nichomachean Ethics. 2™ Ed. T. Irwin, Trans. London: Hackett.

Austin, J.A. (1962) How to Do Things with Words: The William James lectures delivered
at Harvard University in 1955. Oxford, UK: Clarendon Press.

Benhabib, S. (1992) Critique, Norm and Utopia. New York, NY: Columbia University
Press.

Benhabib, S., & Dallmayr, F. R. (Eds.) (1990). The Communicative Ethics Controversy.
New York, NY: MIT Press.

Bernstein, B. (1990) On Pedagogic Discourse. London, UK: Routledge.

Boyd, D. (2015). /t’s complicated: The social lives of networked teens. New Haven, CT:
Yale University Press.

Bowers, C. (2014). The false promises of the digital revolution: How computers
transform education, work, and international development in ways that are
ecologically unsustainable. New York, NY: Peter Lang.

Bruner, J. & Olson, D.R. (1977). Symbols and texts as the tools of intellect. Interchange
8(4), 1-15.

 

8 Queensland’s ‘New Basics’ (1999-2005) reforms introduced curriculum “rich tasks” that required that
students use digital tools to address community problems; current Finnish curriculum reforms are making
comparable efforts.

 

Language and Literacy Volume 20, Issue 3, 2018 Page 196
Buckingham, D., & Sefton-Green, J. (1994). Cultural studies goes to school: Reading
and teaching popular media. London, UK: Taylor and Francis.

Cazden, C.B. (in press/2016). Communicative competence, classroom interaction, and
educational equity. The selected works of Courtney B. Cazden. New York, NY:
Routledge.

Cole, M. (1996). Cultural psychology: A once and future discipline. Cambridge, MA:
Belknap Press of Harvard University Press.

Davies, N. (2009). Flat earth news: An award-winning reporter exposes falsehood,
distortion and propaganda in the global media. New York, NY: Vintage.
Dewey, J. (1907/2012). The school in society & the child in the curriculum. New York,

NY: Courier Press.

Dewey, J. (1916). Democracy in education. New York, NY: Macmillan.

Dewey, J. (1934). Art as experience. New York, NY: Anchor Press.

Dewey, J. (2008). The later works of John Dewey, volume 7, 1925-1953: Ethics. J.
Boydston (Ed.). Carbondale, IL: Southern Illinois University Press.

Fraser, N. (1996) Justice interruptus. New York, NY: Routledge.

Freire, P. (1970). Pedagogy of the oppressed. M. Ramos, Trans. New York, NY:
Continuum.

Graham, P. (2005) Hypercapitalism: New media, language and social perceptions of
value. New York, NY: Peter Lang.

Graham, P. & Luke, A. (2013). Critical discourse analysis and political economy of
communication: understanding the new corporate order (pp. 103-130). In Wodak,
R. (Ed.) Critical discourse analysis: Concepts, history, theory: Vol 1. London,
UK: Sage.

Habermas, J. (1979). Communications and the evolution of society. T. McCarthy, Trans.
Boston, MA: Beacon Press.

Hall, S. (1974) The television discourse: Encoding and decoding. Education and Culture
25, 8-14.

Halliday, M.A.K. (1978). Language as social semiotic. London, UK: Edward Arnold.

Havey, D. & Puccio, D. (2016). Sex, likes and social media. London, UK: Vermilion.

Innis, H.A. (1949). Empire and communications. Toronto, ON: University of Toronto
Press.

Innis, H.A. (1951). The bias of communications. Toronto, ON: University of Toronto
Press.

Jenkins, H., Shresthova, S., & Gamber-Thompson, L. (2016). By any media necessary:
The new youth activism. New York, NY: New York University Press.

Kellner, D. (1978). Ideology, marxism and advanced capitalism. Socialist Review, 42,
37-65.

Klein, N. (2015) This changes everything: Capitalism and the climate. New York, NY:
Simon and Schuster.

Livingston, 8. & Sefton-Green, J. (2016) The class. New York, NY: New York
University Press.

Luke, A. (2004) Notes on the future of critical discourse studies. Critical Discourse
Studies 1(1), 13-15.

Martin, K. (2008) Please knock before you enter: Aboriginal regulation of outsiders and
the implications for research and researchers. Brisbane, QLD: PostPressed.

 

Language and Literacy Volume 20, Issue 3, 2018 Page 197
Pasquale, F. (2015). The black box society: The secret algorithms that control money and
information. Cambridge, MA: Harvard University Press.

Picciano. A. & Spring, J. (2012) The great American education-industrial complex. New
York, NY: Routledge.

Poitras-Pratt, Y. (in press/2018) Meaningful media: A decolonizing journey for an
Indigenous community. New York, NY: Routledge.

Quan-Haase, A. (2016). Technology and society: Social networks, power, and inequality,
2"4 ed. Toronto, ON: Oxford University Press.

Rogers, T., Winters, K., Perry, M. and LaMonde, A. (2015). Youth, critical literacies,
and civic engagement: Arts, media, and literacy in the lives of adolescents. New
York, NY: Routledge. DOI: 10.4324/978 13 15780481

Sanford, K., Rogers, T., & Kendrick, M. (Eds.) (2014) Everyday youth literacies: Critical
perspectives for new times. Singapore: Springer. DOI: 10. 1007/978-98 1-445 1-03-
1

Scollon, R. & Scollon, S. (1981) Narrative, literacy and face in interethnic
communication. Norwood, NJ: Ablex.

Share, J. (2009) Media Literacy is Elementary. 2° Ed. New York, NY: Peter Lang.

Viner, Katherine (2016, June 16). How technology disrupted the truth. The Guardian.
Retrieved from https://Awww.theguardian.com/media/2016/jul/12/how-technology-
disrupted-the-truth

Wilden, A. (1972). System and structure: Essays on communications and exchange.
London, UK: Tavistock.

 

Language and Literacy Volume 20, Issue 3, 2018 Page 198
Routledge

3
5
8 Taylor & Francis Group

Public Integrity

 

ISSN: 1099-9922 (Print) 1558-0989 (Online) Journal homepage: https:/Awww.tandfonline.com/loi/mpin20

Big Data, Big Concerns: Ethics in the Digital Age

Carole L. Jurkiewicz

To cite this article: Carole L. Jurkiewicz (2018) Big Data, Big Concerns: Ethics in the Digital Age,
Public Integrity, 20:sup1, S46-S59, DOI: 10.1080/10999922.2018.1448218

To link to this article: https://doi.org/10.1080/10999922.2018.1448218

i Published online: 09 Apr 2018.

 

o
(sg Submit your article to this journal @

 

lil Article views: 586

 

View Crossmark data@

 

 

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journallnformation?journalCode=mpin20
Dili i 7 $46-S5
Public Integrity, 20 9, 2018 ; Routledge

Copyright © American Society for Public Administration
ISSN: 1099-9922 print/1558-0989 online Taylor & Francis Group
DOI: 10.1080/10999922.2018.1448218

BA) Check for updates

Big Data, Big Concerns: Ethics in the Digital Age

Carole L. Jurkiewicz

University of Colorado Colorado Springs

The collection of data on citizens through digital portals is viewed by organizations as an
opportunity to create value, leverage competitive advantage, and maximize productivity and
efficiencies in service and product delivery. As a condition of accessing digital media, individuals
implicitly agree to allow the collection of data they generate while on a site, as well as content on
the devices used to access the sites, unless steps are taken to limit such access. The growth in the
amount of data collected has increased exponentially and has vastly outpaced awareness of or
concerns about privacy; liability; ownership; property rights; and ethical issues that are emerging
as citizens become aware of the consequences of trading privacy for access. While dominant in
the professional press, the scholarly literature has only just begun to investigate ethical issues in
the age of big data. This treatise will outline the scope of the issues; emerging problems; key ethical
considerations; and areas in critical need of research and development moving forward.

Keywords: artificial intelligence, big data, cybersecurity, privacy issues, technology ethics

Change is occurring at an ever-increasing rate as the pace of life in nearly all respects speeds
forward. Schmidt and Cohen (2013), note Buckminister Fuller's assertion in the early 80s that the
Knowledge Doubling Curve, wherein prior to 1900 human knowledge doubled every century, sped
to every quarter-century by the mid-40s. Today, the rate of doubling, attributable to technological
advances, is considered to be every 18 months, and it is anticipated that within a few years the whole
of human knowledge will double every 12 hours. This rapid pace of change, while creating new
opportunities and advantages, also contributes to increasing levels of anxiety (Mah, Szabuniewicz,
& Fiocco, 2016) as well as concerns that artificial intelligence (AT), made possible through big data,
will make humans obsolete (Dowd, 2017). Ethical discourse, codes, guidelines, and standards have
been put into place, but they are reactive and lag behind the sector they’re intended to regulate,
making them effectively obsolete before being announced (Jurkiewicz & Giacalone, 2016).

With the globalization of economies and the speed with which markets can shift in response
to news from around the world, the lull of predictability that once existed has been erased, along
with the relative comfort of thinking legal and ethical systems will protect us. Growth of
the digital economy requires an individual to be attuned to 24/7 economic, political, and social
markets and take near instantaneous actions to maintain propitiousness. Shifting economic
affiliations; currencies; corruption in financial and public institutions; aggressive marketing
campaigns; and increasing uncertainties with regard to job security impose additional societal
stressors (Johnson, 2009; Schmidt & Cohen, 2013).

 

Correspondence should be sent to Carole L. Jurkiewicz, UCCS School of Public Affairs, 1420 Austin Bluffs
Parkway, Colorado Springs, CO 80918, USA. E-mail: cjurkiew @uccs.edu
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE S47

Humans generally eschew change as it disrupts the physiological drive for homeostasis
(Jurkiewicz, 2008) and induces elevated levels of cognitive dissonance, amplifying the effects
of social influence and facilitating inappropriate emotional reactions (Guazzini, Yoneki, &
Gronchi, 2015). Although it creates efficiencies, technology also imposes the need to learn
new software and applications as well as their argot, and to adjust to increasing demands on
individuals’ time. It also facilitates bullying, as individuals can remain anonymous and as
targeted aggression is becoming more acceptable as a social norm (Brynjolfsson & McAfee,
2016). In addition to the use of big data in the drive for market dominance, it has abetted an
increasing number of individuals and organizations whose job or avocation it is to engage in
hacking, creating computer viruses and worms, advancing the dark Internet, proposing alterna-
tive currencies such as Bitcoin, and advocating extreme causes. Such concerns often leave indi-
viduals feeling helpless and inconsequential, gullible, less trusting of others and institutions, and
that their unique identity is being manipulated and reduced to an algorithmic output (Giacalone
& Jurkiewicz, 2003). This precludes the need for psychological safety and belongingness, as
well as higher-order needs, which in turn creates additional stressors (Koltko-Rivera, 2006).
Such assaults require a balancing act, pitting the need for social connection and relevance
against concerns over manipulation and trickery. Citizens frequently seek legal solutions when
their feelings of safety and belongingness are violated, but these offer evanescent fixes at best,
and are not the ethical solutions needed to address the core issues.

Politics, both national and global, also increase societal pressures rooted in big data including
shifting loyalties; partisanship; political correctness; lobbying; campaign donations; govern-
ment and corporate corruption; illegal nonprofits; taking advantage of fears and uncertainties;
and continually morphing and complex campaign regulations (Grimmelikhuijsen & Snijders,
2016). As global alliances are reshaped by personalities, electorates, economics, social trends,
increasingly inventive methods of obfuscation as shields from transparency and accountability,
the power of lobbyists and political campaign contributions, along with misinformation move-
ments create additional pressures to keep apace. The resultant concerns about one’s existence
and identity further push ethical considerations to the background at a time they are most
needed (Jurkiewicz & Grossman, 2012). While the eGovernment literature discusses collecting
data electronically, it focuses upon employing technology to enhance service and processes
rather than the moral challenges posed by big data, a much broader, complex, and multisector
topic. The purpose here is to introduce the expanse of ethical issues related to big data, provide
examples of the scope of these issues, and suggest approaches to address these concerns.

BIG DATA

A term first used by NASA researchers in 1997 (Cox & Ellsworth, 1997), the definition of
big data is still evolving, but the central elements consistently refer to massive volumes of
information collected through technological means, accumulating at such a velocity that con-
tinually innovating information processing is required to keep pace. The phenomenon has
sparked a new economy and industry sector and has been the catalyst for rapidly transformative
technologies and applications. Given the vast range and amount of data, its utility has led to
systems that improve the quality of life, proprietary and government uses that many view as
secretive and manipulative, and ethical concerns that have yet to be fully articulated or addressed.
$48  JURKIEWICZ

The majority of information that people generate through the use of technology is accessible
and available, or hackable, by anyone with access. While freely offered, the majority of indi-
viduals presume some sense of privacy or ownership regarding the content they generate
(Johnson, 2009; Riglian, 2012). Recognizing this presumption and wanting to avoid conflicts
arising from individuals’ beliefs about privacy violations, social media companies and
government agencies have instituted business models which are dependent upon this shared
content, but do not openly disclose that they are continually tracking and recording not only
the content of the information sent, but the demographics; psychographics; health; geographical
location; devices; search histories; purchasing histories; and other data about the user. Whether
it is a Website; social media forum; text; application; software; or anything accessed through
technological devices, individuals, by virtue of use, are implicitly agreeing to allow those com-
panies to bundle and use or sell that data without hindrance. Everything one says or does using
technology effectively creates a currency from which others are profiting.

Programs embed decision-making with de facto ethical implications in their design, and by
so doing are predetermining options to act with regard to such products as self-driving autos;
smart homes; smart autos; GPS; missile defense systems; educational delivery; clothing; music;
and cyber security, to name a few (Bonnefon, Shariff, & Rahwan, 2016; Getha-Taylor, 2016).
Thus, coders, about whom little is known, are subcontracted by or an employee of a technologi-
cal entity to create the algorithms, often prejudicial (Chiel, 2018), that effectively restrict range
of moral choice (Jurkiewicz, 2002). What we see, have access to, or are aware of is limited by
these algorithms, and their ethical impacts are unknown until the consequences are revealed
(Etzioni, 2014), primarily through news reports such as those about self-driving autos deciding
suicide by collision is preferred over impacting another vehicle; crashing into trucks the color of
which the system cannot differentiate from a skyline; hacking of military defense systems,
medical records, utilities, and the financial infrastructure; the usurping of votes in elections;
fake advertisements feeding tendencies toward groupthink and fear aversion; and creating tech-
nological dependencies in order to exercise influence, to name a few. Additional concerns
include governments spying on citizens; profiling by criminal justice entities; ownership and
copyright exclusions; blurred lines between privacy guarantees and full disclosure (Hastreiter,
2017); inaccurate data leading to invalid algorithms; fake identities and cyber theft; and student
cheating and declining trust in higher education.

EXAMPLES OF ETHICAL CONCERNS ACROSS THE SPECTRUM

Below is a sampling of ethical issues regarding big data that organizational; social; legal; and
political toolkits do not address. They represent a spectrum of current topics that affect society,
sorted by themes to facilitate connecting examples to key ethical topics. While there are many
benefits to be extracted from big data, the focus is on augmenting the need for governments to
establish protections and recourse for citizens harmed by the culling of personal transmissions.

Data Collected Under the Guise of Social Betterment

Collecting data on individual behaviors and preferences for profit while publicly framing it
as a benefit is a key ethical concern. One example is the new iPhone X whose facial recognition
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE $49

system is touted as a security benefit while collecting this data to create saleable content (Poell,
2015; Stanley, 2017) to governments; criminal organizations; proprietary organizations; and to
expand Apple’s own product offerings. Additionally, the large genomic database to which indi-
viduals are paying to contribute, under the innocuous promise of discovering one’s ancestry, is
being sold to insurers; healthcare institutions; research companies; criminal justice entities; and
attorneys, among other entities (DNAeXplained, 2015).

Blackboard and Turnitin, two software tools widely used by many universities as incidences
of plagiarism increase (AIC, 2018; Anonymous Academic, 2017), also track all accessible
information on our computers or included in course content, student/professor communications,
and grade reports (Boyd, 2011). Selling online courses with content created by professors using
Blackboard and its affiliate programs, and selling students’ term papers submitted to Turnitin
(Blackboard, 2016; Green, 2012; Hendry, 2009; Lieberman, 2017; Neal, 2014; Roll, 2017;
Young, 2008; Zimmerman, 2008) is a growing market (e.g., U.S. SEC, 2015).

Sex robots that mimic behaviors displayed in pornography, collected through big data, are
now being produced for multiple reasons, including sexual pleasure; therapy; enacting fantasies
of rape; pedophilia; and prostitution. While advocated as tools to lesson sexual behaviors that
are generally viewed as both immoral and illegal, in practice, it is actually serving to normal-
ize them and increasing their incidence in the populace. Cyborg sex cafes, wherein one can
order a robotic sexual experience along with a beverage, are becoming more widespread,
as is the spread of sexually transmitted diseases resultant of shared equipment (Siddique,
2017). Further, successful lawsuits have been brought against sex toy manufacturers for
violating privacy rights by collecting and selling data transmitted through their usage (Baker,
2017).

Facilitating Unequal Wealth Distribution

Concerns about AI include the dehumanization of individual choice, essentially replacing
individual identity with collective, computerized model citizens and employees (Cellan-
Jones, 2014; Garling, 2014; Simonite, 2017). By 2025, roughly 25-35% of today’s jobs
will be replaced by AI tools developed from big data (Wakefield, 2015), resulting in an
expected $20 trillion profit by eliminating human resource costs. Such shifts will negatively
affect lower social economic groups disparately, as unskilled jobs will be eliminated first,
protecting those of greater complexity and further deepening the divide between economic
classes (Flynn & Robu, 2017). This is expected to lead to greater social strife, extreme
concentrations of power, and widespread government dependence (World Economic Forum,
2016).

Collection of big data voluntarily (under the auspices of making our lives easier and fitting in
with social groups), and involuntarily (by using software and applications that implicitly allow
the collection of our data from any device and for whatever purposes the aggregator decides) is
expected to increase. Deep-learning algorithms are continuing to be developed that build the
large data sets corporations such as Facebook and Google use to track all interactions with their
sites as well as where we travel; what we see; what we consume; and with whom we interact;
among other parameters. Google, among many organizations, is training robots to understand
and behave like humans by watching YouTube videos (Bagot, 2017).
$50 = JURKIEWICZ

Individuals and organizations that are able to afford advanced computer systems, collections
of big data, and people to operate and analyze it are able to conduct financial trades ahead of
those who do not have these systems, affording them much higher profits. Additionally, the
Security and Exchange Commission lacks the resources to configure systems that maintain
continual security coverage to protect us from those seeking to profit by overriding its outdated
systems, unencrypted software, and inadequate firewalls; it is a common and lucrative target for
hackers (Price, 2017).

Cashless societies, advocated by the United Nations, are supported as solutions to theft,
counterfeiting, and increasingly volatile financial valuations, allowing governments to track
unreported income and considered a concept that furthers wealth disparities and disadvan-
tages the most impoverished individuals (Sorrel, 2016). It disallows for negotiation; cash
transactions in areas of high illiteracy; street-level charity; flexibility cash economies proffer-
ing those in desperate political, social, and domestic situations; and fails to take into account
the lack of financial institutions in remote areas with unreliable electronic infrastructure
(Taylor, 2017).

Increasing Social Hostilities for Political Gain

The use of fake news bots and “alternative fact’ Websites in the 2016 U.S. presidential election
exposed approximately 25% of the U.S. population to falsehoods about then candidate Trump,
10% of whom returned often to these sites despite being made aware of the falsehoods (Guess,
Nyhan, & Reifler, 2018). Tracking these contacts, algorithms were created that fed viewers
more of the same, expanding exposure to misinformation. Spreading false information through
social media, e-mails, and phishing scams primes the population to accept misinformation as the
new normal, arguably threatening the basis of democracy (Clifford, 2017; Quick, 2016).
Although an ethically bereft sense of shared identity, it is effective toward the end of mobilizing
support for polarizing leaders and their initiatives (Breland, 2017; Haslam, Reicher, & Platow,
2010; Kaplan, 2017). Russian influence in the U.S. election of 2016 is another manifestation of
how big data can influence democracy (Graff, 2017); it is also used to predetermine which Con-
gressional bills will pass and to influence individual voters. Cambridge Analytica is credited
with creating voter profiles used to facilitate Trump’s election (Confessore & Hakimmarch,
2017; Polonski, 2017).

Social media sites allow for the targeting of specific demographics that facilitate access
by hate groups, which accumulate information on users and target segments of the populace
for discrimination. Spam e-mails and ads framed as supportive sites encourage clicks and
advancing prejudice (Chiel, 2018); viewers are enticed to express hatred toward specific groups,
and in turn are identified and supported in their hate speech through targeted advertisements and
calls to action (Byme, 2017; Collins, Poulsen, & Ackerman, 2017).

Concealed Data Collection for Profit

Ubiquitous camera systems, ostensibly for safety or convenience, generate data that are
aggregated and resold at considerable profit. One’s sexual orientation can be determined with
91% accuracy by analyzing facial features, information that has been used to discriminate,
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE $51

disadvantage, and violate individual privacy (Kosinki & Wang, 2017). These visual databases
have been used by governments for criminal capture, security-based identification, and political
and social activism (Gershgorn, 2017; Scassa, 2017), as well as lip-reading with instant
language translations, furthering concerns about invasions of privacy (Murphy, 2016).

Products such as Apple’s Siri, Amazon Echo, Microsoft’s Cortana, and Google Home
promise to streamline human activities and efficiencies, yet collect and record all sounds, even
when not in use. The data provide unprecedented access to private information, and bolster the
specificity and scope of big data collection (Cassano, 2017), as well as the value of such data in
the marketplace. Similarly, all “connected cars” automatically send data on driver behavior;
location; mechanicals; and more to feed a data bundle of which the market value is predicted
to be in the billions by 2020 (Gitlin, 2018).

Facilitating Human Dependencies on Technology

Creating big data-based applications that decrease the need for complex, logical thinking is
reducing these abilities in humans, deepening dependencies upon technologies and lessening
the ability to tackle crisis situations that applications do not address (Coopersmith, 2017;
Firestone et al., 2012). Because technologies are making more decisions on our behalf, and
because the algorithms behind them are indecipherable to us, the need for safeguards is vital
(Kuang, 2017).

Social media and applications such as Facebook, Uber, and Google once downloaded allow
access to communications, stored data, places traveled, and sites searched on one’s device, even
when the application is closed. This information can be sold; partitioned; aggregated; or used in
any way the organization chooses (Sulleyman, 2017). Facebook’s analog company, Onavo,
offers a free application promoted as keeping your data safe, yet it collects and feeds all infor-
mation to Facebook, including information on other applications a user accesses in an effort to
maximize market domination (Seetharaman & Morris, 2017; Sulleyman, 2017).

Instantaneous social communication and feedback fuels the rise in narcissism and, given
the social construction of reality, determines acceptable opinions; appearance; behaviors; social
aggression; social displays; and is being manipulated to influence decision-making (Chamorro-
Premuzic, 2014; Marshall, Lefringhausen, & Ferenczi, 2015; Pearse, 2012; Twenge, 2013).
Dependence upon social media for one’s self-esteem, coupled with other big data influencers,
is believed to be the cause of increasing depression, suicide, and social and psychological
dysfunction among U.S. youth (Atanasova, 2016; Buffardi & Campbell, 2008; McCain &
Campbell, 2017; Weiser, 2015).

Undermining Individual Integrity

The phenomenon known as the bystander effect (Latane & Darley, 1969) has been expanded
through smart technologies (Stanley, 2017). By providing an audience, they facilitate the
filming of acts of violence; deaths; discrimination; and life-threatening circumstances without
offering assistance as one is likely to do if acting alone (Badalge, 2017). These norms, spread
more rapidly through big data and have greater impact than when the theory was first asserted;
$52 JURKIEWICZ

the most negative, hate-filled, and discriminatory messages are most contagious on social media
and most influential in establishing social and ethical norms (Pressler, 2017).

Axon, a police camera company, has been collecting data recorded on its devices, parsing it,
and selling it to a variety of organizations for a wide range of purposes. It discovered that
providing free cameras to police departments not only gained it goodwill and cemented future
sales, but also created a separate and more lucrative market for selling this data to governments;
agencies; proprietary organizations; software developers; and numerous others. Such data
challenges both the legal provisions against invasion of privacy as well as introduces several
ethical concerns (Cassano, 2017).

SUMMARY

This overview provides a sense of the scope of the ethical issues introduced by big data. While
collecting data on almost everyone about everything is possible, should we? What do we know
about what is collected about us, how it is used, and how it is unwittingly influencing the
choices we make? Evidence of violations of privacy and free will are just now coming to light,
and it is highly likely that what we don’t know is more frightening and pervasive than what we
do. As the rate of change continues to increase, how can we keep current, let alone anticipate
what is to come? Thomson (2015) has ventured that by 2025, we will work with robots on a
regular basis, including some responsible for life-and-death issues, as well as those serving
on corporate boards and providing healthcare; that over 50%of Internet activity will comprise
control functions for offices, homes, and transportation; that functional human organs and
autos will be 3-D printed; and that cities will have streets without traffic signals, to name some
anticipated developments. With an expected 90% of the human population having Internet
connectivity and free, unlimited data storage, the ability for big data to exercise influence will
increase exponentially. The need to address questions of ethicality related to this phenomenon is
overdue.

NEXT STEPS

Calls for regulation of big data and protection of individual data are growing. As widely
reported, we know that self-policing by big data firms does not work (cf. Fair, 2017; Lapowsky,
2015). Industry and professional ethical codes have not worked either (cf. Bohannon, 2016;
Bynum, 2016). No laws in the United States specifically target big data (Wessing, 2014),
and those tangential to the issue have also largely failed. The reasons range from an assertion
that capitalism drives profit at any cost (Walker, 2015); political contributions wield influence
over lawmakers (Glazek, 2017); lack of ethics code enforcement (Bynum, 2016); a dearth of
legal statutes (Wessing, 2014); and the pervasiveness and power of big data’s lobbying industry
(LaPira & Thomas, 2017). Some scholars have called for principle-based ethical
models developed and monitored by professional associations (Moor, 2008; Wiener, 1964),
but without enforcement mechanisms, it is unlikely these would be effective. The European
Union instituted stringent laws regarding the collection and use of big data, and requires all
algorithms to be readily and clearly understandable to the citizenry; the laws have strong
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE $53

enforcement penalties in the billions of dollars for those who do not comply (Kuang, 2017).
This approach—and other similar statues specifically targeting big data—are needed in the
United States, as are those to prevent dissolution of their enactment and enforcement through
political influence.

AN IMMODEST PROPOSAL

The United States is behind the curve on establishing boundaries for the collection and use of
big data, and every day it falls tens of trillions of pieces of data further behind. What to do, and
where to start? Adopting laws and penalties specific to big data regulation similar to those in the
EU is a start. These laws should require organizations to seek approval of data collection and
demonstrate how it specifically fits with their business model. In addition, laws with stiff
penalties to halt the influence of lobbying, gifts, and political contributions need to be
developed immediately, with executive action in the interim. Accountability should be attached
to those at the top of organizations that violate these laws, and they should be fired and
disallowed from conducting business in any data-related capacity into the future. Current
privacy laws and those covering property and ownership of data need to be updated and
strictly enforced, with extreme penalties for violators. The laws must also encompass account-
ability for the mutability of names related to big data, as renaming big data to data analytics,
and artificial intelligence to artificial machines, for example, can render laws without such
flexibility unenforceable.

In addition to the urgent need for objective research to understand fully the dimensions of
ethicality in big data and upon which effective policies can be developed, is the need for citizens
to shed their complacency and automatic acceptance of new technologies and software.
Humans, being fundamentally social beings, will gravitate to systems that facilitate communi-
cation, and thus need to be educated about how and when their data are being sourced, with
accessible and responsive authorities to whom they can report violations. They need to be made
aware of their legal and constitutional rights, and all organizations collecting data of any type
from individuals must engage in radical transparency by fully disclosing that fact and how the
data will be used in simple and direct language. The need for similar laws in all countries would
quell the ability to outsource unethical operations.

Additionally, ethical codes need to be updated regularly and accompanied by powerful
enforcement and penalty tools to assure compliance. Ethics officers should be required in all
organizations above a set size budget, with the reporting structure for these officers being an
independent government or legal body. Given that big data is a global phenomenon, and that
cultural issues have traditionally posed impediments in crafting an ethical code that bridges
these differences (Jurkiewicz, 2012), Wiener’s (1954) classic, The Human Use of Human
Beings, the first to address the impact of technology on values such as health; happiness;
freedom; security; and quality of life, offers a framework for ethics and technology that may
provide the basis for this common foundation (adapted in Table 1), a possible rubric against
which to compare differing assertions on ethicality.

This is only a start, and it will be expensive; it is suggested that all organizations that profit
from big data be required to donate a set percentage of pretax gross profits to a fund to support
these and future efforts. Such a requirement should have no exceptions and should be
$54  JURKIEWICZ

TABLE 1
Tenets for the Development of a Global Code of Ethics for Big Data

 

Benevolence: Individuals should be respected and valued in equal proportions simply by virtue of being human.

Equality: What serves as justice between two individuals or an individual and/or entity should remain the same when
the roles of those two individuals and/or entities are reversed.

Freedom: Individuals should have the autonomy to achieve the full expression of each of the human capabilities s/he
possesses.

Independence: The satisficing required of individuals to participate in society should impinge upon their liberties only
as is essential.

 

Source: Adapted from Wiener, 1954, p. 106.

enforceable to the extent that organizations can be dissolved if they fail to comply. These asser-
tions are stringent, but nothing short of this will have any effect in reigning in the ethical viola-
tions connected with big data.

CONCLUSION

Along with the benefits of big data, such as efficiency, scientific advances, and minimizing
the distance between individuals around the globe, come an increasing number of ethical con-
cerns. Driven by the government and proprietary sectors, the growth of unregulated big data
poses a threat to social and political systems; our psychological, emotional, and physical
health; financial and educational systems; economic welfare; social civilities; and individual
identity.

In addition to the need for objective research on which effective policies can be developed
is the concomitant need for legislative action to protect individual rights and to hold
organizations and individuals legally accountable for invasions of privacy and lack of trans-
parency. While legal action is not a substitute for ethical mobilization, it can move much
more quickly in protecting societies and individuals as ethical systems are developed and
implemented.

An overarching concern related to the interwoven nature of big data across borders is the
question of who should decide the ethical and legal limits, or if they should be imposed
at all. Those with the fastest growing digital economy? This would put Singapore, the
UAE, and New Zealand in charge. The recognized technology up-and-comers? Saudi
Arabia, Latvia, China, and Portugal, to name just four. Those that are on rapid
ascent, among them Egypt, Pakistan, and Nigeria? The United States is included in the
group of countries where the digital economy has stalled, along with South Korea, the
Netherlands, and Denmark (Chaturvedi, Bhalla, & Chakravorti, 2017), so should their
voices be diminished? Who has the authority to make such pronouncements, and what
would lead others to follow?

As top technology leaders, including Elon Musk, Bill Gates (Simonite, 2017), and Stephen
Hawking (Cellan-Jones, 2014), have articulated, big data is the most serious threat we face, and
to fail to address the related ethical concerns could and likely will lead to the end of the human
tace (Bagot, 2017).

At what point will such a warning be moot?
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE S55

REFERENCES

Academic Integrity Council (AIC). (2018). A report on the 2005-2006 survey. Durham, NC: Duke University Press.
Retrieved from _ http://integrity.duke.edu/reports/Survey %202005,%20Report, %20Final %20version, %20May %
2025, %202006.doc

Anonymous Academic. (2017, October 12). Students cheat in ever more creative ways: How can academics stop them?
The Guardian. Retrieved from https://www.theguardian.com/higher-education-network/2017/oct/12/students-in-
ever-more-creative-ways-how-can-academics-stop-them

Atanasova, A. (2016, November 26). How to spot a narcissist on social media. Social Media Today. Retrieved from
http://www.socialmediatoday.com/social-networks/how-spot-narcissist-social-media

Badalge, K. N. (2017, June 12). Smartphones haven’t made us into activists: They’ ve turned us into helpless bystanders.
World Economic Forum. https://www.weforum.org/agenda/2017/06/your-phone-might-make-you-feel-like-an-
activist-but-its-preventing-you-from-really-helping-1

Bagot, M. (2017, October 26). Google is training robots to understand humans by making them binge-watch YouTube
videos. The Mirror. Retrieved from http:/Avww.mirror.co.uk/tech/google-training-robots-understand-humans- 11413196

Baker, V. (2017, March 15). Can a sex toy spy on you? BBC News. Retrieved from http:/Avww.bbc.com/Mnews/world-us-
canada-39280941

Blackboard Lifecycle Services. (2016). Washington, DC: Blackboard. Retrieved from http://Avww.blackboard.com/
sites/student-services

Bohannon, J. (2016). Who’s downloading pirated papers? Everyone. Science, 352(6285), 508-512. doi:10.1126/
science.352.6285.508.

Bonnefon, J., Shariff, A., & Rahwan, I. (2016). The social dilemma of autonomous vehicles. Science, 352(6293),
1573-1576. doi:10.1126/science.aaf2654.

Boyd, R. (2011, August 29). Blackboard: A tale of 2 companies. Seeking Alpha. Retrieved from https://seekingalpha.
con/article/290299-blackboard-a-tale-of-2-companies

Breland, A. (2017, September 9). Trump supporters dig up personal information on thousands of Trump opponents. The
Hill. Retrieved from http://thehill.com/policy/technology/35 1 825-trump-supporters-have-built-a-massive-list-of-
their-opponents-personal

Brynjolfsson, E., & McAfee, A. (2016). The second machine age: Work, progress, and prosperity in a time of brilliant
technologies. New York, NY: W. W. Norton & Co.

Buffardi, L. E., & Campbell, W. K. (2008). Narcissism and social networking Web sites. Personality and Social
Psychology Bulletin, 34(10), 1303-1314. doi:10.1177/0146167208320061.

Bynum, T. (2016, winter). Computer and Information Ethics. In E. N. Zalta (Ed.), The Stanford encyclopedia of philosophy.
Stanford, CA: Stanford University. Retrieved from https://plato.stanford.edu/archives/win20 1 6/entries/ethics-computer.

Byrne, B. P. (2017, September 16). Twitter says it fixed “bug” that let marketers target people who use the N-word. The
Daily Beast. Retrieved from http://www.thedailybeast.com/twitter-lets- you-target-millions-of-users-who-may-like-
the-n-word

Cassano, J. (2017, August 16). Police body camera company, axon, is vacuuming in data, stoking privacy concerns. [B
Times. Retrieved from http://www.ibtimes.com/political-capital/police-body-camera-company-axon-vacuuming-
data-stoking-privacy-concerns-2579 107

Cellan-Jones, R. (2014, December 2). Stephen Hawking warns artificial intelligence could end mankind. BBC News.
Retrieved from http://www.bbc.com/news/technology-30290540

Chamorro-Premuzic, T. (2014, March 13). Sharing the (self) love: The rise of the selfie and digital narcissism. The
Guardian. Retrieved from https://www.theguardian.com/media-network/media-network-blog/20 14/mar/13/selfie-
social-media-love-digital-narcassism

Chaturvedi, R., Bhalla, A., & Chakravorti, B. (2017, July 18). These are the world’s most digitally advanced countries.
World Economic Forum. Retrieved from https://www.weforum.org/agenda/2017/07/these-are-the-worlds-most-
digitally-advanced-countries

Chiel, E. (2018, January 23). The injustice of algorithms. New Republic. Retrieved from https://newrepublic.com/
article/1467 1 O/Anjustice-algorithms

Clifford, V. (2017). News brands are fighting fake news to ensure it does not become “new normal” in 2018.
Campaigniive. Retrieved from https://www.campaignlive.co.uk/article/news-brands-fighting-fake-news-ensure-
does-not-become-new-normal-2018/1452836
$56 = JURKIEWICZ

Collins, B., Poulsen, K., & Ackerman, S. (2017, September 17). Russia used Facebook events to organize anti-immigrant
rallies on U.S. soil. The Daily Beast. Retrieved from http://www. thedailybeast.com/exclusive-russia-used-facebook-
events-to-organize-anti-immigrant-rallies-on-us-soil

Confessore, N., & Hakimmarch, D. (2017, March 6). Data firm says “secret sauce” aided Trump; many scoff. New York
Times. Retrieved from https://www.nytimes.com/2017/03/06/us/politics/cambridge-analytica.htm!

Coopersmith, J. (2017, June 30). Is technology making us dumber or smarter? Yes. The Conversation. https://theconversation.
comis-technology-making-us-dumber-or-smarter-yes-58 124

Cox, M., & Ellsworth, D. (1997). Application-controlled demand paging for out-of-core visualization. Proceedings
from VIS ‘97: The Eighth International Conference on Visualization, Los Alamitos, CA. Retrieved from https://
dl.acm.org/citation.cfm ?id=267068

DNAeXplained—Genetic Genealogy. (2015, Dec. 30). 23 and Me, Ancestry and selling your DNA information.
Retrieved from https://dna-explained.com/2015/12/30/23andme-ancestry-and-selling-your-dna-information

Dowd, M. (2017, March 26). Elon Musk’s billion-dollar crusade to stop the A.I. apocalypse. Vanity Fair. Retrieved
from https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x

Etzioni, A. (2014). The new normal: Finding a balance between individual rights and the common good. New York,
NY: Routledge. doi: 10.4324/978 1315133447.

Fair, L. (2017, August 15). FTC says Uber took a wrong turn with misleading privacy security promises. Federal Trade
Commission News. Retrieved from https://Awww.ftc.gov/news-events/blogs/business-blog/2017/08/ftc-says-uber-
took-wrong-turn-misleading-privacy-security

Firestone, R. W., Firestone, L., and Catlett, J. (2012). The self under siege: A therapeutic model for differentiation. New
York, NY: Routledge.

Flynn, D., & Robu, V. (2017, August 16). Invasion? Takeover? Opportunity? What the robots mean for jobs. World
Economic Forum. Retrieved from https://www.weforum.org/agenda/2017/08/invasion-takeover-opportunity-what-
the-robots-mean-for-jobs

Garling, C. (2014, January 31). As artificial intelligence grows, so do ethical concern s. SFGate. Retrieved from http://
www.sfgate.com/technology/article/As-artificial-intelligence-grows-so-do-ethical-5 194466.php

Gershgorn, D. (2017, August 27). The age of AI surveillance is here. Quartz Media. Retrieved from https://qz.com/
1060606/the-age-of-ai-surveillance-is-here

Getha-Taylor, H. (2016). The problem with automated ethics. Public Integrity, 19(4), 299-300. doi:10.1080/
10999922.2016.1250575.

Giacalone, R. A., & Jurkiewicz, C. L. (2003). Workplace spirituality: On the need for measurement. Journal of Orga-
nizational Change Management, 16(4), 396-399. doi: 10.1108/095348 10310484154.

Gitlin, J. M. (2018, February 22). Car companies are preparing to sell driver data to the highest bidder. Ars Technica.
Retrieved from https://arstechnica.com/cars/2018/02/mo-one-has-a-clue-whats-happening-with-their-connected-
cars-data

Glazek, C. (2017, December 14). How to spend money on elections—And actually win. Fortune. Retrieved from http://
fortune.com/2017/12/14/political-campaign-donations

Graff, G. (2017, August 13). A guide to Russia’s high tech tool box for subverting US democracy. Wired. Retrieved
from https://www.wired.com/story/a- guide-to-russias-high-tech-tool-box-for-subverting-us-democracy

Green, K. C. (2012, April 2). The long (and open?) view on blackboard. Inside Higher Ed. Retrieved from https://www.
insidehighered.com/blogs/digital-tweed/long-and-open-view-blackboard

Grimmelikhuijsen, S., & Snijders, B. (2016). What happens after the storm? Investigating three conditions under which
local governments change integrity policy after scandals. Public Integrity, 18(4), 342-358. doi:10.1080/
10999922.2016.1172931.

Guazzini, A., Yoneki, E., & Gronchi, G. (2015). Cognitive dissonance and social influence effects on preference judgments:
An eye tracking based system for their automatic assessment. International Journal of Human-Computer Studies, 73,
12-18. doi:10.1016/j.ijhcs.2014.08.003.

Guess, A., Nyhan, B., & Reifler, J. (2018, January 9). Selective exposure to misinformation: Evidence from the
consumption of fake news during the 2016 U.S. presidential campaign. European Research Council. http://www.
dartmouth.edu/~nyhan/fake-news-2016.pdf

Haslam, S. A., Reicher, S. D., & Platow, M. J. (2010). The new psychology of leadership: Identity, influence and power.
London, UK: Psychology Press. doi: 10.4324/9780203833896.

Hastreiter, N. (2017, September 28). What’s the future of cybersecurity? Huffington Post. Retrieved from https://www.
huffingtonpost.com/entry/ask-the-thought-leaders-whats-the-future-of-cybersecurity_us_592d8be7e4b07d848fdc068b
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE $57

Hendry, E. (2009, August 3). Students reach settlement in Turnitin suit. Chronicle of Higher Education. Retrieved from
https://www.chronicle.com/blogs/wiredcampus/students-reach-settlement-in-turnitin-suit/7569

Johnson, D. G. (2009). Computer ethics. New York, NY: Pearson.

Jurkiewicz, C. L. (2002). The influence of pedagogical style on students’ level of ethical reasoning. Journal of Public
Affairs Education, 8(4), 263-274. doi:10.2307/402 15580.

Jurkiewicz, C. L. (2008). Class and crisis: Socioeconomic status and the ethics of individ ualexperience. Public

Manager, 38, 3. Retrieved from https://www.td.org/magazines/the-public-manager/class-and-crisis-socioeconomic-
status-and-the-ethics-of-individual-experience

Jurkiewicz, C. L. 2012.Developing a multicultural organizational code of ethics rooted in the moral obligations of
citizenry. Public Organization Review, 12(3), 243-249. doi:10.1007/s11115-012-0187-6.

Jurkiewicz, C. L., & Giacalone, R. A. (2016). Organizational determinants of ethical dysfunctionality. Journal of
Business Ethics, 136(1), 1-12. doi:10.1007/s10551-014-2344-z.

Jurkiewicz, C. L., & Grossman, D. (2012). Evil at work. In C. L. Jurkiewicz (Ed.), The foundations of organizational
evil. Armonk, NY: M.E. Sharpe. doi:10.4324/978 1315699745.

Kaplan, A. (2017, July 7). Pro-Trump media claim “shadow President” Obama is violating the Logan Act. Media
Matters. Retrieved from https://www.mediamatters.org/blog/2017/07/07/Pro-Trump-media-claim-shadow-President-
Obama-is-violating-the-Logan-Act/217176

Koltko-Rivera, M. E. (2006). Rediscovering the later version of Maslow’s hierarchy of needs: Self-transcendence and
opportunities for theory, research, and unification. Review of General Psychology, 10(4), 302-317. doi:10.1037/
1089-2680.10.4.302.

Kosinski, M., & Wang, Y. (2017, July 4). Deep neural networks are more accurate than humans at detecting sexual
orientation from facial images. Open Science Framework. Open Science Framework.

Kuang, C. (2017, November 26). Can A.L be taught to explain itself? New York Times Magazine, pp. 46-53. Retrieved
from https://www.nytimes.com/2017/1 1/2 1/magazine/can-ai-be-taught-to-explain-itself.html

LaPira, T. M., & Thomas II, H. F. (2017). Revolving door lobbying: Public service, private influences, and the unequal
representation of interests. Lawrence, KS: University Press of Kansas. doi: 10.2307/j.cttl qft06g.

Lapowsky, I. (2015). New Facebook rules show how hard it is to police 1.4B users. Wired. Retrieved from https://www.
wired.com/2015/03/facebook-guidelines

Latane, B., & Darley, J. (1969). Bystander “apathy.” American Scientist, 57, 244-268. doi: 10.2307/27828530.

Lieberman, M. (2017, November 8). It’s all in the data. Inside Higher Ed. Retrieved from https://www.
insidehighered.com/digital-learning/article/2017/1 1/08/university-system-maryland-standardizes-data-collection-
improve

Mah, L., Szabuniewicz, C., & Fiocco, A. J. (2016). Can anxiety damage the brain? Current Opinion in Psychiatry,
29(1), 56-63. doi: 10.1097/yco.0000000000000223.

Marshall, T. C., Lefringhausen, K., & Ferenczi, N. (2015). The Big Five, self-esteem, and narcissism as predictors of
the topics people write about in Facebook status updates. Personality and Individual Differences, 85, 35-40.
doi:10.1016/).paid.2015.04.039.

McCain, J., & Campbell, W. K. (2017, November 10). Narcissism and social media use: A meta-analytic review.
Psychology of Popular Media Culture. doi:10.1037/ppm0000137.

Moor, J. (2008) Why we need better ethics for emerging technologies. In J. van den Hoven & J. Weckert (Eds.), Infor-
mation technology and moral philosophy (pp. 26-39). Cambridge, UK: Cambridge University Press. doi:10.1017/
CBO9780511498725.003.

Murphy, B. J. (2016, November 25). Lip reading skills by Google’s AI is on the fleek. Serious Wonder. Retrieved from
http://www.seriouswonder.com/lip-reading-skills-google-ai-on-fleek

Neal, R. W. (2014, March 18). Google sued for data-mining: California students claim violation of educational privacy.
IB Times. Retrieved from http://www.ibtimes.com/google-sued-data-mining-california-students-claim-violation-
educational-privacy- 1562198

Pearse, D. (2012, March 17). Facebook’s “dark side”: Study finds link to socially aggressive narcissism. The Guardian.
Retrieved from  https://www.theguardian.com/technology/2012/mar/17/facebook-dark-side-study-aggressive-
narcissism

Poell, T. (2015). Social media activism and state censorship. In D. Trottier and C. Fuchs (Eds.), Social media, politics
and the state: Protests, revolutions, riots, crime and policy in the age of Facebook, Twitter and YouTube (pp. 189-206).
New York, NY: Routledge. doi:10.4324/9781315764832-18.
$58  JURKIEWICZ

Polonski, V. (2017, August 9). How artificial intelligence silently took over democracy. World Economic Forum.
Retrieved from _ https://www.weforum.org/agenda/2017/08/artificial-intelligence-can-save-democracy-unless-it-
destroys-it-first

Pressler, J. (2017, September 20). This Stanford professor has a theory on why 2017 is filled with jerks. New York
Magazine. http:/mymag.com/daily/intelli gencer/2017/09/robert-sutton-asshole-survival-guide.html

Price, M. (2017, 21 September). U.S. SEC says hackers may have traded using stolen insider information. Reuters.
Retrieved from _ https://www.reuters.com/article/legal-us-sec-intrusion/u-s-sec-says-hackers-may-have-traded-
using-stolen-insider-information-idUSKCN1BW 1K0

Quick, A. (2016, December 8). Fake news is the new normal we must defend. The Southern Illinoisan. Retrieved
from http://thesouthern.com/news/opinion/editorial/quick/quick-fake-news-is-the-new-normal-we-must-defend/
article_f161af7f-O8e8-569c-a770-2303923de076.html

Riglian, A. (2012). “Big data” collection efforts spark and information ethics debate. Tech Target. Retrieved
from http://searchcloudapplications.techtarget.com/feature/Big-data-collection-efforts-spark-an-information-
ethics-debate

Roll, N. (2017, June 19). New salvo against Turnitin. Inside Higher Ed. Retrieved from https://www.insidehighered.
com/news/2017/06/19/anti-turnitin-manifesto-calls-resistance-some-technology-digital-age

Scassa, T. (2017). Law enforcement in the age of big data and surveillance intermediaries: Transparency challenges.
SCRIPT-ed, 14(2), 239-284, doi:10.2966/scrip.140217.239.

Schmidt, E., & Cohen, J. (2013). The new digital age: Reshaping the future of people, nations and business. New York,
NY: Knopf.

Seetharaman, D., & Morris, B. (2017, August 13). Facebook’s Onavo gives social-media firm inside peek at rivals’
users. Wall Street Journal. Retrieved from https://www.wsj.com/articles/facebooks-onavo-gives-social-media-
firm-inside-peek-at-rivals-users- 1502622003

Siddique, H. (2017, July 5). Sex robots promise “revolutionary” service but also risks, says study. The Guardian.
Retrieved from  https://www.theguardian.com/technology/2017/jul/05/sex-robots-promise-revolutionary-service-
but-also-risks-says-study

Simonite, T. (2017, July 7). Two giants of AI team up to head off the robot apocalypse. Wired. Retrieved from https://
www.wired.com/story/two-giants-of-ai-team-up-to-head-off-the-robot-apocalypse

Sorrel, C. (2016, March 15). What happens when we become a cashless society. Fast Company. Retrieved from https://
www.fastcompany.com/3056736/what-happens-when-we-become-a-cashless-society

Stanley, J. (2017, Sept. 14). Apple’s use of face recognition in the new iPhone: Implications. ACLU.
Retrieved from aclu.org/blog/privacy-technology/surveillance-technologies/apples-use-face-recognition-
new-iphone

Sulleyman, A. (2017, August 14). Facebook knows what millions of people do on their phones, even if they don’t actually
use the social network. The Independent. Retrieved from http://www.independent.co.uk/life-style/gadgets-and-tech/
news/facebook-know-smartphones-activity-what-do-not-use-social-network-account-media-privacy-security-a789276 |.
html

Taylor, K. (2017, July 4). Why “cashless societies” don’t benefit the poor. World Economic Forum. Retrieved from
https://www.weforum.org/agenda/2017/07/why-cashless-societies-dont-benefit-the-poor

Thomson, S. (2015, September 15). 13 signs the fourth industrial revolution is almost here. World Economic Forum.
Retrieved from https://www.weforum.org/agenda/2015/09/13-signs-the-fourth-industrial-re volution-is-almost-here

Twenge, J. (2013, 24 September). Social media is a narcissism enabler. New York Times. Retrieved from https://www.
nytimes.com/roomfordebate/20 1 3/09/23/facebook-and-narcissism/social-media-is-a-narcissism-enabler

USS. Securities and Exchange Commission. (2015, Oct 9). Form S-1, Instructure, Inc. Retrieved from https://www.sec.
gow/Archives/edgar/data/1355754/0001 193125 15341090/d932934ds1.htm

Wakefield, J. (2015, September 14). Intelligent machines: The jobs robots will steal first. BBC News. Retrieved from
http://www.bbc.com/news/technology-33327659

Walker, R. (2015). From big data to big profits: Success with data and analytics. Oxford, UK: Oxford University Press.
doi: 10.1093/acprof:oso/9780199378326.001.0001.

Weiser, E. B. (2015). #Me: Narcissism and its facets as predictors of selfie-posting frequency. Personality and Individ-
ual Differences, 86, 477-481. doi:10.1016/j.paid.2015.07.007.

Wessing, T. (2014, July). Regulation of big data in the United States. Global Data Hub. Retrieved from https://mmited-
kingdom.taylorwessing.com/globaldatahub/article_big_data_us_regs.html

*
BIG DATA, BIG CONCERNS: ETHICS IN THE DIGITAL AGE S59

Wiener, N. (1954). The human use of human beings: Cybernetics and society. New York, NY: Doubleday Anchor.

Wiener, N. (1964). God & Golem, Inc.: A comment on certain points where cybernetics impinges on religion.
Cambridge, MA: MIT Press.

World Economic Forum. (2016, August 10). Here’s how your career can survive automation. Retrieved from https://
www.weforum.org/agenda/2016/08/how-you-can-ride-the-wave-of- workplace-change

Young, J. R. (2008). Judge rules plagiarism-detection tool falls under “fair use.” Chronicle of Higher Education, 54(30),
A13. Retrieved from https://Awww.chronicle.com/article/Judge-Rules/19218

Zimmerman, T. A. (2008). McLean students file suit against Turnitin.com: Useful tool or instrument of tyranny?

Conference on College Composition & Communication. Retrieved from http://cccc .ncte.org/cccc/committees/ip/
2007developments/mclean
Ethics and Information Technology (2005) 7:51-59
DOL 10.1007/s10676-005-458 1-4

© Springer 2005

E-democracy, E-Contestation and the Monitorial Citizen*

Jeroen van den Hoven

Abstract. It is argued that Pettit’s conception of “‘contestatory democracy” is superior to deliberative, direct
and epistemic democracy. The strong and weak points of these conceptions are discussed drawing upon the
work of a.o Bruce Bimber. It is further argued that ‘contestation’ and ‘information’ are highly relevant notions
in thinking about, just, viable and sustainable design for E-democracy.

Key words: E-democracy, deliberative democracy, internet, contestatory democracy, digital democracy,

E-voting, Philip Pettit, Bruce Bimber

Introduction

There are many optimistic views on the way the
Internet may revitalize and reinvigorate our democra-
cies.! Many internet-supported democratic experi-
ments have been undertaken in the last 10 years and the
experimentation continues. Developments in the recent
past however also provide some reason for concern. It
has been observed that there are inequities with respect
to access to information and to government services.
People who would benefit most from being involved in
the political process are likely to be excluded, and
factors that limit political engagement off-line are also
responsible for limiting political engagement on-line.
Furthermore the evidence that shows that ICT stimu-
lates the interest in public affairs and enhances the
quality of political engagement is very weak (Bimber,
1998, 2003), and finally there is the fear that the In-
ternet will at best bring like-minded people together,
who have little interest in interaction with those who
have different opinions (Sunstein, 2002).

The experiments, pilots and trials, and scenario’s
concerning E-democracy around the world are
inspired and informed by different fundamental con-
ceptions of democracy. Everyone concerned with E-
Government and Democracy in practice is likely to be
enthusiastic about the opportunities that new infor-
mation and communication technology offers, but all
bring different conceptions of Democracy to the table.

*A version of this paper was presented at Ethicomp 2002,
Lisbon.

' A short Google search session will provide ample evi-
dence for this claim: http://www.e-democracy.org provides
a good starting point, as do the links listed on the site of the
Hansard Society, http://www-.hansardsociety.org Or links
e.g. http://www.publicus.net/articles/edemresources.html

It is important to investigate and articulate the basic
conceptions underlying new forms of IT supported
democratic politics, since different conceptions of
democracy require different IT tools, have different
patterns of technological development, require differ-
ent investment and have different patterns of usage
associated with them. One may find, after having
invested a considerable amount of time, energy and
money in the development and implementation of a
particular conception of Democracy X, that no one is
not particularly happy with the digital counterpart of
X. Defenders of “representative”, ‘deliberative’,
“strong’, “participatory”, “liberal”, or “direct
democracy” all think the Internet has wonderful things
to offer, but they all want to deploy it in different ways
and the technological trajectories of their respective
on-line democracy views pull in different directions.
The disagreement and scholarly debate over rivalling
conceptions of democracy has migrated to cyberspace.

For one person Democracy is all about E-voting,
for another it is all about on-line political debate. As
John Dewey has observed long before the computer
was invented, ‘democracy’ has to be reconstructed
every day anew and we have to see which democratic
arrangements and institutions, seem appropriate,
given the circumstances. This observation is still valid
in the age of the internet and E-democracy.

Basic conceptions of democracy

According to a classical liberal view of the political
process our familiar representative democratic insti-
tutions and arrangements can be construed as an early
administrative technology, i.e. a technology of repre-
sentation and aggregation of individual preferences.
The democratic machinery registers individual
52 JEROEN VAN DEN HOVEN

preferences, expressed as votes, of a population living
in a large geographical area, and aggregates them to
form a single social preference. Political representa-
tives elected in this way have a mandate to decide and
to act in political matters on behalf of those who voted
for them. This is an efficient way of solving the problem
of political action in large populations living in large
geographical areas. Habermas (1999) compares liberal
democracy to a market: individuals reveal their pref-
erences in their votes, majorities are formed in attempts
to seize power and compromises may be negotiated.
Habermas considers the liberal conception too meagre
to serve as a normative conception of democracy, since
it is just about “deals between competing private
interests” (Habermas, 1999, p. 244).

On a radically different and communitarian con-
ception of politics, democracy is constitutive of a
meaningful form of communal life and is to be
understood as a form of collective self-discovery.
Citizens can only thrive in a closely knit group with
shared values and meaningful communication and
interaction. The liberal and the communitarian
conceptions of the citizen differ accordingly. The
liberal citizen is characterized primarily in terms of
his or her individual rights and negative liberties,
which allow him to pursue his private interests
without much interference from either government
or his or her fellow citizens. The communitarian
citizen is characterized in terms of his or her
positive liberties, such as freedom of expression,
political participation and communication, which
all serve the collective search for a common good.
Habermas thinks that the communitarian view is
“too idealistic in that it makes the democratic
process dependent on the virtues of citizens devoted
to the public weal” (ibidem).

Bruce Bimber has criticized the liberal concep-
tion and the communitarian conception of
Democracy as conceptions that could guide the
design of our E-Democracies. He refers to them
respectively as “the populist” conception and the
“community building conception” (Bimber, 1998).7
According to the populist account the influence of
individuals on politics increases as a result of the
political use of the internet. It is a tool that
empowers and engages individuals. The commu-
nity building account holds that the internet will
foster digital political community building. Bimber
however, finds no empirical evidence to support
either of these positions (Bimber, 1998, p. 2003).

? [refer to the on-line version of Bimber’s paper, which
did not retain the page numbers of the publication in Polity.
URL http://www.polsci.ucsb.edu/faculty/bimber/transfor-
mation. html.Pages numbered consequently.

He does see however another type of political
modus operandi prevailing as a result of the
availability of the Internet. He dubs it “‘accelerated
pluralism” and it refers to the fact that single issue
groups, activists, grass root movements, NGO’s
take to the Internet like fish to the water. The
internet exacerbates the proliferation of special
interest groups and moral factions in a globalizing
world of value pluralism.

Individual citizens however do not seize every
opportunity the internet offers to become actively
involved in politics, as liberalist or ‘populist’
accounts sometimes suggest, nor are they eagerly
forming thick electronic political communities, as
the communitarian would predict or require. Citi-
zens instead keep their distance and rely for
information, monitoring and action on a range of
watch dog organizations, activists, NGO’s and
critical journalists. They assume they will warn
them about developments and decisions and poli-
cies that might affect their lives. Bimber’s claim is
based on empirical evidence, but also on a core
insight: the economics of information cost.? He
argues “that at nearly every turn, the anticipated
effects of expanded communication are limited by
the willingness and capacity of humans to engage in
a complex political life’? (Bimber, 1998, p. 3) and
finds ‘‘no connection between information and
political engagement, as measured by knowledge
about politics, voting or sophistication’? (Bimber,
1998, p. 6). He also refers to historical studies that
show no connection between an increase in political
information and political engagement. To the con-
trary there seems to be an inescapable truth: ‘‘the
cognitive structure of the political individual limits
will and capacity to assimilate information system-
atically’’. Citizens’ political participation depends
on interest and motivation and their capacity to
understand, and these are limited resources.
Bimber:

“Put simply, there are just too many issues, too
many decisions, too much complexity to expect
citizens to inform themselves and seize the reins of
government without the intercession of a vast,
elaborate, human infrastructure of information
processors. (..)” (Op. cit., p. 8)

> See Anthony Downs. An Economic Theory of Democ-
racy. Harper and Row, New York, 1957. See also Ferejohn
and Kuklinski (1990). Downs famously stated: “Any con-
cept of democracy based on an electorate of equally well-
informed citizens is irrational, i.e. it presupposes that citi-
zens behave irrationally” (p. 236).
E-DEMOCRACY, E-CONTESTATION AND MONITORIAL CITIZEN 53

Bimber concurs with James Snider’s observation*
that citizens need only be potentially informed to
realize an adequate level of accountability. Officials
will be motivated to act accountably when they know
that citizens can become informed in the future and
may hold them accountable on the basis of that
information. Infomediaries are extremely important
in his respect. They are involved in setting the agenda
and drawing public attention to important issues. The
positive impact of the Internet is not that it triggers
more political involvement and participation, but
that it facilitates access to information, enhances our
strategies and technologies to find it when needed,
and may therefore increase the responsiveness of
government to the public. Again, without a dramatic
change in citizen engagement and participation in the
political process.

The Communitarian conception of E-Democracy
does not fare any better. According to Bimber there
are a number of reasons to doubt that the internet
will foster the building of thick and meaningful
political communities on line. Thick communities
thrive in conditions of familiarity and trust, stable
relationships and social pressure. All three of these
conditions are problematic on-line. The logic and the
dynamics of pluralism however are fully accommo-
dated by the on-line environment. Grass root groups
and single issue movements, activists and critical
journalists are eager to use the net and citizens will
rely on them and defer to them, driven by their
strategies of cognitive economy and reduction of
information cost.

Several normative conceptions of democracy have
been articulated as alternatives to classical liberalism
and classical communitarianism in the theory and
philosophy of democracy. First, the availability of
new information and communication technology has
encouraged thought about direct forms of political
participation. Computer and network technology
from the early days on have given rise to the dream of
politics without political representation, which is
sometimes seen as an obstacle to self rule of the
people. Much time has been spent thinking about
electronic referendums and digital plebiscites. But the
dream of direct democracy has at the same time given
rise to the concerns over “‘push button democracy.”
When democracy would be narrowed down to elec-
tronic plebiscites and referendums, voters would
become overly dependent on the way the options in

+ James H. Snider. ‘““New Media: Potential Information &
Democratic Accountability: A Case Study of Governmen-
tal Access Community Media”, Paper presented at the
Annual Meeting of the American Science Association, San
Francisco (August 29, 1996).

referendums or polls are framed and the way the
political agenda is set.> Research on the so-called
framing problem (is the glass half-full or half-empty?)
indicates that individuals are easily influenced by the
way options are described.° Furthermore the chance
to benefit from discussion and the exposure to opin-
ions of others and to update one’s beliefs accordingly
is limited in this conception. ‘“‘Un-laundered”’ and
possibly manipulated preferences of a numerical
majority could get implemented at the expense of the
reflective and informed opinions of minorities.

A second alternative to the liberal and communi-
tarian conception of democracy has received a great
deal of attention in and outside the context of new
information technology: the deliberative conception
of democracy.’ Deliberative democracy has been
advocated as the most suitable conception of
Democracy to guide design of E-democracy institu-
tions,® because it is supposed to be uniquely suited to
put the properties of the new information and com-
munication technologies to use, especially their
potential to foster dialogue, debate and discussion.
New Media and the Internet clearly offer many new
and fascinating possibilities of sharing one’s opinions
and ideas, engage in debates and exchanges of opin-
ion. The WWW, with its chat environments, forums,
discussion and e-mail lists is surely a deliberator’s
paradise. One of the major problems for deliberative
conceptions of democracy is that they work with the
same unrealistic assumptions (discussed above in
connection with Bimber’s analysis) on the motivation
of people to actively and systematically engage in
political discussion and deliberation, both on and off-
line. The average citizen is not willing to incur the
information cost and transaction cost associated with
political deliberation. Deliberative democratic
arrangements require commitment to the public
cause, involvement in political processes and com-
munication about the political agenda, which is very

> See for the importance of agenda setting. Stephen
Nicholson. Voting the Agenda, Princeton, 2005. Nicholson:
“a fully democratic system should include final agenda
control by the people. This does not mean that the people
need to be involved in every agenda decision, but only that
they have an opportunity for final control over the agenda
should they find it necessary” (p. 132).

® Tversky and Kahneman were the first to describe this
phenomemon in the context of reasoning with probabilities.

7 Work by Joshua Cohen, Amy Gutman and Dennis
Thompson, Habermas, and James Bohman.

8 “Expanding Dialogue, Extending Deliberation: The
Public Sphere, the Internet and Transnational Democ-
racy.” In J. Roberts and N. Crossley (Eds.), After Haber-
mas: Perspectives on the Public Sphere, pp. 131-155.
Blackwell, London, 2004).
54 JEROEN VAN DEN HOVEN

time consuming for ordinary citizens. The idea of a
National “Deliberation Day” on which all citizens
would get a day off to discuss the issues on the public
agenda prior to a national election, would perhaps
trigger those who are inclined to discuss political
matters anyway and would provide others, not so
disposed, with an extra holiday.

A less well-known conception of democracy is also
believed to be congenial to E-democracy. It is some-
times referred to it as an Epistemic Conception of
Democracy.!° This knowledge oriented conception
can be found in the writings of cyber-libertarians,
Internet guru’s, and computer scientists alike. It’s
main idea is that electronic networks connecting
millions of people, combined with an abundance of
freely flowing information, exhibit the features of a
giant neural network, that exhibit emergent intelli-
gence, and that as a whole computes solutions to our
social and political problems. This idea is often
latent, but widespread among cyber enthusiasts. An
epistemic conception puts democracy on a completely
different footing. Benjamin Barber observes: “Taking
their cue from the eighteenth century Frenchman
Condorcet, enthusiasts like Wriston, Toffier, Naisbett
and Bill Gates have all composed odes to the
emancipatory, democratic powers of the startling new
technologies. (...).'! Condorcet’s Jury Theorem says
that if voters are individually better than chance to
get it right on some yes or no question, then under
simple majority rule, they will collectively be infalli-
ble, provided that the group is sufficiently large. The
line of reasoning behind this observation can be
grasped more easily in a simple example. If a coin is
loaded in such a way as to make the chance that it
comes up tails just above 50%, you may not notice
the bias after three attempts, but it will certainly show
after 1000 tosses. The crucial questions here are of
course: is it reasonable to assume that citizens are
better than chance to decide on a binary issue? And
do the truth tracking properties of democracy matter
at all? Truth tracking powers seem of little help in
deciding whether to wage war against Iraq, or to have
policies that allow physician assisted suicide.
Although the arguments on both sides are supported
by — and make reference to — empirical claims, the
decisions are in the end not based exclusively on
factual claims. I have also argued elsewhere (Van den

° Bruce Ackerman and James Fishkin. Deliberation Day.
Yale University Press, New Haven, 2004.

'© David Estlund. “Beyond Fairness and Deliberation:
The Epistemic Dimension of Democratic Authority”. In
Bohman and Rehg (Eds.), Deliberative Democracy, pp.
173-204. MIT Press, Camdridge, Mass. 1997.

"| Barber. Strong Democracy, p. 272, 1998

Hoven, 1999) that there may be other problems with
the epistemic conception and the metaphor of the E-
democracy as a computational ensemble of connected
internet users, which approximates the truth under
certain conditions. Citizens are primarily moral per-
sons of equal moral worth, and they are worthy of
respect as such in the political process. They should
therefore not be construed as mere epistemic nodes in
a network, whose weights may change in order to
produce the best epistemic outcomes.

All of these alternatives (direct, deliberative and
epistemic) to liberal representative democracy and
communitarian democracies are indeed congenial to
the Information Age. The same applies to Bimber’s
alternative conception of accelerated pluralism,
which is presented as a descriptive, not as a normative
account of E-democracy. It has explanatory power
and accommodates his empirical findings, but has
little justificatory power. The alternatives exploit and
express the prominence of the properties of infor-
mation and communication technology: communi-
cation, computation, connectivity, coordination and
access to information, but as we have seen there are
problems associated with each of them. Direct
democracy does not ask enough of citizens. Nothing
more than pushing buttons and casting votes is
required and even that may be too much trouble if
the population were to be polled systematically and
continuously on political matters. Deliberative
Democracy asks too much. Citizens are not prepared
to incur substantially more information and trans-
action cost involved in deliberation, than they do
now. Finally, epistemic democracy is beyond the
language of requirements. Whatever works in terms
of knowledge production is justified.

I will now sketch another conception of Democ-
racy, which in my opinion does not have the problems
of the two classical positions, the liberal and the
communitarian, and does not have the problems of the
three alternatives to them that we have discussed
above. I think moreover that this proposal has some of
the virtues that citizens would expect a conception of
democracy to have in the age of the Internet. In order
to sketch out this new conception I turn to Philip
Pettit’s idea of Contestatory Democracy and the
strand of Republicanism from which it originates. !*

Republicanism and Contestatory Democracy
“Democracy” is traditionally and semantically tied to
the idea of self-government by the people. Self-rule by

means of collective decision-making ideally results in

” Here I closely follow Pettit’s account of republican
freedom and contestatory democracy. See Pettit (1999).
E-DEMOCRACY, E-CONTESTATION AND MONITORIAL CITIZEN 55

a consensus with respect to what should be done.”?

Because of this emphasis on collective will formation
of the people, or aggregation of individual prefer-
ences, the idea of reasonable objection, protest, in-
stitutionalised opposition has receded into the
background. Philip Pettit draws attention to a third
alternative to liberalist and communitarian concep-
tions of Democratic freedom, which reinstates this
democratic idea of contestation. He defends it in the
context of a modern version of Republicanism, which
criticizes liberalism for its a-social account of liberty
and criticizes communitarianism for its conception of
freedom as self-mastery to be attained through
immersion in a pre-political community.

The classical liberal conception of democracy puts
negative freedom centre stage as does the communi-
tarian democracy with positive freedom. Pettit claims
that the distinction between negative versus positive
freedom amounts to a false dilemma and obscures a
third notion of freedom. Next to both liberal negative
freedom, defined as absence of interference and
coercion, and communitarian positive freedom con-
strued as self-mastery and self-discovery, exemplified
in rights of self-expression, political engagement,
there is a third type of freedom which should be put
centre-stage in our thinking about democracy. It
focuses on avoiding the domination by those who
wield arbitrary power over us. Whereas liberalism
concentrates on avoiding restraint and curtailment of
individual liberty (also by laws and regulations), a
republican conception of freedom would be com-
patible with state imposed laws, since laws are not
only state interferences and constraints on individual
freedom, they also provide opportunities for action
and freedom.

Republican freedom is not to be identified with the
“freedom of the ancients” (self-mastery) nor with the
freedom of the moderns (absence of interference by
others),'* but with the absence of mastery by others;
i.e. freedom as non-domination. Non-domination is
the condition of not being exposed to the arbitrary
power of others. There may be situations in which
one enjoys negative freedom, i.e., no one actually
interferes with one’s actions and plans, although
there are people more powerful who could have
interfered, if they would have chosen to do so. In this
case one would be free according to the negative
liberty view, but not be free in another sense, namely

'3 Ideally; see example for dealing with moral disagree-
ment in a democracy. “Moral Disagreement in a Democ-
racy’. In Frenkel Paul, Paul and Miller (Eds.),
Contemporary Political and Social Philosophy, p. 87. Cam-
bridge University Press, Cambridge, 1995.

4 These terms were introduced by Benjamin Constant.

a sense of freedom which highlights the absence of
domination. On the other hand there could be cases
where there are definite constraints on people’s
actions, such as laws. The existence of laws however
would not amount to a loss of freedom, since these
constraints are not arbitrary exercises of power but
track people’s common perceived interests. On the
proposed Republican view of freedom as non-domi-
nation we could be said to be free, since the rule of
law does not count as the exercise of arbitrary power
of others.

The Public Good should not be understood along
liberal lines as a mere aggregation of individual
interest and preferences, neither along communitar-
ian lines as participation in a communally enacted
comprehensive conception of the good, but rather as
a common interest in shared liberty, absence of
domination by arbitrary powers, and an environment
which is conducive to opposition against such dom-
ination.

It is interesting to note that Pettit describes his
Republicanism as an urban infrastructure that needs
to be designed and maintained, and which requires
monitoring and constant vigilance, very much like
gas and water supply, since democracy can only be
“stabilized via arrangements that are no more intel-
lectually beguiling than the infrastructure of gas and
water supply.”!> Pettit is of course right that demo-
cratic regimes and arrangements need to be designed
and maintained, but as his metaphor already sug-
gests, in the age of information technology we will
also have to think about the technology that imple-
ments and supports (E-)democratic arrangements
and practices. As Dewey pointed out it needs to be
serviced regularly.

Contestatory democracy

Government is more freedom friendly, Pettit claims,
when and to the extent it increases non-arbitrariness
of legislation, adjudication and administration. For
government to realize this type of democratisation it
has to have an electoral element. This electoral ele-
ment however is not sufficient to establish a demo-
cratic regime where freedom as non-domination is to
be realized. According to Pettit some simply think as
follows:

1. People will not be dominated by a government to
the extent that it is under their control

'S Pettit. Republicanism. A Theory of Freedom and Goy-
ernment. Oxford University Press, Oxford, p. 239.
56 JEROEN VAN DEN HOVEN

2. Under elective democratisation, direct or repre-
sentative, the people effectively control the gov-
ernment: they govern themselves.

Conclusion: in such a democracy it cannot be that
people are significantly dominated by government.

The conclusion does not follow, Pettit argues,
because there is an equivocation in the use of the
word “people”. People in the first premise is inter-
preted “‘severally”, while “the people” in the second
premise is interpreted collectively. It is quite well
possible that the collective people dominate the peo-
ple understood severally. The expression “tyranny of
the majority’’ describes this situation: the collective
may have the power to treat certain individuals in a
way that does not track their perceived interests.'°
These perceived interests are of course only the
politically avowable ones, those that are consistent
with the desire to live under a scheme that treats no
one as special. (Pettit, 1999, p. 176). Electoral
democracy may lead to “‘elective despotism’’ or
“elective dictatorship”. The best way in which we
could ensure that the people in the distributive sense
(i.e. individually) have control over government is
when they would have a veto right. This would of
course be practically infeasible. In such an arrange-
ment there is no place for political compromise
(Pettit, 1999, p. 179) and collective decision making
would grind to a halt. The central question therefore
is:

“Ts there any procedure that would empower the
avowable perceived interests of individual members
of the community, even those in relevant minori-
ties, but not to the same self-defeating extreme as
the vetoing proposal?” (Pettit, 1999, p. 179).

Pettit proposes what he calls “a contestatory regime’,
Le. a political regime which gives individuals the
means to call into question public decisions on the
basis of their avowable perceived interests, and to
trigger appeal in a forum that they and others can all
endorse as an impartial court of appeal, where all
relevant interests can be taken into account and only
impartially supported decisions are upheld (Pettit,
1999, p. 179/180). Contestation in the context of such
a regime brings to the fore the fact that certain
avowable perceived interests are not taken into
account, and that if they would have been taken into
account the public decision would have been differ-
ent. Contestation in this sense exemplifies a kind of

'6 This is a problem for proceduralist conceptions of
democracy such as majoritarinism. See Gutman and
Thompson’s discussion of its shortcomings (see note 8
above).

defeasibility of public decision in the light of legiti-
mate interests.

This contestatory conception of democracy not
only sees the people as the author of collective deci-
sions (this is the electoral aspect), but also, and per-
haps even more importantly, as potential editors and
censors of collective decisions. The electoral mode of
democracy gives authorship to the people collectively,
the contestatory mode of democratisation gives edi-
torship and censorship to people individually.

A contestatory democratic process is such that
surviving the process reinforces the reasons that an
ordinary individual has for believing that a decision is
consistent with taking people’s avowable perceived
interests equally into account. Public decisions are
warranted to the extent that they are capable of
withstanding individual contestation in forums and
procedures acceptable to all. Contestation is just as
important as deliberation, participation or represen-
tation, if not more important.

The contestatory conception of democracy unifies
many aspects that we associate with modern democ-
racies, such as that political control should be limited
in scope, that it should be subject to an independent
rule of law, subject to constitutional procedures,
exercised on the basis of publicly accessible political
deliberations, implemented in a political systems
where there is room for political opposition. Public
decisions are acceptable to the extent that collective
decisions are capable of withstanding individual
contestation in forums and procedures acceptable to
all.

Contestatory democracy in its middle stage — the
stage between the constitutional and the legal phase —
which according to Gutman and Thompson is the
everyday and real life of democracy” (they refer to it
as ‘““Middle Democracy”), or democracy in action,
does seems to presuppose a fair amount of delibera-
tion and moral argumentation, which both the liberal
and the communitarian conception cannot provide,
since they draw upon respectively non-discursive
voting and numerical aggregation or non-discursive
communal consensus building. If democratic actions
in day to day politics, would be insufficiently rich in
documentation, argumentation and deliberation
there would be little to contest, and it seems that
contestation would collapse into mere protest.

Contestation, information and the monitorial citizen

Information is of crucial importance for contestation.
One needs to know which issues are on the agenda,

' Gutman and Thompson, op. cit., p. 104.
E-DEMOCRACY, E-CONTESTATION AND MONITORIAL CITIZEN 57

which decisions are made and which policies are
proposed, how they affect one’s interests, what others
think and value, and which routes are open to contest
them. No information, no contestation. Sweden has
one of the oldest freedom of information laws and the
rationale for it in the 18" century was that demo-
cratic politics would be devoid of meaning if the cit-
izens would not be able to have the same information
as politicians and administrators and could not have
access to the information that may somehow bear
upon the issues of interest. Elsewhere I have argued
that government information should be subject to the
most generous regime of public access,!® since access
to information can be construed as a Rawlsian social
primary good and is internally related to the notion
of a rational and planning person (Van den Hoven
and Rooksby, 2006).!?

This emphasis on access to information could lead
one to assume that the most appropriate conception
of the citizen within Contestatory Democracy is that
of the Well-Informed citizen, i.e. an individual who
has knowledge of everything that is relevant to his
well being and happiness. Contestatory Democracy
does not necessarily presuppose a conception of the
Citizen as well-informed in a maximal sense. Con-
trary to what Pettit suggests, the contesting citizen is
not like an “editor” or a proof reader of collective
decisions. The contesting citizen is more like a vigi-
lant reader of headlines; he is a “monitoring citizen’.

Schudson (1998) has introduced the idea of the
Monitorial Citizen to capture the citizen’s most ade-
quate information strategy in political information
processing in contemporary democracies. He com-
pares the Monitorial Citizen with parents in the pool
who keep an eye on their kids, while carrying on with
other activities. Monitorial citizens scan rather than
read their informational environment. Citizens use, as
Bimber (concurring with Downs) also emphasized,
cognitive strategies to minimize their information
cost and the cost associated with cognitive process-
ing. Often it is just good enough to scan the head-
lines. Monitorial citizens are defensive rather that
pro-active. Schudson (1998, p. 310) says: “*...the
obligation of citizens to know enough to participate
intelligently in government affairs (must) be under-
stood as a monitorial obligation. Citizens can be
monitorial rather than informed”.

'8 As Clive Walker states “...virtual democracy should
involve access to governmental information on a far
grander scale than ever before allowed or indeed conceiv-
able (op. cit., p. 151)

'° First argued at Ethicomp 1995, see
www.ccsr.cse.dmu.ac.uk/conferences/ethicomp/ethi-
comp95/abstracts/vandenHoven.html

http://

Zaller in his discussion of Schudson’s idea (Zaller,
2003) also cites Downs on the economics of infor-
mation in Democracy. Voters are very economical
with the time they spend on gathering political
information, since it is very costly to gather, compare,
analyse and digest information on a vast number of
political issues. Zaller draws upon this idea to justify a
new standard for the quality of news and journalism.
Journalists should not work with the naive image of
“the well informed citizen”, but with a conception of
the citizen who is always looking for ways to cut
information cost. The Monitorial Citizen, Zaller
argues, is therefore more interested in “fire alarms’’
and “burglar alarms” than in reading up on all
detailed background reports. If the headlines give him
or her reason to study issues more closely, it should be
possible and easy to do so. This layered-structure of
information content, which accommodates the state
of alarm the citizen is in, is what hyperlinked media
can provide and what computers can deliver to the
home of every citizen.

E-contestation

Pettit’s conception of contestatory democracy is
congenial to ICT’s potential for freedom of expres-
sion, voice, coordination, communication and infor-
mation and suggests the crucial importance to
E-Democracy of what I would like to call “E-con-
testation’”’. In addition it has one enormous advan-
tage over other conceptions of democracy: It does not
need to make unrealistic assumptions concerning
political motivation and commitment of individual
citizens. Deliberative democracy seems admirable,
but does not accommodate a declining enthusiasm to
take part in more and more political chat sessions,
political forum discussions, more and more elaborate
and detailed discussions of public policy issues, and
clicking your way through a labyrinth of websites
with political information.

A good example of E-contestation is provided by a
case study from The Netherlands.”° In February 2001
40,000 e-mail messages were sent to parliament which
was preparing for the auction of radio frequencies.
Via websites, which discussed advantages and disad-
vantages of the auction, people were mobilized to
protest against the auction. Although parliament first
wanted to accept the proposal, under pressure of the
E-protest, it retracted. Two times in the last two years
the E-protest against auctioning of frequencies was

0 Jasper Ragetlie. Digitale democratie: stopdeveiling.nl.
In Mark Bovens et al. (Eds.), Renovatie van de Rechtstaat,
Den Haag, Boom Juridische Uitgevers, 2002.
58 JEROEN VAN DEN HOVEN

successful. Without the Internet this would not have
been possible.

There is also a critical interpretation — and a cau-
tionary tale — of this case which shows that there are
also weak spots. The commercial radio stations —
some of them were owned by Rupert Murdoch — were
not very interested in starting all over again — from
what was termed the zero base — and admit new radio
stations in the air that could compete with them. The
DJ’s used the old communication technology radio
(broad casting) quite effectively to draw attention to
the website (narrow casting) which offered options to
send e-mails to parliament.?! In the USA radio sta-
tions that engage in political campaigning lose their
license. It is clear from this case study that E-protest
or E-contestation comes natural to people.” And
even little sparks of interest can kindle huge fires of
protest against potentially oppressive, tyrannical or
dominating political plans. In Estland the TOM
project Today I Decide, provides another interesting
example: Citizens can propose laws and comment
and criticize existing political proposals.

We should think about the design of rules, pro-
cedures and architectures, to avail ourselves of the
promising aspects of the E-contestation, without
getting the less interesting aspects, such as populism,
collective brainwashing, and propaganda. The Inter-
net provides citizens with resources to monitor and
contest. More and more often websites are used to
present relevant performance indicators of services
and organisations which play a key role in citizen’s
lives, such as schools, hospitals, government agencies,
employers, and companies. Citizens may not want to
access these electronic resources all the time, but must
be able to access them if they want to.

The internet is very frequently used to provide the
informational basis for contestation. The following
taxonomy is provided by Albert Meijer:7° (1) public
agencies reporting on themselves. E.g. to provide
information on large infrastructural projects, (2)
public organisation to report on others about best
value and about quality of service in municipalities,
(3) public organisations as a group to report collec-
tively on their performance, to formulate targets,
bench marks and performance indicators, (4) media,

71 See J. Ragetlie. In M. Bovens et al. (Eds.), Renovatie
van de Rechtstaat, Den Haag, 2002

» “Tens of thousands of angry citizens from all over the
world have “flooded”’ the White House with e-mails, as a
part of a Friends of the Earth protest over Bush’s climb-
down on the UN climate treaty, called the Kyoto Protocol.
See http://www.foeeurope.org/press/05.04.01.htm

23 Albert Meijer. Vreemde ogen dwingen, Den Haag,
SDU uitgevers, 2004

e.g. a Dutch newspapers reports on dangerous traffic
junctions and accidents, waiting lists in hospitals,
crime statistics, quality reports of high schools, (5)
social organization, which use electronic score cards
concerning toxic waste, environmental problems, and
pollution hazards in neighbourhoods,”* (6) private
parties, “school wise”, “Up my street”, ‘‘transpar-
ency international”, “‘statewatch,” (7) Research
Centers who monitor socio economic developments,
e.g. price of daily living in particular regions, EU
working group on Legionella infections in hotels in
Europe, (8) National Accountancy Bureau’s.?°

Conclusion

Pettit’s contestatory democracy provides an inter-
esting alternative conception of democracy to guide
our thinking about designing E-democracy. It high-
lights the importance of contestation and provides us
with a normative model of democracy which does
not have to make unrealistic assumptions about the
information and transaction cost citizens are pre-
pared to incur. I have argued that the conception of
the Monitorial Citizen (Schudson) and a conception
of the press, media and infomediaries that make
realistic assumptions about citizen’s information
behaviour (Zaller), fit well with contestatory
E-democracy and growing practices and institutions
of E-contestation.

References

B. Bimber. The Internet and Political Transformation:
Populism, Community and Accelerated Pluralism. Polity,
XXXI(1): 133-160, 1998.

B. Bimber, Information and American Democracy. Cam-
bridge University Press, Cambridge, 2003.

J. Ferejohn and J.H. Kuklinski, Information and Demo-
cratic Process. University of Illinois Press, Urbana and
Chicago, 1990.

J. Habermas. Three Normative Models of Democracy, In:
The Inclusion of the Other, pp. 239-253. The MIT Press,
Cambridge, Mass, 1999.

P. Philip. Republican Freedom and Contestatory Democ-
racy. In Ian Shapiro and Casiano Hacker-Cordon,
editors, Democracy’s Value, pp. 163-191. Cambridge
University Press, Cambridge, 1999.

*4 E.g. The Toxic Release Inventory of the US Envi-
ronmental Protection Agency.

5 Access to this type of meta-information, presented in a
user-friendly way, qualifies as a Rawlsian social primary
good (with some qualifications, see Van den Hoven and
Rooksby (2006)).
E-DEMOCRACY, E-CONTESTATION AND MONITORIAL CITIZEN 59

M. Schudson, The Good Citizen. A History of American Civil
Life. Harvard University Press, Cambridge, Mass, 1998.

C. Sunstein, Republic.com. Princeton University Press,
Princeton, 2002.

J. Van den Hoven. E-Democracy. Etica & Politica, no.1, 1999.
http://www.univ.trieste.it/~etica/1999_2/jeroen.html.

Van den Hoven and Rooksby. Access to Information: A
Broadly Rawlsian Approach. In Van den Hoven and
Weckert, Editors, Moral Philosophy and Information Tech-
nology. Cambridge University Press (Forthcoming), 2006.

C. Walker. Cyber-constitutionalism and Digital Democ-
racy. In Akdeniz, Walker and Wall, editors, The Internet,
Law and Society, Longman, London, 2000.

J. Zaller. A New Standard of News Quality: Burglar
Alarms for the Monitorial Citizen. Political Communi-
cation, 20 109-130, 2003.

J. Zaller. Perversities of the Ideal of the Informed Citizenry.
Paper presented at conference on The Transformation of
Civic Life, Middle Tennessee State University, Nashville,
Tennessee, November 12-13, 1999.
(IJACSA) International Journal of Advanced Computer Science and Applications,

Vol. 2, No. 8, 2011

e-Government Ethics : a Synergy of Computer Ethics,
Information Ethics, and Cyber Ethics

Arief Ramadhan, Dana Indra Sensuse, Aniati Murni Arymurthy
Faculty of Computer Science
University of Indonesia
Depok, Indonesia

Abstract—Ethics has become an important part in the interaction
among humans being. This paper specifically discusses applied
ethics as one type of ethics. There are three applied ethics that
will be reviewed in this paper, i.e. computer ethics, information
ethics, and cyber ethics. There are two aspects of the three
applied ethics that were reviewed, i.e. their definition and the
issues associated with them. The reviewing results of the three
applied ethics are then used for defining e-Government ethics
and formulating the issues of e-Government ethics. The e-
Government ethics position, based on the previous three applied
ethics, is also described in this paper. Computer ethics,
information ethics and cyber ethics are considered as_ the
foundations of e-Government ethics and several others applied
ethics could enrich the e-Government ethics.

Keywords- e-Government; Ethics; Applied Ethics;
Ethics; Information Ethics; Cyber Ethics

Computer

L INTRODUCTION

Basically, the ethics regulates human behavior in doing
something, whether someone doing the right thing or wrong
thing. In determining whether someone doing is true or not,
ethic is more concerned to the acceptability by his social
environment. In this sense, ethics are social centric. An
individual can not properly claim that his action is right
ethically, unless their social environment consider it correct.
This is consistent with what is stated in the [3], that ethic is
relationship conduct pattern based on respect own rights and
others against their environment.

Ethics is closely related to philosophy, so that several
definitions of ethics would involve the word philosophy in it.
As stated in [4], ethics is a branch of philosophy that is
concerned with human conduct, more specifically the behavior
of individuals in society. Other definition in [5] says that ethics
is a branch of philosophy that deals with what is considered to
be right and wrong. In [6], it is described that Ethics is a branch
of philosophy that studies morals and values. In addition,
another definition states that the field of ethics (or moral
philosophy) involves  systematizing, defending, and
recommending concepts of right and wrong behavior [7].

There are two aspects in the definition of ethics: being able
to determine what is right or wrong, good or bad and a
commitment to doing what is right and good [4]. Ethics
examines the rational justification for our moral judgments; it

studies what is morally right or wrong, just or unjust [4]. Ethics
are a subset of values: a value applies to things that are desired
as well as what one ought to do, and can include such concepts
as wealth, happiness, success, and fulfillment [4].

If we examine some various explanations above, it appears
that ethics is closely related to morality. However, ethics can be
not the same as morality. As hinted in [8], morality will be
understood as the set of norms that guide our factual behavior
whereas ethics is seen to be the theory and reflection of
morality.

As stated in [7], philosophers today usually divide ethical
theories into three general subject areas: metaethics, normative
ethics, and applied ethics. Metaethics investigates where our
ethical principles come from, and what they mean [7].
Metaethical answers to these questions focus on the issues of
universal truths, the will of God, the role of reason in ethical
judgments, and the meaning of ethical terms themselves [7].
When compared to normative ethics and applied ethics, the
field of metaethics is the least precisely defined area of moral
philosophy [7]. We may define metaethics as the study of the
origin and meaning of ethical concepts [7].

Unlike the metaethics, normative ethics takes on a more
practical task, which is to arrive at moral standards that
regulate right and wrong conduct [7]. This may involve
articulating the good habits that we should acquire, the duties
that we should follow, or the consequences of our behavior on
others [7]. In a sense, it is a search for an ideal litmus test of
proper behavior [7]. The Golden Rule is a classic example of a
normative principle: We should do to others what we would
want others to do to us [7].

The key assumption in normative ethics is that there is only
one ultimate criterion of moral conduct, whether it is a single
tule or a set of principles [7]. Three strategies that are
associated with normative ethics are also revealed in the [7],
i.e. virtue theories, duty theories, and consequentialist theories.

Applied ethics can be classified into several types. This
division is generally adapted to the needs of the social
environment. Fieser in the [7], states that Applied ethics is the
branch of ethics which consists of the analysis of specific,
controversial moral issues. Other statements is revealed by
Kaddu in [4], that ethics leads to a set of rules of conduct for
specific situations; basic ethical principles guide the

82|

www.ijacsa.thesai.org
(IJACSA) International Journal of Advanced Computer Science and Applications,

development of standards for specific professions and groups.
What was put forward by Kaddu is highly relevant to applied
ethics.

This paper will attempt to define what the meaning of e-
Government ethics. The definition will be associated with the
applied ethics. There are three applied ethics, in the world of
computers science, which will be used as a reference, Le.
computer ethics, information ethics, and cyber ethics.
Therefore, this paper will also try to discuss these three applied
ethics from several perspectives. Some issues related to the
three applied ethics will also be identified.

In addition this paper will also describe the position of the
e-Government ethics against computer ethics, information
ethics and cyber ethics. A diagram of this concept will be used
to describe the position of the e-Government ethics.

II. A REVIEW ON COMPUTER ETHICS DEFINITION AND
ISSUES

At this time, there are several emerging applied ethics, such
as environment ethics, media ethics, etc. Several applied ethics
that related to computer science world is computer ethics,
information ethics and cyber ethics.

As the name implies, computer ethics is closely related to
the use of computers by humans. We suggest that there are two
things in the computer ethics that can be observed, i.e. whether
the computer is used to do the right thing or the computers are
used correctly.

In [9], it is revealed that computer ethics is not simply
ethics rotely applied to computing. Computer ethics has two
parts: (i) the analysis of the nature and social impact of
computer technology and (ii) the corresponding formulation
and justification of policies for the ethical use of such
technology [9].

Computer ethics is a standard for computer use, signifying
the prevention of copyright infringement, such as_ the
reproduction of software, invasion of privacy, and circulation
of objectionable material [10]. Computer ethics is made to
research about security and its beneficial aspects [11].
Computer ethics is also used to refer to professional ethics for
computer professionals such as ethical codes of conduct that
can be used as guidelines for an ethical case [12]. Lee and
Chan, in [13], suggest that the work of computer ethics is not to
create a new system of ethics but rather to apply traditional
ethics and to extend them to cover situations that are attributed
to computers.

Other interesting opinions related to the computer ethics
can be seen in the [14]. The paper has presented computer
ethics as neither a list of ethical principles to obey, nor a
technology deprived of certain values while implementing
those principles. Thus, computer ethics urges scholars to revisit
computer technology and its values [14]. Even though
computer ethics is a field related to and in between science and
ethics, it is a unique and holistic discipline providing principles
for understanding, conceptualization and computer technology
use [14].

Vol. 2, No. 8, 2011

Brey in [15] suggests that there exist two parts of computer
ethics, i.e. mainstream computer ethics and disclosive computer
ethics. We consider, in accordance with our focus, mainstream
computer ethics is relevant to our discussion.

In mainstream computer ethics, a typical study begins by
identifying a morally controversial practice, like software theft,
hacking, electronic monitoring, or internet pornography [15].
Next, the practice is described and analyzed in descriptive
terms, and finally, moral principles and judgments are applied
to it and moral deliberation takes place, resulting in a moral
evaluation, and optionally, a set of policy recommendations
[15]. There are three features of mainstream computer ethics
proposed in [15], i.e. (1) Mainstream computer ethics focuses
on existing moral controversies; (2) Its focus is on practices,
i.e. the individual or collective behavior of persons, and it aims
to evaluate and devise policies for these practices; (3) Its focus
usually is on the use of computer technology, as opposed to,
e.g., its design or advertisement.

Moor suggests that what is special about computer ethics is
that it has a continually large number of evolving situations
which are difficult to conceptualize clearly and for which it is
hard to find justified ethical policies [9]. In summary, what is
unique about computer ethics is computing technology itself,
and what makes computer ethics different as a field of ethics is
their scope, depth, and novelty of ethical situations for which
conceptual revisions and policy adjustments are required [9].

From some of the above explanations, it can be concluded
that there are several issues related to computer ethics, ie.
reproduction of software, invasion of privacy, circulation of
objectionable material, and security. Several other issues are
software theft, hacking, electronic monitoring, and internet
pornography. Some of these issues can also appear in
information ethics or cyber ethics.

Ill. A REVIEW ON INFORMATION ETHICS DEFINITION AND
ISSUES

In simple terms, information ethics can be interpreted as
ethics in the using, accessing and disseminating the
information. In this case, the information is used for the right
things, the information accessed in the right way, and the
information is delivered correctly to the hand who have the
rights.

Information ethics has been developed since the 1980s,
encompassing areas such as computer ethics and global
information ethics [10]. Capurro and Britz, in [16], stated that
information ethics is not only about norms, but also about our
critical reflection on the visions and options for better lives in
the digital age.

Information ethics is an open space of reflection where
commonalities and differences, theoretical as well as practical,
can be discussed without the immediate pressure of decision
making [16]. Information ethics is the new ecological ethics for
the information environment [17]. Information ethics is
essentially concerned with the question of who should have
access to what information [18]. Information ethics relates to
questions of ethics in terms of information or an information-
oriented society [10]. This includes the standard for judging

83 |

www.ijacsa.thesai.org
(IJACSA) International Journal of Advanced Computer Science and Applications,

behavior of an individual or a member of community and
classifying these as moral or immoral [10].

From some of the above explanations, it can be seen that
information ethics is closely related to environmental and
social. As revealed in [19], that information ethics is one aspect
of a much larger philosophy known as social ethics.

It is revealed in [17], that we have to fight any kind of
destruction, corruption, pollution, depletion (marked reduction
in quantity, content, quality, or value) or unjustified closure of
the infosphere, what shall be referred to here as information
entropy. The ethical use of ICT and the sustainable
development of an equitable information society need a safe
and public infosphere for all, where communication and
collaboration can flourish, coherently with the application of
civil rights, legal requirements and the fundamental freedoms
in the media [17].

Information ethics in the future should be a discipline that
carries out four functions, ic. : (1) information ethics is
prescriptive ethics; (2) information ethics is preventive ethics;
(3) information ethics is transformative ethics; and (4)
information ethics must be universally global ethics, not one or
the other, but must consist of both global and local disciplines
[10]. Furthermore, in [10] also stated that for the proper use of
information in an information society, the education relating to
information ethics may present four goals, i.e. (1) respect for
others must be cultivated; (2) although sharing beneficial
information is welcome, other people’s intellectual property
right must not be infringed; (3) various forms of information
will be used productively; and (4) telecommunications and the
Internet will be used for acceptable time periods so that it does
not harm actual life.

There are several issues related to information ethics.
Several issues are emerged with different names or from
different sources, but its essence remains the same. One of the
most important topics in information technology ethics is
privacy [20]. Fallis, in [18], saying that the core issues of
information ethics include intellectual freedom, equitable
access to information, information privacy, and intellectual
property. In addition, in [19], it is also stated the two major
issues of information technology, i.e. the conflict between
observing others’ privacy, and the simultaneous pursuit of
individual freedom and autonomy. Another issue related to
information ethics is responsibility and accountability. This is
in line with what is also revealed in [19], that information
ethics deals with the moral conduct of information-users based
on their responsibility and their accountability.

IV. A REVIEW ON CYBER ETHICS DEFINITION AND ISSUES

“Cyber” is a prefix used to describe people, things, or ideas
that are connected with computers and the internet [21].
Therefore, cyber ethics is closely related to the development of
internet technology, so that some definition of cyber ethics will
include the internet or online terms in it. Indeed, some sources
call cyber ethics with internet ethics. In this paper, the term
cyber-ethics and internet ethics can interchangeable with each
other

Cyber ethics is really about social responsibility in
cyberspace [22]. As stated in [10], Cyber ethics is a system of

Vol. 2, No. 8, 2011

standards that prescribe morality and immorality in cyberspace,
signifying the preservation of freedom of expression,
intellectual property, and privacy. Other definition of cyber
ethics can be found in [23], that is cyber ethics is the discipline
dealing with what is good and bad, and with moral duty and
obligation as they pertain to online environments and digital
media.

Lin, in [24], call cyber ethics as an internet ethics. In the
paper, Lin stated that the right or wrong about the utility of
internet by mankind can be called internet ethics. The utility of
internet by mankind includes interpersonal communication,
information’s delivery, research, storage and so on [24].

It could be argued, that all of ethics, which applies in the
computer ethics and information ethics can also apply on cyber
ethics, but only focused on its application to the internet. As
stated in the [25], the term of internet ethics can be thought as a
special extention of computer ethics, but the main difference
between them is that internet ethics is dealing with the
behaviors performed in internet.

The challenge for cyber ethics is to discuss principles of
morality that can guide human action so that people are
empowered to establish a sustainable, participatory global
information society [26]. Cyber ethics can discuss real
possibilities of development of the information society and
criticize ideologies that portray the information society in
uncritical and one-dimensional ways [26]. By e-mail or
newsgroup, any sort of opinions and thoughts can be spread all
over the world. On the one hand, it can help people
communicate, express opinions and thoughts, and get responses
from other people fast; on the other hand, it also may be
misused, such as quite a few bothering ads, fraud letters,
nonsense articles interfering seriously with other people’s
chances to get useful information [24].

Some of cyber ethics issues raised in [23], i.e. plagiarism,
copyright, hacking, fair use, file sharing, online etiquette
protocols, posting incorrect/inaccurate information,
cyberbullying, stealing or pirating software, music, and videos,
online gambling, gaming, and internet addiction. Several other
current cyber ethics issues are raised in [27], i.e. privacy,
security, electronic monitoring of employees, collection and
use of personal information on consumers, and identity theft.

V. THE DEFINITION, POSITION AND ISSUES OF E-
GOVERNMENT ETHICS

Simply, e-Government ethics can be defined as ethics in the
use of e-Government system, either to insert or update content
into the system or to get content from the system. However, to
give more in-depth understanding, we will try to explain it
further by reviewing the definition of e-Government and see
how e-Government system is implemented.

e-Government is the use of Information Technology (IT) by
public sector organizations [1]. Other definition of e-
Government is public sector use of the internet and other digital
devices to deliver services, information, and democracy itself

[2].

e-Government is also an information system [1]. So, it can
be said that several theories about the information system can

84 |

www.ijacsa.thesai.org
(IJACSA) International Journal of Advanced Computer Science and Applications,

be applied in e-Government. However, e-Government is
different from ordinary information system that is generally
targeting the private sector. The main orientation of e-
Government is the accessibility of information by the public,
rather than financial income [1].

Because the target of e-Government is the public sector,
then the e-Government systems are generally built based on the
web technology. This technology is used because it has ability
to reach people quickly and widely. This also implies that the
users of e-Government systems will generally using computer
in accessing the system.

e-Government has one of the characteristics of
postmodernism, that is the social construct [30]. This shows the
presence of interaction between human beings in e-
Government. The interaction of course need a set of rules to
regulate it. One set of rules that can be applied is ethics.

Based on the above definition and description regarding to
e-Government, then it can be said, that in e-Government, it
could apply three applied ethics, ic. computer ethics,
information ethics and cyber ethics. But, beside these three
applied ethics, in e-Government, it also could apply some
others applied ethics.

e-Government is not simply about information technology
or website, but there are also some aspects of management in it.
As revealed in [1], that e-Government are socio-technical and
there are two aspects in e-Government, ic. the technical
aspects (technology) and managerial aspects. This means, that
there is other ethics that can be applied, that is management
ethics.

Moreover, one of the actors involved in e-Government is
the business. So, the business ethics can also be applied in e-
Government.

There is another aspect in e-Government, that is the aspect
of the object being observed. For example, in [28], it is
proposed a new paradigm in e-Government called e-Livestock.
From the definition of e-Livestock, it can be seen that the
object related to e-Livestock is the animal, i.e. cows or
buffaloes. In this case, in addition to several ethics already
discussed above, there are other ethics that can be related to e-
Government, that is the ethics of animal treatment.

Computer ethics, information ethics, and cyber ethics can
be said as the foundations of e-Government ethics, and there
are another applied ethics as the complement of the e-
Government ethics. Fig. 1 shows the position of e-Government
ethics in relation to the other applied ethics.

Based on Fig. 1, it can also be concluded that some issues
related to computer ethics, information ethics, and cyber ethics,
which has been mentioned previously, could also become an
issue in e-Government ethics. But, of course, it could be added
with other issues, such as the issue of sensitivity of the
information as revealed in [29], or the issue of trustworthiness
of the content of e-Government system.

Vol. 2, No. 8, 2011

E-Government Ethics

 

Figure 1. e-Government ethics position related to computer ethics,
information ethics, cyber ethics and other applied ethics.

VI. CONCLUSIONS

This paper has summarized the definitions and several
issues related to computer ethics, information ethics and cyber
ethics. The three applied ethics turns out to be a foundation for
e-Government ethics.

The definition of the e-Government ethics has been given in
this paper. The position of e-Government ethics among
computer ethics, information ethics, cyber ethics, and other
applied ethics (such as the ethics of management, business,
object treatment, etc.) is also described in this paper.

This paper can be the starting point of research about e-
Government ethics. In the future, there could be many other
applied ethics and issues that can be identified and added to the
e-Government ethics presented in this paper.

REFERENCES
[1] R. Heeks, Implementing and Managing eGovernment An International
Text, London, England : SAGE Publications, 2006.

[2] D. M. West, Digital Government Technology and Public Sector
Performance, New Jersey, USA: Princeton University Press, 2005.

[3] A. Shalbaf, “A view of Problems and Practical Pattern of Promotion of
Ethics in Educational Organizatons,” Iranian Journal of Ethics in
Science and Technology, vol. 4, no. 1, 2009.

[4] S.B. Kaddu, “Information Ethics: a student’s perspective,” International
Review of Information Ethics, vol. 7, 2007.

85|Page

www.ijacsa.thesai.org
[5]

[6]

7]

[8]

19]

[10]

fl)

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

(IJACSA) International Journal of Advanced Computer Science and Applications,

U. Averweg and G. Erwin, “Towards a Code of Cyberethics for a
Municipality in South Africa,” in Proceedings of the Fifth International
Conference on Electronic Business, 2005, pp. 522-527.

R. Yucel, H. Elibol, and O. Dagdelen, “Globalization and International
Marketing Ethics Problems,” International Research Journal of Finance
and Economics, issue. 26, pp. 93-104, 2009.

J. Fieser, “Ethics”, Internet Encyclopedia of Philosophy, A Peer-
Reviewed Academic Resources. [Online]. Available:
http://www.iep.utm.edu/ethics/, 2009.

B. C. Stahl, “Information, Ethics, and Computers: The Problem of
Autonomous Moral Agents,” Minds and Machines, vol. 14, pp. 67-83,
2004.

J. H. Moor, “Reason, Relativity, and Responsibility in Computer
Ethics,” Computers and Society, pp. 14-21, March 1998.

H. Ki and S. Ahn, “A Study on the Methodology of Information Ethics
Education in Youth,” International Journal of Computer Science and
Network Security, vol. 6, no. 6, pp. 91-100, 2006.

M. Namayandeh and H. Taherdoost, “Review Paper on Computer Ethics
and Related Research Models,” Journal of Open Problems in Science
and Engineering, vol. 1, no. 1, pp 42-47, 2009.

R. Heersmink, J. V. D. Hoven, N. J. V. Eck, and J. V. D. Berg,
“Bibliometric Mapping of Computer and Information Ethics,” CWTS
Working Paper Series, p. 13, 2010.

W. W. Lee and A. K. K. Chan, “Computer Ethics: an Argument for
Rethinking Business Ethics”, in The 2nd World Business Ethics Forum:
Rethinking the Value of Business Ethics, 2008.

A. Kuzu, “Problems Related to Computer Ethics: Origins of The
Problems and Suggested Solutions,” The Turkish Online Journal of
Educational Technology, vol. 8, issue. 2, pp. 91-110, 2009.

P. Brey, “Disclosive Computer Ethics: The Exposure and Evaluation of
Embedded Normativity in Computer Technology,” Computers and
Society, vol. 30, no. 4, pp. 10-16, 2000.

R. Capurro and J. B. Britz, “In search of a code of global information
ethics: The road travelled and new horizons,” Ethical Space: The
International Journal of Communication Ethics, vol. 7, no. 2/3, pp. 28-
36, 2010.

L. Floridi, “Information Ethics: An Environmental Approach to the
Digital Divide,” Philosophy in the Contemporary World, vol. 9, no. 1,
2001.

D. Fallis, “Information ethics for twenty-first century
professionals,” Library Hi Tech, vol. 25, no. 1, pp. 23-36, 2007.
C. P. Chuang and J. C. Chen, “Issues in Information Ethics and
Educational Policies for the Coming Age,” Journal of Industrial
Technology, vol. 15, no. 4, 1999.

A. R. Peslak, “Current Key Privacy Factors: Development and

Analysis,” Journal of Information Technology Impact, vol. 6, no. 3, pp.
171-186, 2006.

library

Vol. 2, No. 8, 2011

[21] F. L. Wilczenski and S. M. Coomey, “Cyber-Communication: Finding
Its Place in School Counseling Practice, Education,and Professional
Development,” ASCA, pp 327-331, 9:4 April 2006.

S. Mahfood, A. Astuto, R. Olliges, and B. Suits, “Cyberethics Social

Ethics Teaching in Educational Technology Programs,” Communication

Research Trends, vol. 24, no. 4, pp. 3-43, 2005.

[23] D.  Pruitt-Mentle, “2008 National Cyberethics, Cybersafety,
Cybersecurity Baseline Study”, Educational Technology Policy
Research and Outreach (ETPRO), National Cyber Security Alliance
(NCSA), October 2008.

[24] J. X. Lin, “Educational Enlightenments from Internet Ethics Issues,”
Journal of Information, Technology and Society, 2003(2), pp. 65-72.

[25] F. Odabasi and E. B. Kuzu, “A Proposal For Ethics Training In Internet
Cafes,” in Proceedings of the 7th WSEAS International Conference on
Education and Educational Technology, pp. 141-144, 2008.

[26] C. Fuchs, R. M. Bichler, and C. Raffl, “Cyberethics and Co-operation in
the Information Society,” Sci Eng Ethics, Vol. 15, pp. 447-466, 2009.

[27] T. A. Kraft and J. Carlisle, “Computer Ethics: A Slow Fade from Black
and White to Shades of Gray,” in Proceedings of Information Systems
Educators Conference, 2010.

[22]

[28] A. Ramadhan and D. I. Sensuse, “e-Livestock as a New Paradigm in e-
Government,” in Proceedings of the 3rd International Conference on
Electrical Engineering and Informatics (ICEEI 2011), IEEE Press, vol.
1, 2011.

[29] Z. Fang, “E-Government in Digital Era: Concept, Practice, and
Development,” International Journal of The Computer, The Internet and
Management, vol. 10, no. 2, pp. 1-22, 2002.

[30] A. Ramadhan, D. I. Sensuse, and A. M. Arymurthy, “Postmodernism in
e-Government,” International Journal of Computer Science Issues, vol.
8, issue. 4, no. 1, pp. 623-629, July 2011.

AUTHORS PROFILE

Arief Ramadhan. B.Sc in Computer Science (Bogor Agricultural University,
Indonesia, 2005), M.Sc in Computer Science (Bogor Agricultural University,
Indonesia, 2010), Ph.D Student in Computer Science (University of
Indonesia), Research Assisstant at University of Indonesia. Member of e-
Government Lab at University of Indonesia.

Dana Indra Sensuse. B.Sc in Geology (Bogor Agricultural University,
Indonesia, 1985), M.Sc in Library and Information Studies (Dalhousie
University, Halifax, Canada, 1994), Ph.D in Information Studies (Toronto
University, Canada, 2004), Lecturer at University of Indonesia, Head of e-
Government Lab at University of Indonesia.

Aniati Murni Arymurthy. B.Sc in Electrical Engineering (University of
Indonesia, 1973), M.Sc in Computer and Information Sciences (Ohio State
Univ., Ohio, USA, 1981), Ph.D in Computer Science (University of Indonesia,
1997), Professor at Faculty of Computer Science in University of Indonesia,
Lecturer at University of Indonesia, Head of Pattern Rec, Image Proc, and
CBIR Labs at University of Indonesia.

86 |

www.ijacsa.thesai.org
The Ethical Problem of Framing e-Government in Terms of e-

Commerce

Bernd Carsten Stahl

Centre for Computing and Social Responsibility, De Montfort University, UK

bstahl@dmu.ac.uk

Abstract: This paper discusses one aspect of the relationship that the use of information and communication technology
(ICT) in business has with the use of ICT in government and administration. It argues that democracies rely on their
ethical legitimacy and that framing e-Government and e-Democracy in commercial terms can jeopardise this legitimacy.
For this purpose the paper distinguishes between e-Government as service delivery and e-Democracy as the more
radical use of ICT for democratic deliberation and policy formulation. It argues that the commercial paradigm can support
some of the moral values underpinning democracy but it can also have a negative effect on them by equating customers
and citizens, by likening the political and the economic system and by promoting hidden agendas and ideologies. The
conclusion argues that democratic decision makers need to pay attention to these relationships. Otherwise they not only
endanger the success of e-Government and e-Democracy but may even threaten the basis of the moral legitimacy of

democratic forms of government.

Keywords: e-Government, e-Democracy, e-Commerce, legitimacy, ethics, morality

1. Introduction

e-Government is a growth industry whose
potential has been recognised by most of the big
players in the hardware and software market from
IBM to Microsoft. While the end of the dot.com
boom seems to have taken the glamour out of
information and communication technology (ICT),

the fundamental advantages it offers to
organisations are still the same. Quick
communication, better access to information,

shrinking transaction costs, or greater flexibility
are just some of the more important ones. While
many businesses and industries were quick to
take on these advantages and move into e-
business and e-Commerce, the same is not
always true for government and _ public
administration. Nevertheless, most industrialised
societies have started down the road of the use of
information and communication technology (ICT)
in government and some have made considerable
progress.

In many respects the development of e-
Government seems to follow the example of e-
Commerce. High level administrators or political
leaders recognise the potential of a certain
technology and decide to deploy it in their area of
responsibility. Given that the systems as well as
the vendors and their personnel are usually
experienced in e-Commerce or e-business, similar
systems are used in e-Government and similar
processes are installed. Furthermore, the rhetoric
of e-Government uses arguments and logic which
stem from the business world. In many cases this
happens deliberately and with the best of
intention. The perceived weaknesses_ of
democratic governments and administrations
include a high level of bureaucracy, a duplication
of efforts in departments that do not communicate

ISSN 1479-439X

Reference this paper as:

7?

and a general sluggishness and lack of response.
Given that businesses increasingly try to
overcome these problems, to become agile
competitors, the hope is that the use of the
paradigm of business in government, which is
transported through the medium of e-Government,
will alleviate these problems.

This paper will evaluate this development from an
ethical perspective. It will ask what the
implications of the use of the commercial
paradigm on e-Government will be by
concentrating on the moral basis of democracy.
The paper will start out by discussing the moral
foundation of democracy and its links to ethics
and a morality. It will then analyse the concept of
e-Government and introduce the important
distinction between e-Government as_ the
technological delivery of administrative services
and e-Democracy as_ the _ technological
enhancement of primary democratic processes.
From there the paper will proceed to take a look at
e-Commerce and why it seems to be a suitable
paradigm for e-Government. The use of this
paradigm will then be analysed and especially the
limits of its use will be discussed. The result of the
discussion will be that e-Commerce is a legitimate
paradigm in some respects because it stands for
values such as efficiency gains or better
distribution that are common to democracies. At
the same time, business ideas can only represent
democratic processes within relatively narrow
limits. The central problem is that the
conceptualisation of humans differs fundamentally
between the business world and politics. In
business people are most importantly consumers
whereas in democracy, people are predominantly
citizens. The danger of the commercial paradigm
is that it implies that citizens can be reduced to
consumers. This change of the conceptualisation

@©Academic Conferences Ltd

Stahl B (2005) “The Ethical Problem of Framing e-Government in Terms of e-Commerce” The Electronic
Journal of e-Government Volume 3 Issue 2, pp 77-86, available online at www.ejeg.com
Electronic Journal of e-Government Volume 3 Issue 2 2005 (77-86)

of humans creates a change from e-Democracy to
e-Government, it excludes certain members from
inclusion, and it generally affects the character of
democracy. This, it will be argued, threatens the
moral legitimacy of democracy which is the central
basis of its acceptability and therefore of its
success. The conclusion will therefore be that
politicians as well as_ information systems
professionals must make sure that they keep the
sometimes fine line between business processes
and political processes in mind in order to avoid a
failure of the (political as well as technical) system
and retain its legitimacy.

2. The ethics of democracy

As it is the purpose of this paper to argue that the
commercial paradigm can threaten the moral
legitimacy of democracy we will have to clarify first
what democracy is and how it is related to ethics.
This section will therefore start with a review of
some of the defining aspects of democracy in
order to then establish its relationship with ethics
and morality. It will end by briefly looking at some
of the weaknesses of democracy.

2.1 The concept of democracy

A look at the etymology of democracy shows that
it literally means "rule of the people". It is the
conceptual opposite of forms of government
where single persons or minorities rule. While the
idea of democracy, of the people governing
themselves, may appear almost trivial to many
citizens of Western democracies who never
experienced another form of government, it is
important to note that it is anything but trivial. It
contains a number of _ implications and
suppositions that need to be spelled out in order
to understand the importance of ethics for
democracy and also the conditions under which
democracies can function and be stable.

One implication of democratic rule is that the will
of the community is created bottom-up, that the
individual members of society collaborate to
determine what society does. Democracy is a
formal process that leaves the outcome of the
decision process mostly open. It only determines
the external format necessary to make decisions.
These decisions refer not only to actions but,
more importantly, to intentions. That means that
democracy is the process of collective forming of
a political will as well as the way of realising this
will (cf. Richardson 1999).

This implies that every member of society is
recognised as a person, that the rights of all
persons are equal, that the individual is protected
from the arbitrariness of society. At the same time
democracy stands for an attitude by individuals

www.ejeg.com

78

that implies responsibility for the commonwealth,
tolerance, and courage (cf. Sdéderbaum 2000;
Hengsbach 1991). The very heart of democracy is
the deliberative process that allows the forming of
the political will. This is based on the idea that the
members of the democratic society are willing and
able to exchange ideas and arguments in such a
way as to come to acceptable and legitimate
majority decisions (Habermas 1998).
Communication can thus be said to be the
essence of democracy (Ricoeur 1991).

Another possible approach to democracy is to
look at its purpose. The formal and functional
description of democracy given above implies
purposes but does not spell them out. One can,
however, see democracy as a means to and end.
The ends that democracy is supposed to realise
could then be the safeguarding of internal peace
and individual freedom (Hayek 1994).

These few short characterisations of democracy
can not claim completeness. What they should be
useful for, however, is to point us in the direction
of the ethical basis of democracy, to show us why
accepted moral rules and_ their ethical
justifications are of central importance to the
functioning of democracy.

2.2 The ethical basis of democracy

There are different reasons why democracy is
linked to ethics and morality. Maybe the most
obvious one is that it is a system that distributes
power. Power affects our moral rights and
obligations, the way we can and should behave,
and it is also of theoretical and reflective
importance. The most important aspect of ethics
and power in democracies is that democratic
processes give power legitimacy. Power as the
ability to make others do one's bidding is a
necessary part of any community and it can only
be held if the affected parties believe it to be
justified and legitimate. In a post-metaphysical
society the source of legitimacy of power can
apparently only come from the assumption that
democratic processes, albeit fallible, create the
most reasonable results that can be expected
(Habermas 1998). Power can always be misused
but democracies seem to be better at avoiding or
ending misuse than authoritarian forms of
government (Kiting 1997). Democracies are
decentralised and this decentralisation allows
reasonable local solutions (Beck 1986). The
participation of individuals which is constitutive for
democracies allows regulations which are
acceptable to all (Kant 1992; Tocqueville 1998). In
addition to the provision of a legitimate distribution
and execution of power, strength of democracies
is that they facilitate the change of power
relationships in a peaceful and way.

©Academic Conferences Ltd
Another link between ethics and democracy can
be developed from the underlying anthropological
assumptions. Democracy is based on a view of
humans that is itself ethically charged. The citizen
of democracy is modelled after the enlightenment
idea of humans, as free, autonomous and moral
agents. Democracy can only exist with this
(sometimes counterfactual) view of its members.
This anthropological view assumes moral values
such as the fundamental equality of all citizens
and it esteems the classical liberal individual as
the basis of community.

Finally, democracy promises to deliver moral
values to society as a whole by forming the
autonomous individual through socialisation and
education. Democracy requires and disseminates
knowledge and it provides the court of public
exchange for the creation of knowledge (cf. Rauch
1993). The institutions and members. of
democracy promise the achievement of progress
in material, social, intellectual and many other
respects. One important basis of this argument is
the close link between democracy as a political
framework and capitalism as the corresponding
economic framework (Becker 1976; Friedman
1994). Many of the moral arguments supporting
democracy can be found in a similar form for
capitalism. Democracy and its emphasis on the
individual is supposed to give people the skills
and the desire to perform well economically and
the aggregation of individual performance should
lead to an improvement in general welfare. Only
on the basis of a functioning economic system
can wealth be redistributed to the needy which
again strengthens the moral case for democracy
(Rorty 1996). The combination of capitalism and
democracy should not only increase welfare in
individual states but also lead to an equalising
effect between countries and, at least for those
countries that participate, bring a generally high
standard of living (Cohen 1996).

As another moral advantage, democracy is
supposed to be peaceful. Since the sovereign is
the people itself and the people (as opposed to
the elites or aristocrats) suffer the most from a
war, democracy is often depicted as intrinsically
peaceful. Furthermore, war tends to disrupt
commerce. Thus, the business people in
democratic states, who have a strong political
influence, are supposed to be _ peaceful
(Tocqueville 1999).

2.3 Moral weaknesses of democracy

The last section may have struck the reader as
overly optimistic and, in fact, democracy may not
always display the moral advantages just

www.ejeg.com

79

Bernd Carsten Stahl

described. From the first start of democracy, there
has always been the suspicion that it is nothing
but the rule of mob (Aristoteles 1967). It has often
been suspected that democracies are intrinsically
instable, for different reasons. Rorty (1996)
suspects that democracies require a high level of
material well-being to function; that they cannot
survive real hardship. Maybe even worse is the
material emptiness of democracies. Plato (1973)
believed that they have to disintegrate because
they know no boundaries and until today it is open
whether liberal democracies can provide humans
with an idea of the "good life" which has always
been central to ethical thought (Postman 1992).

There is the problem of theory and practice, the
question whether democratic states can really live
up to the expectations levelled at them.
Experience tells us that the noble idea of free
forming of the political will bottom-up may not
work in practice. The view of humans that informs
democracy will often not be displayed and
reflected by democratic institutions. The welfare
argument may be weak because experience
shows that not everybody participates in the
generation and sharing of wealth. Finally, it can
even be argued that democracies are not peaceful
but that they create perverse incentives which
make them intrinsically more belligerent than
autocratic regimes (Tocqueville 1999).

While we should thus take the moral advantages
of democracy with a grain of salt, we should be
aware that they have one central function. They
legitimize the democratic form of government.
Whether fact or fiction, the moral side of
democracy allows us to distribute power and
resources, to find collective solutions, to create a
shared vision of the good life. All of this is never
perfect. It can only work because the vast majority
of the affected accept it as morally justified.
Arguably, every form of government needs this
sort of justification and democracy seems best
suited to provide it in the modern’ world.
Admittedly, these are strong assumptions and
might lead to a lengthy debate on political theory
and practice. | will not be able to dwell on them
here and hope that the reader finds them
sufficiently plausible to follow the rest of my
argument which is based on the assumption that
ethics and morality play an essential role in
legitimizing democracy.

3. e-Government and e-Democracy

The "e-" in front of a noun usually denotes the use
of ICT for the purposes that the noun traditionally
stands for. E-commerce uses ICT for commercial
purposes, e-learning uses ICT for education etc.
Similarly, the term "e-Government' stands for the

ISSN 1479-439X
Electronic Journal of e-Government Volume 3 Issue 2 2005 (77-86)

use of ICT in the realm of government. Clearly,
the area covered by the term "government" is
immense and depends on the definition of
government. In the widest sense it can stand for
all of the activities by municipal, regional, or
national governments and administration. It can
also include activities of the legislative and judicial
power. The word e-Government is often used in
such a wide sense which can be problematic. In
this paper we will distinguish between e-
Government as the administrative use of ICT and
e-Democracy as the use of ICT for genuine
democratic purposes. This distinction is important
because the use of the paradigm of business can
hide or imply a shift from e-Democracy to e-
Government and thereby threaten the legitimacy
of the democratic form of government.

3.1 e-Government and service delivery

E-government understood as the use of ICT for
the purposes of the executive branch of
government is advancing quickly and covering
more and more areas in a geographic as well as
thematic sense. For which purposes is ICT used
by governments? The answer to this question
depends on the type and particularities of
government. Generally, there seems to be a trend
to include as much as_ possible into e-
Government. One can fundamentally distinguish
between internal process of governments and
external relationships where the latter can be
divided into relationships with citizens or
constituents and others, such as_ other
governments or organisations. ICT can thus be
used for purposes as different as internal data
exchange for the streamlining of workflows or
international development (Thompson 2003).

While e-Government could thus theoretically span
a wide range of activities, it appears that
governments and their bureaucracies have a
strong tendency to favour activities that could
broadly be described as service delivery. This is
arguably the case because bureaucracies have
the task of delivering services and because there
is an intrinsic affinity between governments and
ICT, which can also be called a technology of
“command and control" (Postman 1992, 115). It is
not possible to prove this point here but
disregarding the reasons for the development one
can easily find that a large number of publications
about the topic of e-Government are concerned
with service delivery. (For a plethora of examples
cf. Bannister & Remenyi (eds.) 2003.)

Most of us have come across examples of this
trend. Municipalities post local information on the
Internet, tax returns can be done electronically,
drivers licences can be applied for online etc.
While this development is beneficial in many

www.ejeg.com

80

respects it also seems to take away awareness
from other applications of ICT, namely those that
are directly linked to democratic processes, which
we will call e-Democracy (Wastell 2003).

3.2 e-Democracy and the radicalisation
of democratic processes

While e-Government as_ service delivery is
arguably the prominent face of the use of ICT in
democratic institutions, there is another side,
which is more interesting and which has the
potential to radically change our understanding of
democracy. We will call this side "e-Democracy"
and it stands for the use of ICT for the purposes of
democratic deliberation and policy formulation.
One can often find the idea that ICT and
specifically the Internet are inherently democratic
technologies. The reasoning is that "(1)
Democracy means power in the hands of
individuals (the many); (2) information is power;
(3) the Internet makes vast quantities of
information available to individuals; (4) therefore,
the Internet is democratic" (Johnson 2001, 211;
cf. Johnson 2000). This democratic promise was
one of the main motivators for the investment in
Internet technologies by government, most
notably the Internet backbones in the USA (Gore
1995). One should note that this inherent
democratic character of Internet technology is
often used as a moral argument to support its
development and implementation (cf. Stichler &
Hauptman 1998). On this basis some authors go
so far as to develop grand visions of technological
utopias where constant interaction leads to an
ideal democracy which displays high ethical
values (Lévy 1997; Meeks 2000).

Why is this form of democracy so desirable?
Collectively it allows for new forms of free and
equal deliberation. Everybody can make his or her
voice heard on all matters of interest. In fact,
systems have been built that allow for public
discourse of socially relevant topics which are
based explicitly on the (ethical) principles of
Habermasian discourse theory (Heng & de Moor
2003). This means that ICT can be used to
approximate the ideal speech situation where only
the power of the better argument counts. This
collective advantage can be translated into the
maximisation of knowledge and therefore in an
optimal viability of the outcomes of deliberations.
At the same time the participation of stakeholders
guarantees the moral viability of discussions.
Also, the chance to participate in discourses and
thereby influence the outcome of democratic
decisions is supposed to bring about
emancipation and empowerment of the individuals
(Blanke 1998; Hirschheim & Klein 1994).

©Academic Conferences Ltd
In the context of this discussion one should note
that the introduction of this sort of online
deliberation and policy formulation could have
radical consequences. It leads away from the
established representational model of democracy
toward a more direct type of democracy. This can
be seen as positive for the reasons given above
but it can also be problematic. Either way this
vision of a more radical technology-mediated
democracy is highly ethically charged. It affects
the individual's rights and obligations, it is based
on our view of human beings and it changes the
distribution of power. This radical democracy
could thus strengthen the moral legitimacy of
democracy but it can also produce problems.

3.3. Problems of e-Government and e-
Democracy

Both, e-Government and e-Democracy, run into
problems. The problems of e-Government tend to
be of a technical nature whereas e-Democracy
faces more fundamental obstacles. e-Government
faces problems of technical implementation, of
user involvement, of cooperation between
different administrative departments and the like.
These are typical problems of systems design,
implementation, and use that we know from the
information systems literature. While these
problems are not trivial, there are established
ways of addressing them.

The problems of e-Democracy are more serious.
While the promises that it holds are immense, the
criticism is just as impressive. Some authors state
that e-Democracy simply does not live up to its
promises, that instead of promoting democracy,
ICT has undemocratic effects (Breen 1999), that
instead of decentralising access, it centralises it
(Yoon 1996) that it stabilises power structures
instead of changing them (Stallman 1995;
Weizenbaum 1976). Another fundamental
problem is that of the democratic ideal that seems
to be promoted by ICT, namely direct democracy.
This may appear attractive for several reasons,
but it also threatens to turn into the plebiscite that,
since Plato, has been feared as the ugly face of
democracy (Ess 1996; Paletz 2000).

Apart from these problems which draw into doubt
whether e-Democracy is really desirable at all,
there are also numerous practical problems.
Among these we find the nature of the Internet
which is designed to avoid central control which
might make it difficult to regulate it to the extent
that it might be suitable for e-democratic purposes
(Lessig 2001). Then there is the complex of
problems caused by globalisation and the change
of the nature of the state. E-democracy offers the
vision of a world-wide democracy but at the same

www.ejeg.com

81

Bernd Carsten Stahl

time we do not know how global problems can be
addressed. Our political system and = our
democracies are based on the nation-state,
whose future is uncertain (cf. Castells 1997).

4. The commercial paradigm in e-
Government and e-Democracy

Thus far it was argued that its ethical qualities are
of central importance for the legitimacy of
democracy. It was then discussed that within
democracies ICT can be used in two
fundamentally different ways: as a tool for
administration and service delivery or as a means
of changing the way democracy is conducted. The
former, here called e-Government, is relatively
unproblematic as it only changes the modes of
delivery of established processes. The latter, e-
Democracy, holds radical promises as well as
potential pitfalls, both of which are closely linked
to its moral foundation. What this paper is
interested in is how the use of business as a
paradigm influences the discussion, perception,
and use of ICT in a democracy. This section will
therefore introduce what the concept of a
business as a paradigm means before discussion
which effect the commercial paradigm has on e-
Government and e-Democracy

4.1 The (e-)Commercial paradigm

E-commerce, understood as the use of ICT in
business, is as complex a topic as e-Government
or e-Democracy. It has been the object of
extensive attention by IS researchers. This paper
does not aim to reflect this research or to say
anything about e-Commerce as such. Instead, it is
interested in the use of e-Commerce as a
rhetorical tool, in the way perceptions of e-
Commerce are used to shape expectations of e-
Government and e-Democracy. E-commerce is
thus seen as a paradigm in a weakly Kuhnian
(1996) sense. Paradigms shape our view of
reality, they determine what is important and
relevant. In this sense the use of ICT in the
economic system can be said to be a paradigm of
the use of ICT in the political system.

In order to understand this paradigm we need to
briefly discuss which advantageous
characteristics e-Commerce has that makes it
attractive to people who are interested in political
applications of ICT. Arguably the most important
reason for the use of the commercial paradigm is
the huge success of e-Commerce. While some of
the initial enthusiasm has died down when the
stocks of the dot.com collapsed, the markets of e-
Commerce and e-business is still growing.
Technology is now mature enough to offer its
advantages at low costs and ICT in business has

ISSN 1479-439X
Electronic Journal of e-Government Volume 3 Issue 2 2005 (77-86)

by now become so ubiquitous that some authors
see it as a commodity (Carr 2003). The
advantages of e-Commerce can be divided in
those for the individual organisation that adopts it
and those for society at large. For the individual
organisation, e-Commerce promises cost
reduction and market expansion (Shin 2003). The
most visible aspect of this is the reduction of
transaction costs (Welty & Becerra-Fernandez
2001; Castells 2000). Social advantages of e-
Commerce are increased competition (Spinello
2000), the creation of new markets (Schiller 1999)
and the aggregate effects of cost reduction, which
should result in an increase in overall welfare.
Related advantages with moral undertones are
liberty and efficiency. Liberty stands for greater
consumer choice but also for a more flexible
design of economic relationship, as for example in
teleworking (McCalman 2003). Efficiency is the
cause of welfare increases but it also implies
other business virtues such as flexibility, and
customer-centeredness.

4.2 Reasons for the use of the
commercial paradigm

The reason why e-Commerce may seem like a
useful paradigm for the use of ICT in governments
is its success. If e-Commerce could be
successful, so the argument goes, then e-
Government or e-Democracy should copy the
approaches and processes and thereby copy the
success. Furthermore, business in general is
perceived to be able to overcome problems
inherent in democratic decision making and
administration and the adoption of the commercial
paradigm is implied to improve this situation.

Spelt out in more detail, there are several explicit
or implicit arguments for the adoption of the
commercial paradigm. First, there is the technical
one. Many of the commercially available systems
have now reached a level of maturity that allows
businesses to depend on them and to generate
steady profits. Using such established systems
(possible systems would include enterprise
resource planning or customer relationship
management) would allow administrations to
avoid the tedious systems development process.

Apart from the technical side, there are
organisational issues. Democratic institutions are
often perceived to be bureaucratic and slow, to be
inflexible and disregard the needs of the citizens.
Business in general and e-Commerce in particular
are viewed differently. Modern businesses need to
be agile, to understand their environment
including competitors, suppliers, and customers,
and they can focus their efforts when necessary.
Translated to governments this would mean that

www.ejeg.com

82

decisions could be made more quickly while
incorporating the important stakeholders.
Commercial processes should overcome
bureaucracies and allow a focus on the citizen
Last but not least, it should go some way toward
addressing the problem of motivation. Civil
servants and democratic politicians are often
viewed as unsuitable for their jobs and not
sufficiently motivated. The business world,
allegedly, knows how to deal with this sort of
problem through a sophisticated management of
incentives and human resources.

Then there are the commercial benefits based on
market principles that could be translated in
political benefits. Among them we find a greater
liberty and more choice for the consumer. In terms
of democracy this might translate into competition
between jurisdictions or between organisations
within jurisdictions. This should lead to more
freedom and better services for the citizen.

While these are probably not all of the advantages
of the commercial paradigm, they encompass the
most important ones. For our argument it is
important to see that these contentions are of a
moral nature. Whether it is the mere improvement
of business processes, the saving of costs, or the
general overhaul of administrations, they all
translate into moral goods such as freedom,
welfare, and distribution of the citizen.

4.3 Problems of the commercial paradigm

While there are good reasons to apply the
commercial paradigm to ICT use in government
and politics, some of which have a clear moral
content, there are also plausible arguments to be
made against it. We will briefly look at the limits of
the analogy of customer and citizen, at problem of
the analogy of business and politics, and finally at
genuine political problems.

The first problem of the commercial paradigm is
the equation of customers and citizens. This is
useful in so far as citizens have the same role as
customers, namely as recipients of services and
goods. However, it is important to see that there
are fundamental differences between customers
and citizens. A company caters to the needs of
customers but only when and if this is in its own
interest. A customer who is overly troublesome,
cannot pay, threatens the organisation's integrity,
etc. has no right to be catered to. This is different
for citizens who remain citizens no matter what.
While a customer usually has the choice between
suppliers, the same is rarely true for citizens. We
cannot choose which country or state we want to
live in. Furthermore, the state has a huge amount
of power over the citizen, which is not comparable
with the firm's power over the customer. Most

©Academic Conferences Ltd
importantly, customer and company are
fundamentally separate entities, whereas citizen
and state depend on one another. Ideally, in a
democracy, the government should represent the
state, which is a manifestation of the people who
are citizens of the state. A government thus has to
accept citizens because it is (indirectly) acting on
their behalf. Describing citizens as customer thus
takes away their input and ownership in state and
government and thereby robs the state of its own
power basis.

The next group of problems of the commercial
paradigm consists of the analogy of political and
economic system. Can the state be run like a
company or a market and is political leadership
like commercial management? There are some
reasons to negate this question. First, there is the
problem of competition. We have seen that the
strength of the commercial paradigm is partly
based on competition, which is supposed to
create more motivation and better welfare. In
politics there is the question whether competition
between governments is fundamentally possible
and if it is desirable. What should competition in
government look like? Should we have to financial
authorities and the one with the better tax rate
wins? This seems unlikely. Competition is only
possible between states and there it is
questionable because citizens are not free to
choose. Another problem of competition, when
applied to citizens, is that, by definition, it
produces losers. If citizens were to compete for
state services, then some would not get them.
This is sometimes possible and legitimate (for
example for research grants) but in many cases
the nature of state services rules out that some
have to lose (e.g. social welfare).

Second, there is the problem of efficiency.
Efficiency is supposed to be one of the great
strengths of businesses and something that
politics and public administration lack. However, a
closer look reveals that it is difficult to define
efficiency. In economics it is usually held to be
Pareto-optimality (Sen 1987), which means that a
state is efficient where no more mutually
beneficial exchanges are possible. The problem
with this definition is that it neglects the question
of justice as a state where one person owned
everything and nobody else owned anything
would be efficient. This definition of efficiency
therefore does not seem useful but others are not
readily available.

Possibly even more serious than the problems of
the comparability between economic and political
sphere are the genuine political problems the
commercial paradigm creates. Among them there
is the question of public goods. While markets

www.ejeg.com

83

Bernd Carsten Stahl

may be good at allocating scarce goods under
competition they are notoriously bad at dealing
with public goods (such as the environment,
public infrastructure, etc.) One of the legitimating
aspects of democratic governments is that they
are able to use an impartial perspective in the
allocation and management of public goods, e.g.
network infrastructure (Chapman & Rotenberg
1995). Similarly, it can be argued that the
commercial realm is not good at providing other
aspects which are vital for the ethical legitimacy of
democracies. Among them one finds access,
which is often discussed in terms of ICT and the
digital divide (Breen 1999), but which extends
more generally to access to the political life. Even
more important is the question of the distribution
of power. Democracies require the fiction that
everybody has equal access to power. Vhile this
is arguably not always the case, there still is a
high degree of theoretical equality between
citizens. The capitalist system has no intrinsic
interest in equality of access and power (Introna
2001) and its application to politics could thus
jeopardise democratic legitimacy.

The most serious issue with regards to the
political problems of the commercial paradigm is
that it transports a more or less hidden political
ideology, namely capitalist liberalism. The very
idea that the state can be seen as an economic
system implies that it should be left alone and is
self-regulating, the way markets are usually
described. This is an ideology because it hides
vested interests and describes as natural and
unchangeable what is in fact man-made and
contingent (Hirschheim & Klein 1994). At the
same time this economic description of politics
finds many proponents because it plays to the
libertarian culture of the early Internet (Fagin
2000; Winner 2000). The political culture of e-
Democracy as based on e-Commerce is therefore
not a neutral construct but carries with it a load of
implications that are not necessarily accepted by
everyone (Kester 1998). Among _ these
implications there is the suggestion that
commercial exchange is the key social interaction
which in turn implies a commodification not only of
knowledge and information (Yoon 1996) but, at
the extreme, of human relationships and political
exchange.

5. Conclusion

This paper argued that democracy depends on its
ethical legitimacy. The use of ICT in democracies
can have positive as well as negative effects on
this ethical legitimacy. A considerable part of
literature and research on e-Government and e-
Democracy is framed in terms of business,
particularly e-Commerce. The point of interest in

ISSN 1479-439X
Electronic Journal of e-Government Volume 3 Issue 2 2005 (77-86)

this paper was which influence this use of
commerce as a paradigm has on the foundational
legitimacy of democracy. The results of this
discussion were somewhat ambiguous. On the
one hand the commercial paradigm promotes
values such as efficiency, service quality, speed
of delivery etc. that can also be valuable for the
(moral) legitimacy of democracy. On the other
hand it can produce problems due to the
confusion of customers and citizens, the
dissimilarity of political and economic system and
the hidden agenda and ideology it can carry.
Finally, there is the danger that the commercial
paradigm shifts attention from the possibly radical
(and threatening) potential of e-Democracy, with
all its moral advantages and disadvantages, to the
more manageable realm of e-Government. This
shift that Wastell (2003) has analysed may affect
the legitimacy of ICT in democracies because it
hides the radical potential of e-Democracy and
concentrates on those processes in e-
Government that are similar to business
processes by design.

Why should we care about all of this? If the
argument is correct then a shift of attention to the
business-like side of administration can jeopardise
the viability of democratic structures. Democracy
is about the exchange of views and the finding of
compromises between different groups and
individuals. The more international and globalise
our societies become the more important it will be
that we manage to include as many of the
different voices as possible. The traditional nation
state with a more or less homogenous population
may have found it easy to determine the people's
will. This is becoming more difficult and it is one of
the strengths of democracy that it allows the
deliberation of diverse participants. This is where
ICT could play an important role and where it
could strengthen the moral acceptability of
democracy by widening and deepening measures
of participation.

Thinking about democracy in terms of business
can seriously threaten this legitimising potential of
ICT. Service delivery is clearly a part of any
administration but the concentration on service
delivery misses the point of democracy. This is
where the argument of this paper should have a
clear practical impact. Decision makers who
determine the use of ICT in democracies need to
be aware of this pitfall. While vendors of
information systems often have more experience
with commercial systems and naturally try to
extend their use in government, politicians and
bureaucrats need to be aware that this can lead to
a dangerous narrowing of the use of ICT. While
framing e-Government and e-Democracy in terms
of (e-)commerce is not a bad thing per se, political

www.ejeg.com

84

decision makers need to be aware that it can
produce moral problems that not only jeopardise
the success of e-Government and e-Democracy
but that can affect the very legitimacy of
democratic structures. The commercial paradigm
is certainly not the only factor that plays a role
here, but it is a good indicator of the view that is
held about democracy. Democracy contains
certain economic structures and sets their
framework. If this relationship is turned around
and the economic system starts to dominate the
political then its legitimacy is threatened. The
commercial paradigm cannot single-handedly lead
to such a result. But it can indicate that decision
makers are willing to accept such a reversal of
roles and it may go some way to promoting it.

References

Aristoteles Die Nikomachische Ethik, dtv /
Artemis, Zurich und Munchen, 1967

Bannister, F. & Remenyi, D. (eds.) Proceedings of
the 3rd European Conference on e-
Government, Trinity College Dublin, 03 - 04
June 2003

Beck, U. Risikogeselischaft. Auf dem Weg in eine
andere Modeme, Suhrkamp, Frankfurt a. M,
1986

Becker, G. S. The Economic Approach to Human
Behavior, The University of Chicago Press,
Chicago, London, 1976

Blanke, H. T. ‘Librarianship and Public Culture in
the Age of Information Capitalism’ In: Ethics,
Information and Technology: Readings,
Stichler, R. N.& Hauptman, R. (eds.), pp. 184 -
199, MacFarland & Company, Jefferson, North
Carolina, 1998

Breen, M. 'Counterrevolution in the Infrastructure -
A Cultural Study of Techn-Scientific
Impoverishment' In: Ethics and Electronic
Information in the 27st century, Pourciau, L. J.
(ed.), pp.29 - 45, Purdue University Press,
West Lafayette, Indiana, 1999

Carr, N. G. 'IT Doesn't Matter’ Harvard Business
Review (81:5), 41 - 49, 2003

Castells, M. The Information Age: Economy,
Society, and Culture. Volume |: The Rise of the
Network Society, 2nd edition, Blackwell,
Oxford, 2000

Castells, M. The Information Age: Economy,
Society, and Culture. Volume ||: The Power of
Identity, Blackwell, Oxford, 1997

Chapman, G. & Rotenberg, M. 'The National
Information Infrastructure: A Public Interest
Opportunity' In: Computers, Ethics & Social
Values, Johnson, D. G. & Nissenbaum, H.
(eds.), pp. 628 - 644, Prentice Hall, Upper
Saddle River, 1995

Cohen, M. G. 'Democracy and the Future of
Nations - Challenges for Disadvantaged

©Academic Conferences Ltd
Women and Minorities' In: States Against
Markets - The limits of globalization, Boyer, R.
& Drache, D. (eds.), Routledge, London, New
York, 1996

Ess, C. 'The Political Computer: Democracy,
CMC, and Habermas' In: Philosophical
Perspectives on Computer-Mediated
Communication, Ess, Charles (ed.), pp. 187 -
230, State University of New York Press,
Albany, 1996

Fagin, B. 'Liberty and Community Online’ In:
Cyberethics - Social and Moral Issues in the
Computer Age, Baird, R. M., Ramsower, R. &
Rosenbaum, S. E. (eds.), pp. 332 - 352,
Prometheus Books, New York, 2000

Friedman, M. ‘Introduction to the Fiftieth
Anniversary Edition’ In: The Road to Serfdom,
Hayek, F.A., Fifthieth Anniversary Edition
Chicago: The University of Chicago Press,
1994

Gore, A. 'Global Information Infrastructure’ In:
Computers, Ethics & Social Values, Johnson,
D. G. & Nissenbaum, H. (eds.), pp. 620 - 628,
Prentice Hall, Upper Saddle River, 1995

Habermas, J. Faktizitat und Geltung: Beitrage zur
Diskurstheorie des Rechts und des
demokratischen Rechtsstaats, Suhrkamp,
Frankfurt a. M., 1998

Hayek, F. A. vonThe Road to Serfdom, Fifthieth
Anniversary Edition, The University of Chicago
Press, Chicago, 1994

Heng, M. S. H. & de Moor, A. 'From Habermas's
Communicative Theory to Practice on the
Internet! Information Systems Journal (13),
2003, 331 - 352

Hengsbach, F. Wirtschaftsethik - Aufbruch -
Konflikte - Perspektiven, Herder Verlag,
Freiburg i. B., 1991

Hirschheim, R. & Klein, H. K. 'Realizing
Emancipatory Principles in Information
Systems Development: The Case for ETHICS'
MIS Quarterly (18:1), 1994, 83 - 109

Introna, L. 'Workplace Surveillance, Privacy, and
Distributive Justice’ In: Spinello, R. A. &
Tavani, H. T. (eds.), pp. 418 - 429, Jones and
Bartlett, Sudbury, Massachusetts et al., 2001

Johnson, D. G. Computer Ethics, 3rd edition,
Prentice Hall, Upper Saddle River, New
Jersey, 2001

Johnson, D. G. 'Democratic Values and the
Internet' In: internet Ethics, Langford, D. (ed.),
pp. 181 - 196 McMillan, London, 2000

Kant, |. ber den Gemeinspruch: Das mag in der
Theorie richtig sein, taugt aber nicht fiir die
Praxis, Meiner, Hamburg, 1992

Kester, G. H. ‘Access Denied: Information Policy
and the Limits of Liberalism’ In: Ethics,
Information and Technology: Readings,
Stichler, R. N.& Hauptman, R. (eds.), pp. 207 -
230, MacFarland & Company, Jefferson, North

www.ejeg.com

85

Bernd Carsten Stahl

Carolina, 1998

Kuhn, T. S. The Structure of Scientific
Revolutions, 3rd edition, The University of
Chicago Press, Chicago and London, 1996

Kung, H. Weltethos fiir Weltpolitik und
Weltwirtschaft. 3rd edition, Pieper Verlag,
Munchen, 1997

Lessig, L. 'The Laws of Cyberspace’ In: Readings
in Cyberethics, Spinello, R. A. & Tavani, H. T.
(eds.), pp. 124 - 134, Jones and Bartlett,
Sudbury, Massachusetts et al., 2001

Lévy, P. Cyberculture, Editions Odile Jacob,
Paris, 1997

McCalman, J. 'What Can we Do For Corporate
Nomads? IT and Facilities Management In: /7-
Based Management: Challenges and
Solutions, Joia, L. A. (ed.), pp. 130 - 142, Idea
Group Publishing, Hershey, et al., 2003

Meeks, B. N. 'Better Democracy Through
Technology’ In: Cyberethics - Social and Moral
Issues in the Computer Age, Baird, R. M.,
Ramsower, R. & Rosenbaum, S. E. (eds.), pp.
288 - 294, Prometheus Books, New York,
2000

Paletz, D. L. ‘Advanced Information Technology
and Political Communication’ In: Cyberethics -
Social and Moral Issues in the Computer Age,
Baird, R. M., Ramsower, R. & Rosenbaum, S.
E. (eds.), pp. 285 - 287, Prometheus Books,
New York, 2000

Platon Der Staat, Alfred Kréner, Stuttgart, 1973

Postman, N. Technopoly - The Surrender of
Culture to Technology, Vintage Books, New
York, 1992

Rauch, J. Kindly Inquisitors: the new attacks on
free thought, The University of Chicago Press,
Chicago, 1993

Richardson, H. S. 'Institutionally Divided Moral
Responsibility’ In: Responsibility, Paul, E. F.,
Miller, F. D. & Paul, J. (eds.), pp. 218 - 249,
CUP, Cambridge, 1999

Rorty, R. ‘The Notion of Rationality’ In: Debating
the State of Philosophy - Habermas, Rorty and
Kolakowski, Niznik, Josef, Sanders, John T.
(eds.), Praeger, Westport, Connecticut /
London, 1996

Ricoeur, P. 'Certitudes et incertitudes de la
revolution chinoise' In: Lectures 7 - Autour du
politique, Ricoeur, P., pp. 315 - 340, Seuil,
Paris, 1991

Schiller, D. Digital Capitalism: networking the
global market system, MIT Press, Cambridge,
Massachusetts / London, 1999

Sen, A. On Ethics and Economics, Basil
Blackwell; Oxford, New York, 1987

Shin, N. Productivity Gains from IT’s Reduction of
Coordination Costs. In: Creating Business
Value with Information Technology:
Challenges and Solutions, Shin, N. (ed.), pp.
125 - 145, Idea Group Publishing, Hershey PA,

ISSN 1479-439X
Electronic Journal of e-Government Volume 3 Issue 2 2005 (77-86)

London, 2003

Spinello, R. Cyberethics: Morality and Law in
Cyberspace, Jones and Bartlett, London, 2000

Séderbaum, P. ‘Institutional Theory in Relation to
Environment and Development - On
individuals as actors guided by an ideological
orientation’ In: Efficiency, Justice, and Stability
- Ethical Perspectives in Economic Analysis
and Practice, Grenholm, C.-H. & Helgesson,
G. (eds.), pp. 37 - 50, University of Uppsala,
Uppsala, 2000

Stallman, R. 'Are Computer Property Rights
Absolute?' In: Computers, Ethics & Social
Values, Johnson, D. G. & Nissenbaum, H.
(eds.), pp.115 - 119, Prentice Hall, Upper
Saddle River, 1995

Stichler, R. N. & Hauptman, R. (eds.) Ethics,
Information and Technology: Readings,
MacFarland & Company Jefferson, North
Carolina, 1998

Thompson, M. 'ICT, Power, and Developmental
Discourse: A Critical Analysis' In: Global and
Organizational Discourse About Information
Technology, Wynn, E., Whitley, E., Myers, M.
D. & DeGross, J. (eds.), pp. 347 - 373, Kluwer
Academic Publishers, Dordrechtm, 2003

Tocqueville, A. De la démocratie en Amérique ll,
Gallimard, Paris, 1999

www.ejeg.com

86

Tocqueville, A. De la démocratie en Amérique |,
Gallimard, Paris, 1998

Wastell, David G. ‘Organizational Discourse as a
Social Defense: Taming the Tiger of Electronic
Government’ In: Global and Organizational
Discourse About Information Technology,
Wynn, E., Whitley, E., Myers, M. D. &
DeGross, J. (eds.), pp. 179 - 195, Kluwer
Academic Publishers, Dordrecht, 2003

Weizenbaum, J. Computer Power and Human
Reason, W. H. Freeman and Company, San
Francisco, 1976

Welty, B. & Becerra-Fernandez, |. 'Managing
Trust and Commitment in Collaborative Supply
Chain Relationships' Communications of the
ACM (44:6), 2001, 67 - 73

Winner, L. 'Cyberlibertarian Myts and the
Prospects for Community’ In: Cyberethics -
Social and Moral Issues in the Computer Age,
Baird, R. M., Ramsower, R. & Rosenbaum, S.
E. (eds.), pp. 319 - 331, Prometheus Books,
New York, 2000

Yoon, S.-H. 'Power Online: A Post-Structuralist
Perspective on Computer-Mediated
Communication’ In: Philosophical Perspectives
on Computer-Mediated Communication, Ess,
Charles (ed.), pp. 171 - 196, State University
of New York Press, Albany, 1996

©Academic Conferences Ltd
Ethics and Privacy of Communications
in the E-Polis

Gordana Dodig-Cmkovic and Virginia Horniak
Department of Computer Science and Electronics
Malardalen University
Vasteras, Sweden

INTRODUCTION

The electronic networking of physical space promises wide-ranging advances in science,
medicine, delivery of services, environmental monitoring and remediation, industrial
production, and the monitoring of persons and machines. It can also lead to new forms of
social interaction [..]. However, without appropriate architecture and regulatory controls
it can also subvert democratic values. Information technology 1s not in fact neutral in its
values; we must be intentional about design for democracy. (Pottie, 2004)

Information and communication technology, ICT, has led to the emergence of
global web societies. The subject of this article is privacy and its protection in the
process of urbanization and socialization of the global digital web society referred
to as the e-polis. Privacy is a fundamental human right recognized in all major
international agreements regarding human rights such as Article 12 of the Universal
Declaration of Human Rights (United Nations, 1948), and it will be discussed in
the chapter under the heading Different Views of Privacy.

Today’s computer network technologies are sociologically founded on hunter-
gatherer principles. As a result, common users may be possible subjects of
surveillance and sophisticated Internet-based attacks. A user may be completely
unaware of such privacy breaches taking place. At the same time, ICT offers the
technical possibilities of embedded privacy protection obtained by making
technology trustworthy and legitimate by design. This means incorporating options
for socially acceptable behavior in technical systems, and making privacy
protection rights and responsibilities transparent to the user.

The ideals of democratic government must be respected and even further
developed in the future e-government. Ethical questions and privacy of
communications require careful analysis, as they have far-reaching consequences
affecting the basic principles of e-democracy.
VALUES OF THE E-POLTIS

In our post-industrial age we are witnessing a paradigm shift from techno-
centrism to human-centrism and the emergence of an entirely new value system
which holds out the prospect of a new Renaissance epoch. The arts and
engineering, sciences and the humanities are given a means whereby they can reach
a new synthesis (Dodig-Crnkovic, 2003). This meeting of cultures is occurring to a
great extent in cyber space, making issues of cyber ethics increasingly important.

One expression of a new rising human-centrism is the emergence of e-
government which changes the citizen-government relation, making the political
system transparent and more accessible to the citizen in the participatory
democracy. It is therefore argued that a rethinking of the idea of development in
the contemporary globally-networked civilization is necessary (Gill, 2002).
Networking at the global level must be seen in a symbiosis with local resources.
Social cohesion in this context results from the ability to participate in the
networked society through mutual interaction, exchange of knowledge and sharing
of values. The problem of promoting e-government in developing countries via
virtual communities’ knowledge-management is addressed by Wagner, Cheung,
Lee, and Ip (2003).

PRIVACY MATTERS

Before the advent of ICT, communication between people was predominantly
verbal and direct; (Moore, 1994, Agre and Rotenberg, 1997). Today we
increasingly use computers to communicate. Mediated by a computer, information
travels far and fast to a virtually unlimited number of recipients, and almost
effortlessly (Weckert, 2001). This leads to new types of ethical problems including
intrusion upon privacy and personal integrity. Privacy can be seen as a protection
of two kinds of basic rights:

e Priority in defining ones own identity. (This implies the right to control the use of personal
information that is disclosed to others, as personal information defines who you are for the
others. As a special case the freedom of anonymity can be mentioned. In certain situations we
are ready to lend our personal data for statistical investigations, for research purposes and
similar, under the condition that anonymity 1s guaranteed.)

e The right to private space. (This is generalized to mean not only physical space but also
special artifacts that are exclusively associated with a certain individual, such as a private
diary or private letters - or disk space.) The privacy of ones’ home is a classic example of a
private space which moreover is related to ones own identity. It is also an instructive
archetype because it shows the nature of a private space as a social construction. You are in
general allowed to choose whom you wish to invite to your home. However, under special
circumstances it is possible for police, for example, to enter your home without your consent,
this being strictly regulated by law.
Historically, as a result of experiences within different cultures a system of
practices and customs has developed that defines what is to be considered personal
and what is public, see (Warren and Brandeis, 1890), (Thompson, 2001). A basic
distinction in human relations is consequently that between the private (shared with
a few others) and the common (shared with wider groups), (DeCew, 2002). Fried
(Rosen, 2000) claims that only closely related persons can have true knowledge of
an individual.

According to Mason (2000), privacy can be studied through the relationships of
four social groups (parties). The first party is the individual himself/herself. The
second party consists of those others to whom the first party provides specific
personal information for the sake of creating or sustaining a personal relationship
or in return for services. The third party consists of all of the other members of
society who can get access to an individual’s private information, but who have no
professional relation to the individual and no authority to use the information.
Finally, the fourth party is general public who are in no direct contact with the
individual’s private space or information. During the interaction between parties,
individuals invoke different levels of privacy. The advantages of close
relationships are compared with the risks of the release of information and its
inappropriate use which could result in a loss of personal space or harm to ones
identity.

DIFFERENT VIEWS OF PRIVACY

The acquisition, storage, access to and usage of personal information is
regulated and limited in most countries of the world by legislation. However, each
part of the world has its own laws. In the US, separate laws apply to different kinds
of records. Individual European countries have their own specific policies
regarding what information can be collected, and the detailed conditions under
which this is permissible. (For an international survey of privacy laws, including
country-by-country reports, see Privacy and Human Rights 2004; see also Briefing
Materials on the European Union Directive on Data Protection).

The current political situation in the world and the threat of terrorist attacks
has led to governmental proposals in the European Union requiring Internet Service
Providers to store personal information, for example data relating to Internet
traffic, e-mails, the geographical positioning of cellular phones and similar, for a
period of time longer than is required of them at present (ARTICLE 29 Data
Protection Working Party).

Although relevant legislation is in effect locally, there are difficulties with
respect to the global dissemination of information. To avoid conflicting situations,
there is a need for international agreements and legislation governing the flow of
data across national borders.

COMPUTER ETHICS

Information and communication technology, ICT, is value-laden, as is
technology in general, and is changing our ways of conceptualizing and handling
reality, (Bynum and Rogerson, 2003, Spinello, 2003). It is not always easy to
recognize intrinsic values incorporated in an advanced technology. Specialized
technical knowledge is often needed for an understanding of the intrinsic
functionality of a technology, for example, how information is processed in a
computer network.

The need for a specific branch of ethics for computer and information systems,
as compared with a straightforward application of a general ethical theory to the
field of computing is discussed by (Bynum, 2000, Floridi and Sanders, 2002 and
Johnson, 2003). Tavani (2002) gives an overview of this so called uniqueness
debate. While the philosophical discussion about its nature continues, computer
ethics/cyber ethics is growing in practical importance and is establishing itself as a
consequence of the pressing need for the resolution of a number of acute ethical
problems connected with ICT.

The changing resources and practices appearing with ICT both yield new
values and require the reconsideration of those established. New moral dilemmas
may also appear because of the clash between conflicting principles when brought
together unexpectedly in a new context. Privacy, for example, is now recognized as
requiring more attention than it has previously received in ethics, (Moor, 1997).
This is due to reconceptualization of the private and public spheres brought about
by the use of ICT, which has resulted in the recognition of inadequacies in existing
moral theory about privacy. In general, computer ethics can provide guidance in the
further development and modification of ethics when the existing is found to be
inadequate in the light of new demands generated by new practices, (Brey, 2000).

For Moor (1985), computer ethics is primarily about solving moral problems
that arise because there is a lack of policy (policy vacuum) about how computer
technology should be used. In such a case, the situation that generates the moral
problem must firstly be identified, conceptually clarified and understood. On the
other hand, Brey claims that a large part of work in computer ethics is about
revealing the moral significance of the existing practices that seem to be morally
neutral. ICT has implicit moral properties that remain unnoticed because the
technology and its relation to the context of its use are not sufficiently understood.
Disclosive computer ethics has been developed in order to demonstrate the values
and norms embedded in computer systems and practices. It aims at making
computer technology and its uses transparent, revealing its morally relevant
features.

FATR INFORMATION
PRACTICES

One of the fundamental questions related to the expansion of community
networks is the establishment of fair information practices that enable privacy
protection. At present it is difficult to maintain privacy when communicating
through computer networks, which are continually divulging information. An
example of a common concern is that many companies endeavor to obtain
information about the behavior of potential consumers by saving cookies on their
hard disks. Other possible threats against citizen’s privacy include the unlawful
storage of personal data, the storage of inaccurate personal data, the abuse or
unauthorized disclosure of such data that are issues surrounding government-run
identity databases. Especially interesting problems arise when biometrics is
involved (for identity documents such as passports/visas, identity cards, driving
licenses). Remote electronic voting is dependent on the existence of voters’
database, and there are strong privacy concerns if the same database is used for
other purposes, and especially if it contains biometric identifiers.

Many countries have adopted national privacy or data protection laws. Such
laws may apply both to data about individuals collected by the government and to
personal data in the hands of private sector businesses. The OECD have defined
fair information practices which include the following principles: Collection
limitation, Data quality, Purpose specification, Use limitation, Security, Openness,
Individual participation and Accountability (see OECD Guidelines on the
Protection of Privacy).

The exceptions to these principles are possible in specific situations, such as
law enforcement investigations, when it might not be appropriate to give a suspect
access to the information that the police are gathering. Nonetheless, the principles
of fair information practices provide a framework for the privacy protection.

Legitimacy is a social concept developed during human history, meaning
“socially beneficial fairness”. It concerns classical social problems such as the
prisoner’s dilemma and the “tragedy of the commons” in which individuals may
profit at the expense of society. Social interactions without legitimacy lead society
into an unsustainable state.

However, traditional mechanisms that support legitimacy, such as laws and
customs are particularly ineffective in the cyberspace of today with its flexible,
dynamic character, (Whitworth and de Moor, 2003). The remedy is the
incorporation of legitimacy by design into a technological system. That process
begins with a legitimacy analysis which can translate legitimacy concepts, such as
freedom, privacy and intellectual property into specific system design demands. At
the same time it can translate technological artifacts such as computer programs
into statements that can be understood and discussed in terms of ethical theory.

Legitimate interaction, with its cornerstones of trustworthiness and
accountability, seems a key to the future of the global information society. This
implies that democratic principles must be built into the design of socio-technical
systems such as e-mail, CVE’s (Collaborative Virtual Environments), chats and
bulletin boards, electronic voting systems and similar. As the first step towards that
goal, the legitimacy analysis of a technological artifact (software/hardware) 1s
suggested.

“Trust is a broad concept, and making something trustworthy requires a social
infrastructure as well as solid engineering. All systems fail from time to time; the legal
and commercial practices within which they're embedded can compensate for the fact that
no technology will ever be perfect.” (Mundie, at al., 2003)

In any computer-mediated communication, trust ultimately depends not on
personal identification code numbers or IP addresses but on relationships between
people with their different roles within social groups. The trust necessary for
effective democracy depends on communication and much of the communication is
based on interaction over computer networks. Trust and privacy trade-offs are
normal constituents of human social, political, and economic interactions, and they
consequently must be incorporated in the practices of the e-polis. The bottom line
is of course the transparency of the system and the informed consent of all the
parties involved.

CONCLUSION

ICT supports and promotes the formation of new global virtual communities
that are socio-technological phenomena typical of our time. In an e-democracy
government, elected officials, the media, political organizations and citizens use
ICT within the political and governance processes of local communities, nations
and on the international stage. The ideal of e-democracy is greater and more direct
citizen involvement. For the modern civilization of a global e-polis, the optimal
functioning of virtual communities is vital. What are the basic principles behind
successful virtual community environments? According to Whitworth there are two
such principles:

e =Virtual community systems must match the processes of human-human interaction.

e Rights and ownership must be clearly defined.

It is technically possible for ICT to incorporate these principles which include
privacy protection via standards, open source code, government regulation etc.
(Pottie, 2004, Tavani & Moor, 2000), including also trustworthy computing,
(Mundie, at al., 2003).

A process of continuous interaction and dialogue is necessary to achieve a
socio-technological system which will guarantee the highest standards of privacy
protection. Our conclusion is that trust must be established in ICT, both in the
technology itself and in the way it is employed in a society.

REFERENCES

Agre PE & Rotenberg M ed. (1997). Technology and Privacy: The New
Landscape. MIT Press.

ARTICLE 29 Data Protection Working Party. Retrieved December 11, 2004,
from http://europa.eu.int/comm/internal_market/privacy/workingroup_en.htm

Brey P (2000). Method in Computer Ethics: Towards a Multi-Level
Interdisciplinary Approach. Ethics and Information Technology, 2:3, 1-5.

Briefing Materials on the European Union Directive on Data Protection.
Retrieved December 11, 2004, from http://www.cdt.org/privacy/eudirective/ The
Center for Democracy & Technology.

Bynum TW & Rogerson S ed. (2003). Computer Ethics and Professional
Responsibility. Blackwell.

Bynum TW (2000). Ethics and the Information Revolution, In: G. Collste.
Ethics in the Age of Information Technology. Linkoeping, Sweden: Center for
Applied Ethics Linkoeping Universitet, 32-55.
Dodig-Crnkovic G (2003). Shifting the Paradigm of the Philosophy of Science:
the Philosophy of Information and a New Renaissance. Minds and Machines:
Special Issue on the Philosophy of Information, Volume 13, Issue 4.

DeCew J (2002). Privacy, The Stanford Encyclopedia of Philosophy. Retrieved
December 11, 2004, from

http://plato.stanford.edu/archives/sum2002/entries/privacy

Floridi L & Sanders J (2002). Mapping the Foundationalist Debate in Computer
Science, a revised version of Computer Ethics: Mapping the Foundationalist
Debate. Ethics and Information Technology, 4.1, 1-9.

Gill KS (2002). Knowledge Networking in Cross-Cultural Settings. AJ &
Society, 16: 252-277.

Johnson DG (2003). Computer Ethics, The Blackwell Guide to the Philosophy
of Computing and Information. Edited by Luciano Floridi. Blackwell Publishing.

Mason RO (2000). A tapestry of Privacy, A Meta-Discussion. Retrieved
December 11, 2004, from Attp://cyberethics.cbi.msstate.edu/mason2/

Moor JH (1997). Towards a Theory of Privacy for the Information Age.
Computers and Society, Vol. 27, No. 3.

Moor JH (1985). What is computer ethics? Metaphilosophy, 16/4.

http://www.ccsr.cse.dmu.ac.uk/staff/Srog/teaching/moor. htm

Moore, B, Jr. (1994). Privacy: Studies in Social and Cultural History. Armonk,
NY: M. E. Sharpe, Inc.

Mundie, C, de Vries, P, Haynes, P and Corwine M, (2003)

Trustworthy Computing White Paper
http://www.microsoft.com/mscorp/twe/twe_whitepaper.mspx

Privacy and Human Rights 2004, An International Survey of Privacy Laws and
Developments. Retrieved December 11, 2004, from
http://www. privacyinternational.org/survey/phr2004

Pottie GJ (2004). Privacy in the Global Village. Communications of the ACM,
47(2): 2-23.

OECD Guidelines on the Protection of Privacy - Recommendation of the
Council Concerning Guidelines Governing the Protection of Privacy and
Transborder Flows of Personal Data (23rd September, 1980)

Rosen J (2000). Why Privacy Matters. Wilson Quarterly, Vol 24, Issue 4.

Spinello RA (2003). Cyberethics Morality and Law in Cyberspace. Jones and
Bartlett Publishers.

Tavani HT (2002). The uniqueness debate in computer ethics: What exactly is
at issue, and why does it matter? Ethics and Information Technology, 4: 37-54, and
references therein.

Tavani HT & Moor JH (2000). Privacy Protection, Control of Information, and
Privacy-Enhancing Technologies. Proceedings of the Conference on Computer
Ethics-Philosophical Enquiry.

Thompson PB (2001). Privacy, secrecy and security. Ethics and Information
Technology, 3.

United Nations (1948). Universal Declaration of Human Rights, General
Assembly resolution 217 A (III). http://www.un. org/Overview/rights. htm]

Wagner C & Cheung K & Lee F & Ip R (2003). Enhancing e-government in
developing countries: managing knowledge through virtual communities. The
Electronic Journal on Information Systems in Developing Countries, 14, 4, 1-20.

Warren S & Brandeis LD (1890). The Right to Privacy. Harvard Law Review,
Vol. 4, No.5.

 
Weckert J. (2001). Computer Ethics: Future directions. Ethics and Information
Technology, 3.

Whitworth B & de Moor, A (2003). Legitimate by design: Towards trusted
virtual community environments. Behaviour and Information Technology, 22:1,
p31-51.

Terms and Definitions

Computer ethics: A branch within applied ethics dealing with ethical questions concerning ICT.
Computer ethics includes application of ethical theories to issues regarding the use of computer
technology, together with standards of professional practice, codes of conduct, aspects of
computer law and related topics.

Cookies: Information about a user that is stored by the server on the user’s hard disk. Typically, a
cookie records user’s preferences when using a particular stte which often happens without user’s
knowledge even though they must nominally agree to cookies being saved for them.

CVE’s: Collaborative Virtual Environments

Cyberethics/ Cyber ethics: See Computer ethics. Broadly speaking, cyber ethics deals with the
conduct of individuals with respect to the information world, in the words of Moor “the
formulation and justification of policies for the ethical use of computers.” Narrowly, Cyber-ethics
refers to Computer ethics discipline in cyberspace.

Cyber space: A virtual space that consists of resources available through computer networks. It
also refers to the culture developed by electronically connected communities. The term was first
coined by William Gibson in the book Neuromancer.

Technology legitimate by design: Technology that is designed in such a way as to promote its
legitimate use and to prevent its misuse.

Disclosive ethics: A multi-level interdisciplinary approach concerned with the exposition of
embedded values and norms in computer systems, applications and practices.

Design for democracy: The incorporation of options for socially acceptable behavior in technical
systems, making the basic principles of privacy protection, rights and responsibilities transparent
to the user.

Policy vacuum: James Moor has defined this term, meaning the absence of policies for
governing conduct in new situations resulting from the use of new technologies. The central task
of computer ethics is to fill the policy vacuums resulting from the use of computers, by
formulating guidelines for their use.

Uniqueness debate: A discussion among ethicists whether computer ethics is a unique field of
ethics, or merely a straightforward application of existing ethical theories to the specific
technology (computers).

Privacy, right of: The right of a person to be free from intrusion into or publicity concerning
matters of a personal nature called also right to privacy.
See discussions, stats, and author profiles for this publication at: https://www.researchgate. net/publication/259644296
Framing the Questions of E-Government Ethics

Article in The American Review of Public Administration - February 2015

DOL: 10.1177/0275074013485809

CITATIONS READS
18 1,374
lauthor:

 

= Alexandru Roman

 

California State University, San Bernardino

39 PUBLICATIONS 342 CITATIONS

SEE PROFILE

All content following this page was uploaded by Alexandru Roman on 15 February 2015.

The user has requested enhancement of the downloaded file.
Article

 

American Review of Public Administration
° ° 2015, Vol. 45(2) 216-236
Framing the Questions of © The Author(s) 2013

Reprints and permissions:

E -G overnme nt Eth i cs: An sagepub.com/journalsPermissions.nav

DOI: 10.1 177/02750740 | 3485809

Organizational Perspective arp.sagepub.com
@SAGE

Alexandru V. Roman!

Abstract

Scholars have suggested that the delineation of a field’s “big” questions is critical for its
cohesive and practical intellectual growth. Instilling a habitual practice of focusing inquiries on
fundamental questions is particularly warranted for fledgling areas of research. Currently, while
there is already a rich body of literature that addresses administrative, computer, information,
and cyber ethics, only a limited number of writings discuss ethical problems specifically within
the e-government context. It can be argued that the e-government condition introduces a
distinctive type of ethical problems; questions regarding which have yet to be properly framed.
This article suggests five critical questions of e-government ethics at the organizational level that
justify notable academic attention.

Keywords
administrative/public service ethics, information and communication technology, e-government

The daily effort to fill meaning vacuums created by the search to serve “the public interest” is an
unavoidable experience of the work of public administrators. By regularly taking decisions within
the discretionary realm of everyday public administration and within the context of a multitude of
accountability obligations, public servants are often faced with ethical challenges (Waldo, 1988).
Some moral decisions become routinized or ritualized, while others remain emotion-based con-
structions of identities and personal relationships (Maynard-Moody & Musheno, 2006).
Textbook ethical templates, that attempt to guide through the moral implications of adminis-
trative behavior, often underestimate the consequential nature of their theoretical empowerment
of the individual. In theory, individuals might have the ability to make a certain ethical choice; in
practice, however, more often than not, people are trapped in organizational and social habits
over which they have trivial influence. In regards to deceit, for instance, Bok (1978/1999) writes,

The social incentives to deceit are at present very powerful; the controls, often weak. Many individuals
feel caught up in practices they cannot change. It would be wishful thinking, therefore, to expect
individuals to bring about major changes in the collective practices of deceit by themselves. (p. 244)

 

'California State University San Bernardino, USA

Corresponding Author:
Alexandru V. Roman, Assistant Professor of Public Administration at California State University San Bernardino, USA.
Email: alexandru.v.roman@gmail.com

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 217

 

Multipurpose guidelines of administrative ethics are an unquestionable need. Yet, an ade-
quate scholarly prudence against an oversimplification tendency is in order when suggesting
ethical principles of conduct. Such caution is especially defensible when dealing with com-
plex systems such as e-government, which are replete with ambiguity and conflicting inter-
ests. Organizational routines can consume the individual to the point where one’s ethical
autonomy becomes almost nonexistent. Chambliss (1996) argues that individuals who have
the power to make system changing decisions have the luxury of facing ethical “dilemmas”;
all other organizational members, all the same, consistently face ethical “problems,” which
most of time they cannot influence nor solve. Furthermore, “we must conclude that the power
of the individual’s conscience is very weak relative to that of legitimated authority in modern
organizations and social structures more generally, and that current ethical standards do too
little to limit the potential for evil in modern organizations” (Adams & Balfour, 2004, p. 151).
For considering that cyberspace is more controlling than we are currently willing or able to
admit (Lessig, 2006), the danger and possibility of administrative evil is indeed shaping to be
very real.

Technology, routinely outlined as a mere mechanical aid to our life, is far from the neutral
instrument that it is made out to be; it actively and profoundly shapes our interactions, perspec-
tives, decisions, moral valuations, and imagery (Verbeek, 2011). The recent turn to a heavy
reliance on information communication technologies (ICTs) for purposes of governance has the
potential to motivate fundamental, perhaps even disruptive, changes at the organizational, and
policy levels (Bovens & Zouridis, 2002; Dunleavy, Margetts, Bastow, & Tinkler, 2006;
Milakovich, 2012).

Just as much as laws, the design of IT systems can have strong effects in embodying and freezing a
particular set of administrative capabilities—literally “embodying” since in “legacy” systems a given set
of procedures will be written up in millions of lines of programming or code, which then becomes
expensive to change or modify at a later stage. (Dunleavy et al., 2006, p. 25)

Such shifts are bound to come with significant ethical problems, especially for an individual’s
organizational habitus; yet, these implications are seldom explored by public administration
scholars. Extant literature provides a rich choice of works that address in part administrative eth-
ics or e-government, but only a limited body of scholarly writings that specifically discuss the
ethical problems associated with the redesign of public organizations under the tenets of
e-government (Mullen & Horner, 2004). Technology does have important objective capacities,
but it is also subjected to organizational structures, social interactions, and meaning creation
processes (Orlikowski, 1992, 2000). In this sense, there is a sensible gap in our understandings
of the ethical problems that might arise as public organizations are transformed within the con-
text of e-governance. Although information, computer and cyber ethics can provide a useful
foundation for generating such understandings, neither of the three, in part nor in aggregate,
provides an entirely satisfactory framework.

In this analytical article I argue that research on e-government ethics should attend to
enquiries that are sensibly different from the questions addressed by information, computer or
cyber ethics. E-government is surely more than the infrastructure and mechanisms provided
by technology, it is ultimately about people, power, and meanings that are routinely con-
structed by social and organizational interactions. In e-government, ideology and technology
hang together.

Drawing directly and heavily on Chambliss’ (1996) structural theory of ethical problems in
organizations, I suggest five critical questions that current and future research on e-government
ethics should carefully examine and perhaps focus on:

Downloaded from arp.sagepub.com by guest on February 14, 2015
218 American Review of Public Administration 45 (2)

 

(1) What are the powerful interest(s) (groups) in e-government?

(2) What are the fundamental dynamics behind the conflicts that are casually labeled as
moral conflicts in e-governance?

(3) What are the moral agendas of e-government interest groups?

(4) How are groups’ statuses shifting in the milieu of e-government?

(5) How can e-government ethics keep pace with the evolving and dynamic ethical prob-
lems of e-governance?

At this point, it is necessary to note that the scope of this article is limited in several ways.
First, the article cannot and does not solve the issue of what e-government “is.” The primary
concern is to convince that the idea of e-government 1s value-burdened and ideologically driven.
Second, this is not an in vivo observation and does not target, hence, cannot claim, a comprehen-
sive overview of all the disruptive implications associated with the use of technology in the
organizational setting. In the discussion of e-governance, defining organizational habitus is in
itself a trying task. Third, it is beyond the scope of this article to explore all the values and inter-
ests associated with all conceptual framings of e-government. There are obviously many fasci-
nating insights to be learned from a systematic review of the game of interests; not the least
intriguing being that such game is covert and private sector defined. Fourth, the discussion is
constrained to the organizational level. Finally, the article is principally captivated with delineat-
ing the questions of e-government ethics research and not with the identification or provision of
exacting answers or solutions. In many ways, then, this article can be interpreted as an extension
and application of the structural theory of ethical problems in organizations to the context of
e-government. The writing’s modest ambition doesn’t substantially extend beyond that of an
exploratory essay, and the analysis should be understood as such.

Beyond this introduction, the following discussion is constructed within three broad sections.
The first section will provide a brief overview of e-government and administrative ethics litera-
ture. The second section will analyze and discuss the ethical questions of e-government through
the framework provided by Chambliss (1996). In the concluding section, I will suggest a set of
broader inferences that warrant future academic attention.

The Silence of E-government Ethics

Although the idea of electronic government has entered the mainstream American academic lit-
erature in 1993, within the context of National Performance Review (NPR; Heeks & Bailur,
2007; Lenk & Traunmiiller, 2002), the concept has yet to cement a consensus around a specific
definition (Dawes, 2008; Hardy & Williams, 2011). It has been suggested that e-government is
nothing more than a new label for processes that have been part of governance for decades
(Gil-Garcia, 2012). Depending on the research contexts, scholars’ backgrounds, assumptions or
the function being emphasized—what e-government “is” will be governed by the context.

In broad terms, e-government is commonly defined as the use ICTs in governance (Dawes,
2008; Fountain, 2001; Maureen Brown, 2007). Some scholars make a clear delineation between
e-government and e-governance (Calista & Melitski, 2007; D’agostino, Schwester, Carrizales, &
Melitski, 2011); others discern between e-government and e-democracy (Backus, 2001; Lee,
Chang & Berry, 2011; Lenihan, 2005; Stahl, 2005); while on other occasions e-government is
merely broken down into functions such as e-service, e-organization, e-democracy, and
e-partnering (Carrizales, 2008). The difficulty in reaching a consensus over the definition of
e-government reflects, among others, the deeper underlying struggle for affirmation and indem-
nification within the field of study. For the scope of this discussion, e-government is defined as
the use of ICTs for purposes of governance, while e-governance can be broadly delineated as the
art of governance that emphasizes ICTs (Roman, 2013).

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 219

 

E-government research, “viewed as the offspring of information systems and public adminis-
tration . . . the child of two parents that are themselves perceived as intellectual weaklings”
(Heeks & Bailur, 2007, p. 261), is often criticized for the lack of methodological and theoretical
rigor (Coursey & Norris, 2008; Grénlund, 2005; Gronlund & Andersson, 2006; Hardy &
Williams, 2011). Although part of the criticisms, particularly in the case of failures to rely on
established theories, is undoubtedly justified, one would perhaps have to acknowledge that this
might be an inevitable dynamic for any set of literature that is still in its early stages of develop-
ment. In the last decade, however, the area has grown considerably (Scholl, 2009) and now pro-
vides research of noticeably improved quality.

Ultimately, the academic difference regarding the nature of e-government is rooted in the
tussle over values and the failure to accept politics as a dominant variable. There is an inherent
and troubling disagreement between e-government discourse and its policy design. The force
behind the e-government push was highly political in character and was originally driven by the
New Public Management (NPM)-inspired ideological belief that government was failing its cus-
tomers (Milakovich, 2012). The NPM-enthused language of NPR and subsequent policy formu-
lations were predominantly instrumental and targeted primarily organizational process flows
(Fountain, 2001); the “characteristic impetus of this ‘reform’ [NPM] movement was toward frag-
menting government organizational systems and strengthening the role of corporate sector actors
(including the IT industry) in providing government services” (Dunleavy et al., 2006, p. 84). The
latter is in sharp contrast with the claims of transformation and democracy normally made in
the political discourse and often taken for granted in the literature. The refusal to acknowledge
the inexorable ideological nuances of the concept, and the conceivably naive faith in technolo-
gy’s supposedly deterministic nature, have made progress in the conceptualization and operation-
alization of the concept rather difficult and slow.

The literature on administrative ethics, unlike the literature on e-government, is well estab-
lished and relies on a rich array of cross-disciplinary research. Philosophical inquiries and frame-
works about right and wrong have been omnipresent in the historical intellectual development of
humanity. Public administration scholars, in particular, have generated an extensive body of
quality literature on ethical considerations. While, according to T. L. Cooper (2001), administra-
tive ethics did not emerge as a significant field of study until 1970s, since then, the area has
developed with great determination. Rohr’s (1978/1989) discussion of public administrators
within the context of constitutional-driven values; Harmon’s (1974) examination of social equity;
Frederickson’s (1982) analysis on the administrator—citizen nexus; Hart’s (1984) support for vir-
tue as a core dimensions of public administration; and T. L. Cooper’s (2006) responsible admin-
istrator decision-making framework—are just a few examples of the seminal works available for
researchers to draw upon. In practice, administrative ethics is also heavily and effectively empha-
sized. Institutions such as the Organisation for Economic Co-operation and Development, United
Nations, the American Society for Public Administration, International City/County Management
Association, and National Institute of Governmental Purchasing regularly advocate and support
the development and acceptance of codes of ethics.

There is, nevertheless, a somewhat more limited academic attention to administrative ethics
within the context of e-government; some have even argued that there is a significant gap in the
extant literature (Mullen & Horner, 2004). Yet, despite that fact that neither established ethics
scholars nor e-government scholars dedicate regular attention to the topic, there is still a rather
adequate starting point for molding understandings and future research. For instance, Johnson
(1997) provides an encompassing discussion of possible ethical implications of computer-
mediated environments. Anderson’s (2004) suggests ethical codes as foundation for developing
an e-government ethics discourse (even given the codes’ intrinsic shortcomings). Berman and
Mulligan (1999) and Kapucu (2007) discuss privacy and confidentiality concerns associated with
e-government and the usages of automatically collected data. Mullen and Horner (2004), based

Downloaded from arp.sagepub.com by guest on February 14, 2015
220 American Review of Public Administration 45 (2)

 

Table I. Number of News Releases Discussing Ethics Within E-Government.

 

Year Number of stories

 

2000
2001

2002
2003
2004
2005
2006
2007
2008
2009
2010
2011

Total

wnobon ohm Oo OW WU

na
an

 

Source: Lexis-Nexis.

on van den Hoven (2000), delineate categories of ethical issues associated with e-government
(related, dependent, determined, and specific). Stahl (2005) warns about the ethical dangers of
framing e-government with a reliance on e-commerce perspectives. Finally, Ramadhan, Sensuse,
and Arymurthy (2011) support framing e-government ethics as a combination of computer, infor-
mation, and cyber ethics.

Overall, still, few writings discuss the ethical problems arising at the organizational level as a
result of e-government-driven transformation. The majority of scholarly literature either dis-
cusses ethics as one dimension of a broader set of implications from technology-induced shifts,
or treats the concept solely within the context of transparency, accountability, and security. Here,
it is of the essence to observe a vital nuance that differentiates the idea of e-government from the
mere adoption of ICTs. The concept of e-government is satiated with philosophical and ideologi-
cal connotations; e-government is, in a certain sense, a state of mind induced by belief that gov-
ernment should be improved within the transformational capacity provided by ICTs (Milakovich,
2012; Roman, 2013). Technological determinism is currently one central but untested assump-
tion that at times seamlessly operates throughout the majority of writings and practices in
e-government. The latter regularly leads to unfounded beliefs that the introduction of ICTs on its
own will guarantee improvements in administration. Technology is by no means insusceptible to
ideological manipulations. Hence, within this context, examining the ethics of digital governance
would call for a discussion of the broader assumptions behind e-government frameworks, with a
particular focus on political discourse. The fields of computer, information, and cyber ethics
provide avenues that public administration scholars can draw upon, but, given that e-government
is much more than “just technology,” this literature is obviously not sufficient.

Somewhat curiously, the debate in popular press about e-government ethics has been just as
quiet as the one in academia. There have been relatively few major news releases that discuss
ethics within the e-government context (Table 1).! Even fewer of these releases approach the
discussion on ethics in a manner that would fit the perspective suggested here.

Although in practice it is questionable whether e-government has yet to motivate any genuine
transformation and the evolution has been rather incremental (Milakovich, 2012; Norris & Moon,
2005; West, 2005); it is still perhaps merited to expect that it would eventually lead to authentic
shifts in governance and remapping of the public sphere. “The social and political dimensions of
communications innovations have always matured more slowly than the technology itself”

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 221

 

(Hindman, 2009, p. 129). If ICTs do indeed become the preferred tool of governance, then, I
argue that it is critical that we understand the nature of the ethical implications associated with
such developments; with those at the organizational level being of distinct interest.

The Critical Questions

Due to the complexity of the topic addressed here, it is suitable to place the discussion within a
specific frame. The structural theory of ethical problems in organizations (Chambliss, 1996), was
found appropriate for the purposes of this article. While, interpretations such as the one offered
by T. L. Cooper (2006) or Mullen and Horner (2004; the former focuses on decision making
while the latter specifically targets e-government ethical problems), would have been fitting as
well, Chambliss’s (1996) lens was chosen for several reasons:

(1) It specifically deals with ethical problems at the organizational level.

(2) The framework accounts for the behavioral bounds of the individual within the struc-
tures imposed by organizational routines, norms, and roles.

(3) The lens’ philosophical construct is based on conflict (for instance conflicts between
interests, images, or meanings advocated by different groups).

(4) The framework is flexible and was generated based on the study of a complex adminis-
trative environment. The methodological approach employed by Chambliss (1996)
allows important levels of generalizability.

(6) The framework deals with and explains decision-making dynamics within the psycho-
logical nexus created by, at times contradictory, personal and organizational values.

The structural theory of ethical problems in organization is constructed around five basic
principles of emergence of moral issues. It is within the frame provided by the five dimensions
that I will suggest the five critical questions.

Ethical Problems Reflect the Conflict of Powerful Interest Groups

What Are the Powerful Interest(s) (Groups) in E-Government? The ramifications of this question
might appear at first misleadingly petty. In e-government, interest groups are neither easily iden-
tifiable nor does there appear to be any genuine examination of their games, which might errone-
ously lead one to believe that they might not matter. There is a subtle assumption in formulating
the question in this manner. As Chambliss (1996) argues,

For the most part, ethical problems in organizations do not spring full-blown from the independent mind
ofa single practitioner, struggling with her lone conscience. They tend, rather, to be specific manifestations
of well-articulated disputes between interest groups—typically, between rival professions or
constituencies. (pp. 94-95)

At the organizational level, within the framework of e-government, there are at least six broad
interest groups that easily come to mind: frontline public servants, public managers, political
appointees, consultants, contractors, and IT professionals. A variety of other interest groups pop-
ulate the organization’s external environment. It is now common for private IT companies, for
instance, to build, develop and manage entire sets of governmental IT functions (Dunleavy et al.,
2006), hence, giving them intimate proximity to the organizational space. Moreover, given the
intrusive character of ICTs as well recent administrative developments, it becomes somewhat
difficult to argue that any interest or interest group finds itself completely isolated from the
immediate organizational milieu.

Downloaded from arp.sagepub.com by guest on February 14, 2015
222 American Review of Public Administration 45 (2)

 

Each group has its own interpretations, culture, preferred moral obligations, and understand-
ings of what public service is; and it is highly likely to attempt to enforce those views in shaping
the e-governance discourse and implementation. Depending on available resources or common-
alities in trained groups’ perspectives, interests can align in support of a specific expectation of
e-government. For instance, managers might emphasize efficiency and standardization as the
core directions in the adoption of e-government (Fountain, 2001; Milakovich, 2012); while
street-level public servants might prefer responsiveness and flexibility. It is quite possible that the
outcomes of these conflicts would fit within a power law pattern—the dominant groups will
shape the bulk of the terms. The nature of e-government could dramatically change the underlin-
ing dynamics that make an interest group powerful within its medium (Hindman, 2009), so there
is much left to be learned here. Thus far, it would appear that an elite few of private sector players
such as IBM, EDS, Cap Gemeni, and Lockheed Martin hold the edge in terms of knowledge,
expertise and ability to frame and define the operationalization of e-government platforms
(Dunleavy et al., 2006).

It should be noted that it is unclear whether users (consumers) of e-government services
could be considered an interest group. Do the users make an interest group or are they simply
individuals separated by the successful fulfillment or automation of a onetime need? With few
exceptions, there is little to suggest that the drive for e-government has been motivated by a
demand side push. Moreover, scholars have not been able to provide conclusive answers in
terms of who are the most active participants within the e-government context (Brainard, 2003;
Elwood, 2006; McCall, 2003; Schlossberg & Shuford, 2005) or how digital participation
affects broader democratic constructs. Genuine democratic decision making and accountabil-
ity might have to rely heavily on authentic deliberation (Gutmann & Thompson, 2004); yet,
there are many who would openly question whether e-government can realistically nurture
meaningful deliberation. In terms of political engagement, Prior (2007), for instance, finds that
Internet use increases political knowledge among the already active individuals, while apathy
levels rise among the rest. Those with strong party affiliations are more likely to visit web
political portals and engage in digital dialogue (Foot & Schneider, 2006). As Hindman (2009)
perceptively notes,

Paradoxically, the extreme “openness” of the Internet has fueled the creation of new political elites. The
Internet’s successes at democratizing politics are real. Yet the medium’s failures in this regard are less
acknowledged and ultimately just as profound. (p. 4)

En masse, then, a question that appeared trifling at first, uncovers issues that are rarely
addressed by e-government ethics research or by studies of e-governance in general. Politics is
an open secret of e-government that should not be ignored as many instrumental-rational per-
spectives would otherwise suggest. Leaving politics and conflict out of e-government guaran-
tees that any understandings, which are eventually constructed, will be quixotic in character
and of little use to the concerns of practice. Clearly delineating the powerful interests and
interest groups and the dynamics that make them powerful becomes indispensable for generat-
ing non-naive understandings of the ethical implications of e-government. Making an allow-
ance for the fact that “old” interest groups can be much more adroit at exploiting technology
for their goals (Kakabadse, Kakabadse, & Kouzmin, 2003) it becomes difficult to imagine
“new” interests finding wiggle room on the front stage of organizational power structures and
policy making.

Indeed, the Internet may be better suited to furthering organized and organizational interests rather than

the citizens in unorganized or informal sectors that many Internet optimists wish to empower . . . The
Internet appears to foster and intensify closed, corporatized policy networks. (Rethemeyer, 2007, p. 199)

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 223

 

Fundamental Conflicts Between Groups Become Labeled as Moral Conflicts

WhatAre the Fundamental Dynamics Behind the Conflicts That Are Casually Labeled as Moral Conflicts
in E-Government? Technology advancements create policy vacuums (Moor, 1998). Certain orga-
nizational interactions, which are now common under the increased reliance on technology,
might not have occurred before.

Initially, there may be no clear policies on such matters. They never arose before. There are policy
vacuums in such situations. Sometimes it may be simply a matter of establishing some policy but often
one must analyze the situation further . . . One often finds oneself in a conceptual muddle . . . The
evaluation of a policy will usually require a close examination and perhaps refinement of one’s values
... Of course, with the discovery of new consequences and the application of new technology to the
situation, the cycle of conceptual clarification and policy formulation and evaluating may have to be
repeated on an ongoing basis. (Moor, 1998, pp. 16-17)

The issue of the digital divide is a case in point. One of the overarching scopes of e-government
was to improve democratic outcomes by harmonization of the relationship between agencies and
its constituencies through increased access and participation in decision making of other previ-
ously marginalized groups. Yet, active and consequential participation in governance, one that
goes beyond downloading a form or paying taxes, requires time, financial investments, but above
all—suitable levels of education. Availability and access alone do not secure the outcomes
expected by design (Colleste & Holmqvist, 2004; Tapia & Ortiz, 2010). Some have even sug-
gested that the skills needed for meaningful use of the Internet are more constraining than access
(Dijk, 2005; DiMaggio, Hargittai, Ceste, & Shafer, 2004; Mossberger, Tolbert, & Stansbury,
2003). There are hopes that rapid expansion of broadband access via mobile phones will amelio-
rate the condition; yet, increased mobile access does not modify the underlying construction of
the issue, which remains one of knowledge.

Furthermore, new ethical challenges are guaranteed to arise with the increased use of new digital
approaches. For instance, the use of Twitter or Facebook? to communicate and interact with citizens
might often overlook the reality that the assumption that the addressees will actually read the mes-
sage cannot readily be upheld. Ironically, without appropriate support mechanisms, by its nature,
e-governance might offer the least amount of benefits to those the most in need of them and in its
unpredictable evolutions could become increasingly dividing and disconnected from citizens.

At the other end of the spectrum, it has been argued that “public participation which is auto-
matic, unrestrained, or ill-considered can be dangerously dysfunctional to political and adminis-
trative systems” (Cupps, 1977, p. 478). As an example, Kerwin and Furlong (2011) have
suggested that e-rulemaking might threaten public agencies’ ability to perform effectively.
Contradictory to the telos of e-rulemaking, the combination of administrative constraints and
increased ability of interest groups to access rulemaking might create the perverse incentive for
agencies to become less concerned with public input and shop, implicitly and explicitly, for sup-
portive stakeholders. Hence, framing the issues associated with e-rulemaking or the digital divide
within technical connotations, cannot hide the fact that the question is still a political interpreta-
tion of who and how “should” participate. At heart, the design and implementation of e-government
is a political exercise of shaping definitions, values, and worthiness. “What could be described as
political arguments or turf battles are translated into moral terms and become ‘ethical problems”
(Chambliss, 1996, p. 95). Deconstructing the ethical problem of the digital divide and active
participation into political components helps us see that its answer does not entirely rest on eth-
ics, but significantly so on policy.

Internet politics seems to nurture some democratic values at the expense of others. If our primary concern
is the commercial biases of traditional media organizations, or the need for a strong corps of citizen

Downloaded from arp.sagepub.com by guest on February 14, 2015
224 American Review of Public Administration 45 (2)

 

watchdogs, then online politics may indeed promote positive change. Yet it is crucial to remember that
democratic politics has other goals, too. No democratic theorist expects citizens’ voices to be considered
exactly equally, but all would agree that pluralism fails whenever vast swaths of public are systematically
unheard in civic debate. The mechanisms of exclusion may be different online, but . . . they are no less
effective. (Hindman, 2009, p. 12)

This second critical question, too then, veils an intricate set of consequences that is often,
somewhat curiously, overlooked in e-government ethics research. E-governance is a heavily
value-laden construct. As it is the case with the ethical implications of citizens’ participation in
governance, what might appear at first to be a technological issue, is undeniably a reflection of
the fundamental conflict in interpretations of the normative nature of 21st century governance.
In describing the ethical challenges of ICTs in governance in technical terms, we are, as it were,
attempting to answer political questions with programing codes. E-government ethics are inex-
tricably bound to competing interests. Constructing ethical suggestions outside the latter real-
ization, would probably lead to overoptimistic and marginalizing views, mechanisms, and
frameworks.

Ethical Issues Reflect Conflicts of Groups’ Moral Agenda

What Are the Moral Agendas of E-Government Interest Groups? Organizational groups differ
significantly in their moral agendas. Each group will attempt to force “their ethics” in making
ethical decision in e-government. Clashes between their competing and at variance sets of
values and moral agendas can lead to challenging ethical issues. Frontline public servants often
have sensibly different professional moral agendas (Chambliss, 1996; Maynard-Moody &
Musheno, 2006; O’Leary, 2006) than those of managers or political appointees. Most impor-
tantly, however, is that the culture and professional moral agendas of IT developers, most of
whom are employed by the private sector, can be dramatically at odds with the values of public
services.

Professional groups have their own languages, their own ways of doing things, and their own
understanding of the world-what is generally called a ‘culture.’ Bureaucratic culture can sit uncomfortably
with the individualistic, heroic culture of the programmer and the faddish culture of the management
consultant and New Public Management-influenced managers. (Goldfinch, 2007, p. 923)

Public servants are morally committed to serve the “public interest” and might resist or
sabotage anything (e.g., technology) that interferes with their goals and beliefs (O’Leary,
2006). Managers are largely morally committed to balancing organizational needs and
upholding broader management and administrative values (T. L. Cooper, 2006); which might
motivate them to embrace e-government in terms of efficiency and productivity, rather than
accountability and responsiveness. Political appointees are morally committed to political
constructs and are highly likely to try to shape e-government implementation with intent of
controlling bureaucratic activity and discretion. In contrast, Dunleavy et al. (2006) suggest
that the IT profession cannot be characterized as having strong rules on ethics or responsi-
bilities to the client (p. 37). As such, information systems (IS) developers might fail to be
morally committed to any public service dimension and only care about the performance and
profits.

The profession of ISD [information systems development] is characterized by specialized training and

circumscribed theorizing. Since the dawn of business computing, training in IS has meant ‘computer
training’, and IS professionals remain technologists at heart. (Lyytinen & Robey, 1999, p. 94)

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 225

 

Yet,
These system-level bureaucrats have the discretionary power to convert legal frameworks into concrete
algorithms, decision trees, and modules. They are constantly making choices-which definitions should
be used, how should vague terms be defined, how are processes to be designed and interlinked?
Therefore, just as the street-level bureaucrats were not in their time docile policy implementation robots,
but policy makers them-selves . .. Who checks the developers and their systems? To whom are they
accountable for the manner in which they have converted analogue legislation into digital decision trees,
scripts, and algorithms? (Bovens & Zouridis, 2002, pp. 181-182)

The complexity of the condition dangerously extends beyond the conflicting moral agendas.
Technology has been shown to have dehumanizing and deskilling effects. Systems that are built
to support decision making can turn into data dumping grounds with the end user’s job reduced
to data collection, devoid of a meaningful understanding of the process or the sense associated
with such data (Fountain, 2001). Scholars have previously suggested that individual decision
making in cyberspace might employ different moral constructs (Johnson, 1997; Johnson &
Miller, 2009; Mullen & Horner, 2004). Similar actions can be undertaken in both environments;
yet, as a rule, the moral character of those actions will differ in important ways (Johnson &
Miller, 2009). Furthermore, Roman and Miller (2013) caution that a more detailed evaluation of
the design of e-government might uncover a refined attempt to reinforce technical-rationality in
public administration. In terms of public organizations, the substitutions of person-to-person
interactions with ICT-based technical-rational interfaces might lead to unintended shifts in
moral agendas. The implications are incremental and might not become immediately obvious,
yet in aggregate, and in the long run, they could become quite dramatic. With possibly diluted
levels of personal accountability, due to adoption of digitally supported decision-making sys-
tems, the dilemma of “dirty hands” might become a dilemma no more, since the moral guilt
could be shifted to the “system.” In this context, due to psychological distance and process
ambiguousness using Thompson’s (1987) logic, neither democratic responsibility nor account-
ability develops into a predominant ideal. The implications are indeed serious with a potential
for the disastrous. After all, if one would accept Adams and Balfour’s (2004) controversial argu-
ment in its entirety, then, moral vacuity is the price to be paid for worshiping of technical-
rationality. “Within a culture of technical rationality, a model of professionalism that drives out
ethics and moral reasoning offers all too fertile soil for administrative evil to emerge” (Adams
& Balfour, 2004, p. 41).

Recall Philippa Foot and Judith Jarvis Thomson’s classical runaway trolley moral dilemma.
For the first scenario assume the role of an onlooker standing on a bridge. You see a trolley car
with no brakes racing down the track on which five unaware workers are standing. Unless you do
something, they will die. Next to you there is, however, a button that you can press (click) to
divert the trolley car onto a side track on which only one worker is standing. It is up to you to
decide to press the button to save the lives of five workers by condemning the one standing of the
side track. In the second scenario, assume no side track and replace the button with a heavy indi-
vidual. You still can save the five workers. This time, though, you would have to push the indi-
vidual onto the path of the trolley, thus blocking it from reaching the workers.

The decision-making dynamics involved in the first scenario appear reasonably simple, sacri-
fice one innocent life to save five. Nonetheless, the same principles feel out of place under the
second setup. Notwithstanding the usual debate regarding the responses and difference between
these two settings, there is one important technology—decision-making nexus that should be
addressed here. There is something inherently different about moral decision making under the
two scenarios. Under the first one, pressing (clicking) the button is somewhat less personal than
pushing the individual off the bridge; there is a psychological distance between the actor and the

Downloaded from arp.sagepub.com by guest on February 14, 2015
226 American Review of Public Administration 45 (2)

 

immediate core of the developments. Intimate contact with the action, actors, and consequences
appears to trigger a more complex set of reactions. According to the dual-process morality theory
(Green, 2003, 2009; Green, Morelli, Lowenber, Nystrom, & Cohen, 2008; Green, Nystrom,
Engell, Darley, & Cohen, 2004; Green, Sommerville, Nystrom, Darley, & Cohen, 2001) deonto-
logical moral judgments are based on automatic emotional responses while consequential moral
judgments (utilitarian) are based on more detailed and controlled cognitive processes. As per
dual-process morality theory, the reason for such a significant difference in reaction under the
apparently similar scenarios is due to the involvement of two distinct psychological/neural sys-
tems; one more emotional (associated with the anterior cingulated cortex) and another more
controlled and less emotional (associated with anterior regions of the dorsolateral prefrontal cor-
tex). When the emotional system is engaged (second scenario) it regularly dominates consequen-
tial reasoning. The conditions would have to be made sufficiently attractive in order for
consequential reasoning to overtake the emotional response.

In simple terms, when the decision maker is undertaking the action through an emotional filter
(e.g., button, web link, ICTs), thus becoming emotionally detached and psychologically distant
from the action, he or she is more likely to employ utilitarian logic. The transformation is subtle
and happens at the neural level. It is most likely uncontrolled and could perhaps even go unno-
ticed by the individual; yet, en masse the accumulation of these seemingly minor alterations in
moral reasoning is crucially consequential. Traditionally for changes to be acknowledged as
historical, they had to be accompanied by falling walls and overthrown governments. ICTs might
be changing that by making scale transformation almost unremarkable and dull. The slope in this
evolution could become very slippery; ICTs might indeed seamlessly motivate major modifica-
tions in the normative nature of public administration. The implications of the possible dehuman-
ization of public administration are legion; not the most trifling being that it “often masks
administrative evil” (Adams & Balfour, 2004, p. 19).

When Group Statuses Shift, Ethical Problems Increase

How Are Groups’ Statuses Shifting in the Milieu of E-Government? It has been suggested that digital
governance requires a “new” organizational culture, one that supports and aligns with the tenets
behind e-government (Torres, Pina, & Royo, 2005). With the context of the invasion of the orga-
nizational space IT systems, “The Weberian concept of government organization as a self-
contained, socio-technical system where agencies are defined by their in-house operations and
technology no longer seems adequate” (Dunleavy et al., 2006, p. 15).

Every technology also requires the inculcation of a form of life, the reshaping of various roles for
humans, the little body techniques required to use the devices, new inscription practices, the mental
techniques required to think in terms of certain practices of communication, the practices of the self
oriented around the mobile telephone, the word processor, the Word Wide Web and so forth. Even in its
conventional sense, then, technologies require, for their completion, a certain shaping of conduct, and
are dependent upon the assembling together of lines of connection amongst a diversity of types of
knowledge, forces, capacities, skills, dispositions and types of judgment. (Rose, 1999, p. 52)

The ability and skills associated with the use of Internet based digital platforms have been
shown to be critical for contemporary policy making. Johnson & Miller (2009) argues that ICTs
can lead to an “instrumentation of human action”; while Fountain (2011) expects a rationaliza-
tion and bureaucratization of public governance as a result of e-government adoption.

In sum, the use of Internet in bureaucracy is likely to lead to greater rationalization, standardization, and

use of rule-based systems. The rules, may not be visible because most of them will be hidden in software
and hardware. But they will remain and may increase in power. (Fountain, 2001, p. 62)

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 227

 

Although public administration scholars have addressed the possible changes in organiza-
tional cultures and expectations as a result of e-governance; how group statuses shift receives
noticeably less attention. With the increased use of ICTs for purposes of governance the status of
frontline public servants is expected to be transformed and perhaps significantly diminished.

The sheer dynamism caused by the introduction of computers affected both the organization of the
street-level bureaucracy and the under-lying legal setup. In a relatively short period of time, the street-
level bureaucracy has changed into what we could call a screen-level bureaucracy ... Many decisions
are no longer made at the street level by the worker handling the case; rather, they have been programmed.
into the computer in the design of the software... a number of major executive organizations have
progressed even further and are rapidly developing into what could be termed system-level bureaucracies.
(Bovens & Zouridis, 2002, p. 177)

Technology has the potential to transform a sector in a matter of months making one’s process
knowledge and learned skills almost obsolete. Im (2011) suggests that some job types might
disappear (e.g., data collectors) while others might come in higher demand (e.g., managers).
More critically, ICTs might have deskilling and dehumanizing effects on users (Fountain, 2001).
It has also been suggested that as a result of e-government, public servants could experience
increased pressures and work demands, but also shifts in job roles (Norris & Moon, 2005).
“Today, a more true-to-life vision of the term “bureaucracy” would be a room filled with softly
humming servers, dotted here and there with a system manager behind a screen” ( Bovens &
Zouridis, 2002, p. 175).

Public administrators often develop personal and emotional ties with the citizens that they
serve (Lipsky, 1980; Maynard-Moody & Musheno, 2006) or with certain missions or ideals
(O’Leary, 2006). Knowledge and regular direct involvement in interpreting public values and
public needs, empower public servants and provide them with the basis to question the moral
preeminence of otherwise taken for granted organizational practices. Yet, within the context of a
decreasing status, it is highly likely that accountability and moral responsibility will be delegated
to the realm of technology. Automation and standardization at one level of decision-making pro-
cess will most likely lead to formalization in the following steps (Bovens & Zouridis, 2002).

Limited discretion, while theoretically desirable in a principal-agent model, might lead to
disinterested and morally detached agents. For instance, as frontline public servants take upon
less decision-making responsibility they could minimize or even stop their typical challenges of
interpretations that come from “the top.” Furthermore, failure to ethically connect with adminis-
trative decision making might lead to a moral collapse, an inability to diagnose or understand the
real implications of one’s actions. Ethical deliberation is a dynamic process that demands per-
sonal involvement; yet, human judgment is often found to be normatively objectionable under
the design of e-government infrastructures as it is believed to muddle the process.

Chambliss (1996, p. 97) argued that ““professionalization’ in part describes a shift from a
technical to a moral orientation to one’s work. As power goes up, so does responsibility. Hence,
the increase of ethical conflicts.” In the case of ICTs, however, the trend might direct changes
toward deprofessionalization. A weakening of public administrators’ voices in governance could
make frontline public servants, even managers, less willing to embrace moral responsibility and
make them susceptible to administrative apathy. “[D]igital rigidity . . . reduces the responsive-
ness of public administration and hence, undermines the legitimacy of governance” (Bovens &
Zouridis, 2002, p. 182).

Perhaps more critically, within the context of shifting statuses, the presence of technology
could stimulate what Milgram (1974) calls an “agentic shift,” where the public servant renounces
personal and social responsibility for the substance and implications of one’s decisions. “It’s on
the web” or “I don’t make the decisions” can easily become habitual and acceptable answers to
citizens’ inquiries. Scholars have already noted cases when the digitalization of administrative

Downloaded from arp.sagepub.com by guest on February 14, 2015
228 American Review of Public Administration 45 (2)

 

processes led to decreased accountability levels on the part of public administrators (Romzek &
Johnston, 2005). “The ethical framework within a technical-rational system posits the primacy of
an abstract, utility-maximizing individual while biding professionals to organizations in ways
that make them into reliable conduits for the dictates of legitimate authority, which is no less
legitimate when it happens to be pursuing an evil policy” (Adams & Balfour, 2004, p. 153).

The decreasing role of in-house design and implementation of government ICT systems has
led to the fact that “the capability for defining and developing these critical systems now lies
outside the competence of public officials” (Dunleavy et al., 2006, p. 5). Private IT firms and IT
developers now have access to and control the flow of a large part of knowledge that was previ-
ously the sole domain of government. “These firms often seem to monopolize (or are allowed
even encouraged to monopolize) the necessary expertise and organizational capacities to service
and develop the very large-scaled government systems of big nations states” (Dunleavy et al.,
2006, p. 5). Holistically, then, the private sector, through the assumptions built into the design of
the digital platforms, becomes the invisible architect with control over nature of e-government.
With decreasing levels of IT expertise and limited ability to act as “intelligent customers” it is
difficult to envision how public agencies would be able to monitor the ethical discipline of IT
contractors and imported digital platforms. In practical terms, the impacts of the shifting of power
toward IT contractors should not be underestimated. In their excellent analysis of the Netherlands,
Canada, United States of America, New Zealand, Japan, Australia, and United Kingdom,
Dunleavy et al. (2006) conclude that “industrial power for IT corporations is a more important
negative influence on government IT performance than the public management influences”
(pp. 130-131). “How do we,” then, “protect liberty when the architectures of control are managed
as much by the government as by the private sector?” (Lessig, 2006, p. xv).

The decreased sensitivity of e-government design to political will, due to private sector
monopoly over knowledge, will undoubtedly lead to lower levels of publicness. The importance
of questions such as how it works or what are the nonmonetary costs—will surely soon be
drowned by the noise from the fact that it works. The automatization of any process, regardless
of its importance and complexity, will doom the process to the backend of organizational struc-
tures and will starve it of any meaningful attention (Dunleavy et al., 2006). The estrangement
between individual responsibility and organizational conduct, which is now expressively deter-
mined by routine failures to fully understand informational systems, can eventually lead to what
Thompson (1980) names the problem of the many hands. Because many different parties, both
public and private, partake in varying ways in the decisions related to e-governance processes,
but with few of them clearly grasping the impact of own inputs, it becomes difficult, if not impos-
sible, to identify who is morally responsible for a given outcome.

Here, it should be considered that within the e-government context, the shifts in groups’ sta-
tuses are acerbated by several important social dynamics. First, the antigovernment rhetoric of the
last four decades as well as a continuous supply of public and private scandals have led to histori-
cally low levels of trust in government (P. J. Cooper, 2009; Sachs, 2011). If in 1960s Americans
would have been shocked by the idea that public servants or government might have lied to them
(Bok, 1978/1999); currently, such revelations would have a hard time triggering anything more
than a belabored fake surprise. Second, as a result of the privatization/reinvention/outsourcing/
transformation type efforts, current governance exhibits high levels of devolution, which con-
straints the power of any specific group, but also makes accountability difficult to trace (Kettl,
2002, 2009). Third, easy access to information (regardless of it quality or relevance) has made
challenging the legitimacy and professionalism associated with administrative decision much
easier, but also fashionable. Finally, in the mist of historically unprecedented budget constraints
and within the context of a newly coined mentality of “where is my bailout?” public administra-
tion will again be expected to deliver more with much, much less (Milakovich, 2012). In reality,
however, the only thing that government can credibly do with less—is less (Bozeman, 2007).

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 229

 

Ethical Problems Change Over Time

How Can E-Government Ethics Keep Pace With the Evolving and Dynamic Moral Problems of
E-Governance? Caught up in today’s technological swirl, it is easy, and maybe even tempting, to
forget that until 1993, the ability to reach global audiences was in the realm of a privileged few.
In many ways, technology no longer constrains our imagination; it is our imagination that might
be falling behind.

Computers are logically malleable. This is the feature that makes computers so revolutionary. They are
logically malleable in that they can be manipulated to do any activity that can be characterized in terms
of inputs, outputs, and connecting logical operations. Computers can be manipulated syntactically and
semantically. Syntactically, a computer’s performance can be changed through alterations in its program.
And semantically the states of a computer may represent anything one chooses from the sales of a stock
market to the trajectory of a spacecraft. Computers are general purpose machines like no others. (Moor,
1998, p. 15)

Castells (2000) suggest that a virtual enterprise, within a network society, develops its own
culture; one that it is not new in the traditional sense, but one that is constantly changing.
Therefore, it is change and not stability that dominates the interpretation and meaning of
e-government related ethical problems. According to Moor (1985), technology innovation will
continue to outpace the understanding provided by extant ethical frameworks; as such, in some
sense, ethics might always be a step behind technology. Continuous and remorseless organiza-
tional change, however, might cause high levels of anxiety and a struggle to uphold even a mini-
mum level of ethical integrity (Sennett, 1998).

In just two decades the rate of use of ICTs in governance has reached great proportions. In
many developed and developing countries it is now almost impossible to imagine certain services
outside e-government frameworks (Milakovich, 2012). Efficiency, financial and political pres-
sures would lead agencies to commit or transition from one digital platform to the next before the
benefits or implications of such transformations could be satisfactorily understood. Unlike some
other areas of applied ethics, e-government ethics does not have the luxury of relying on own-
established structures, and it seldom, if ever, can afford being reactive.

Notwithstanding the apparent unpretentiousness, this last question may turn out to be the most
acute. Due to its normative connotation and broader professional consequences, establishing
ethical frameworks is critically difficult and time consuming. For instance, until recently, there
was no code of ethics in economics, nor was there necessarily a genuine intention to develop one
(Casselman, 2012). Within e-government adoption, the developments sometimes happen much
faster and they are probably more difficult to predict than in economics. There are few lagging or
leading indicators, while most of the consequences are in real time. E-government, as a construct,
is sufficiently complex and often misunderstood, which would make certain substantive and
tangible changes in administration “go unnoticed” for decades. Thus, it is crucial to identify the
approach that would allow e-government ethics become a practical “‘in-time” consideration to
accompany all future developments in e-governance. Although the actual nature of an arising
ethical problem might be difficult to foresee; delineating the organizational origin of such con-
flicts might be more manageable.

Chambliss (1996) argues ethical problems faced by professional groups are neither random
occurrence, nor are they individual dilemmas faced by particular individuals.

They are, rather, structurally created and occur in bulk. They arise when the goals of two professions
clash, or when occupational groups have different motives, or when “the system”—probably at some
point definable as a field of interest groups—thwarts the efforts of certain people to do what they see as
their jobs. (pp. 116-117)

Downloaded from arp.sagepub.com by guest on February 14, 2015
230 American Review of Public Administration 45 (2)

 

Certainly, not all ethical conflicts happen at the group or organizational levels. Administrators
will continue to struggle between the manifold obligations imposed by professional, administra-
tive and social contexts. What I argue here, then, is that the nature of e-governance raises ethical
conflicts that are constantly changing and routinely fall outside the decision-making power of the
individual public servant.

Ethical issues are not a mere competition of ideas; they are a competition of people, who have their
various goals and methods. They represent real problems in organizational action, constrained by legal,
economic, social, and personal peculiarities. Education, sensitivity, and awareness may marginally
affect political alignments, but ethical problems are not solvable by changing peoples thoughts. The
problems are not inside people s heads. (Chambliss, 1996, p. 118, emphasis added)

Conclusions

Public administration scholars have suggested that delineation of the “big” questions is critical
for cohesive and practical knowledge creation within any discipline (Behn, 1995; T. L. Cooper,
2004; Denhardt, 2001; Kirlin 1996, 2001; Neumann, 1996). According to Behn (1995) research
inquiry should not start with methodology or data; it should start with questions. In the spirit of
such calls, this article suggested five critical questions that future research on e-government eth-
ics can build upon. I am not arguing that these are the only possible questions or that this is the
most appropriate manner of framing them. I expect them to be challenged on the basis that they
are at the mercy of the theoretical lens that I chose to employ to guide my narrative. Framing
critical questions, like anything else, can be done badly. My failures are nothing but a confession
of the limits of my knowledge, training, and resourcefulness. If it accomplishes nothing else, this
discussion is valuable if tt pushes other scholars to critique it and attempt to refute what has been
argued. What is of import here, is for the posed questions to stimulate reexaminations and scru-
tiny of perspectives and taken for granted assumptions. Above all, this writing seeks to motivate
future research.

One of the great challenges of e-government ethics is that we often identify technology as
being deterministically positive in nature and immune to politics or rhetoric. This is very much
not the case. Let’s not forget that cyberspace (cybernetics and space) first emerged within the
science fiction literature and, contrary to today’s broad positive connotation, was used by Gibson
(1984) to relay the concern for the dehumanizing effect of permeating and intrusive technology.
Some might even argue that there is more “1984” in technology than there is liberty (Lessig,
2006). Research is starting to show that the original claims that equated digitalization of gover-
nance with democracy, liberty and equality sound particularly hollow when examined in practice.
There is a familiar interdependence between political narratives and the use of ICTs for purposes
of governance. With the digital space, “who speaks and who gets heard . . . [are] . . . two separate
questions. On the Internet, the link between the two is weaker than it is in almost any other area
of political life” (Hindman, 2009, p. 17). Moreover, technology impacts moral decision making
at the organizational level in ways that we have yet to adequately understand. Extreme digitaliza-
tion leaves little valuable room for contextual sensitivity and can sustain unreasonable degrada-
tion to moral responsibility (van den Hooven, 1998).

There is a definite link between moral judgment and intuition or “gut feelings” about right or
wrong (Green, 2003; Haidt, 2001; Maynard-Moody & Musheno, 2006), but how technology
impacts our “gut feelings” remains to be understood. The association between intuition and the
use of ICTs, both in terms of its social construction and neural dimensions becomes critical.
Moral dilemmas in which the participant is actively involved trigger greater activity in brain
areas that are associated with social cognition and emotions (Green et al., 2001; Green, 2003).
Whether pushing the individual of the bridge is more “personal” (Green et al., 2001), it is

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 231

 

intentional, direct (Moore, Clark, & Kane, 2008) or it involves physical contact (Cushman,
Young, & Hauser, 2006)—what should be considered here is that—detachment from the “vic-
tim” leads to a more consequential moral reasoning. If indeed technology affects individuals on
a neural level and creates a more detached public administrator, this could set the grounds for a
number of moral and responsibility vacuums, which warrants significant attention. Sandel (2009)
argues that “moral reflection is not a solitary pursuit but a public endeavor. It requires an
interlocutor—a friend, a neighbor, a comrade, a fellow citizen” (p. 29). Whether ICTs can be
successfully shaped into an interlocutor for purposes of moral decision making remains to be
seen. Research, unfortunately, has shown an increasing and constant disengagement from politics
among citizens, especially within the youth, (Macedo et al., 2005). Thus far, ICTs appear to have
failed to make a difference. “Scholars, public officials, and journalist have paid a great deal of
attention to online politics. Citizens themselves, though, have directed their attention elsewhere”
(Hindman, 2009, p. 81).

The discussion in this article does not pretend to encompass the emergence of all possible
ethical issues within e-government-shifting organizational contexts; but, it does suggest what I
believe to be some major ones. There is surely more to e-government ethics than a lesson on
trivial organizational group conflicts; it is certainly much more than that. The primary weakness
of this article, then, is that, while it explores, it does not fully answer any of the questions that
it raises. Even so, in congruence with its exploratory telos, the article might provide an original
perspective within which to construct an understanding of the ethical challenges raised by
e-government diffusion. Specifically, it is suggested that there is an abundance of “unmasked
interests” in the infrastructures of e-government. Also, ICTs affect administrators at the psycho-
logical level by inducing different patterns of moral reasoning; an idea that has been greatly
underexplored by public administration literature. Furthermore, it has been argued that
e-government might motivate fundamental shifts in power alignments within organizational
interest(s) (groups). Although, it is still premature to clearly determine the specific nature of the
power remapping, it is certain that this would lead to an increased number of ethical problems.
To minimize the ramifications of such dynamics it becomes imperative to generate a framework
to deal with the potential rise of this new set of ethical challenges. Developing a code of
e-government ethics, although necessary, will not suffice. “[C]odes of ethics function all too
often as shields; their abstraction allows many to adhere to them while continuing their ordinary
practices ... .The codes must be but the starting point for a broad inquiry into the ethical quan-
daries encountered at work” (Bok, 1978/1999, p. 246).

On the whole, the questions suggested here are not necessarily critical because they are some-
how inherently unique, but rather because we often, perhaps conveniently or even by design,
forget to ask them of e-government. Simply put, useful understandings cannot be constructed
without posing the appropriate questions. Each of the five questions is researchable and would
provide many insights that would be of high utility to practice. In each case researchers can draw
on established literature from other areas, with the one related to the technology-induced psycho-
logical changes being of greatest research appeal. Collectively these questions place conflict,
interest groups, politics, and power of organizational structures in the discussion of the ethical
problems of e-government. They undoubtedly might have multiple answers and these answers
might be diachronic or contextually dependent. Yet, this does not mean that these questions are
not worth asking. The omnipresent complexity of e-government adoptions cannot serve as an
excuse for leaving the ethical implications of e-government design open to naive claims of tech-
nological determinism. Ironically, continuously posing critical questions, thus motivating a dia-
lectical awareness, is probably as important as answering them.

When dedication to ethics is more in rhetoric than in action, what purports to be a genuine
effort for transformation may mask contempt with status quo and an unsupported faith in tech-
nological determinism. There is a dangerous fallacy in thinking that administrative decisions

Downloaded from arp.sagepub.com by guest on February 14, 2015
232 American Review of Public Administration 45 (2)

 

are ethically autonomous from the structures of ICTs. The vital part, sought in this article, is to
get practitioners and scholars thinking about the nontriviality of interests and moral agendas
within the e-government-ethics nexus. By design, the “e” in e-government does not, I submit,
ineludibly stand for ethics,? although considerable thought should be given to it. There is, no
doubt for many reasons, nothing glorious about routine functions, hence, their logical digitali-
zation; yet, the routine blurs the serious implications of the long term amassed consequences.
When decisions cease to absorb moral engagement and operate in a predetermined and predict-
able manner, such as automated decision making, concerns about ethics will perhaps move
downward the organization’s priority list. If e-government is to eventually become the gover-
nance medium of choice, ethics cannot be treated as a passing fancy, it needs a genuinely
unremitting vigilance.

Declaration of Conflicting Interests

The author declared no potential conflicts of interest with respect to the research, authorship, and/or publi-
cation of this article.

Funding

The author received no financial support for the research, authorship, and/or publication of this article.

Notes

1. The results were obtained by searching in the Lexis Nexis database for—ethic! W/50 e-govern! OR
digital govern! OR e-democracy OR e-participation OR e-rulemaking and #GC343# within “all news”
category. This search finds documents where either variations of “e-govern” [e.g., e-government,
e-governance], variations of “digital govern” [e.g., digital governance, digital government], e-democracy,
e-participation, or e-rulemaking are within 50 words of “ethics” (or its variants) within news releases
in United States. Employing a maximum of 250 words would not change the results significantly, but
the link between the discussion of ethics and e-government would be rather weak. The final count
includes only newswire and press releases, newspapers, magazines and journals and news transcripts.
In addition, publications in which the discussion on ethics was peripheral to e-government discus-
sion were not considered. This approach is far from perfect in terms of gauging the level of attention,
especially given some level of bias that is unavoidably introduced by training into the coding process;

alone,

1

however, the results still offer an interesting contrast. For instance, searching for “e-govern
produces thousands of hits.

2. The case can be made that any social media platform, used for purposes of interaction, information
exchange and collaboration, is limited in its capacity to assure that the message has been received and
read by all members.

3. I owe a great intellectual debt for framing many of my ideas, including this one, to Dr. Patricia
Patterson. The blame for everything that it is written here is still mine in its entirety.

References

Adams, G. B., & Balfour, D. L. (2004). Unmasking administrative evil. Armonk, NY: ME Sharpe.

Anderson, R. E. (2004). Ethics and digital government. In A. Pavlichev & D. G. Garson (Eds.), Digital
government: Principles and best practices (pp. 218-234). Hershey, PA: Idea Group Publishing.

Behn, R. D. (1995). The big questions of public management. Public Administration Review, 55, 313-324.

Berman, J., & Mulligan, D. (1999). The internet and the law: Privacy in the digital age: Work in progress.
Nova Law Review, 23, 549-927.

Bok, S. (1999). Lying: Moral choice in public and private life. New York, NY: Vintage Books. (Original
work published 1978)

Bovens, M., & Zouridis, S. (2002). From street-level to system-level bureaucracies: How information and
communication technology is transforming administrative discretion and constitutional control. Public
Administration Review, 62, 174-184.

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 233

 

Bozeman, B. (2007). Public values and public interest: Counterbalancing economic individualism.
Washington, DC: George Washington Press.

Brainard, L. (2003). Citizen organizing in cyberspace. American Review of Public Administration, 33,
384-406.

Calista, D., & Melitski, J. (2007). E-government and e-governance: Converging constructs of public sector
information and communications technologies. Public Administration Quarterly, 31, 87-99, 101-120.

Carrizales, T. (2008). Functions of e-government: A study of municipal practices. State and Local
Government Review, 40, 12-26.

Casselman, B. (2012, April 14). Economists set rules on ethics. The Wail Street Journal. Retrieved from
http://online.wsj.com/article/SB 10001424052970203436904577 14894041 0667970.html

Castells, M. (2000). The rise of the network society. Vol. 1. The information age: Economy society and
culture (2nd ed.). Oxford, UK: Blackwell.

Chambliss, D. F. (1996). Beyond caring: Hospitals, nurses and the social organization of ethics. Chicago,
IL: University of Chicago Press.

Colleste, G., & Holmqvist, J. (2004). Information technology and democratic values. The ETHICOMP
E-Journal, I, 1-13.

Cooper, P. J. (2009). The war against regulation from Jimmy Carter to George W. Bush. Lawrence:
University Press of Kansas.

Cooper, T. L. (2001). The emergence of administrative ethics as a field of study in the United States. In T.
L. Cooper (Ed.) Handbook of administrative ethics (pp. 1-36). New York, NY: Marcel Dekker.

Cooper, T. L. (2004). Big questions in administrative ethics: A need for focused, collaborative effort. Public
Administration Review, 64, 395-407.

Cooper, T. L. (2006). The responsible administrator: An approach to ethics for the administrative role (5th
ed.). San Francisco, CA: Jossey-Bass.

Coursey, D., & Norris, D. F. (2008). Models of e-government: Are they correct? An empirical assessment.
Public Administration Review, 68, 523-536.

Cupps, S. D. (1977). Emerging problems of citizen participation. Public Administration Review, 37,
478-487.

Cushman, F., Young, L., & Hauser, M. (2006). The role of conscious reasoning and intuition in moral judg-
ment: Testing three principles of harm. Psychological Science, 17, 1082-1089.

Dawes, S. S. (2008). The evolution and continuing challenges of e-governance. Public Administration
Review, 68, S86-S102.

D’agostino, M., Schwester, R., Carrizales, T., & Melitski, J. (2011). A study of e-government and e-gov-
ernance: An empirical examination of municipal websites. Public Administration Quarterly, 35, 3-25.

Denhardt, R. B. (2001). The big questions of public administration education. Public Administration
Review, 61, 526-534.

Dijk, J. A. G.M. V. (2005). The deepening divide: Inequality in the information society. Thousand Oaks,
CA: Sage.

DiMaggio, P., Hargittai, E., Ceste, C., & Shafer, S. (2004). Digital inequality: From unequal access to dif-
ferentiated use. In K. Neckerman (Ed.), Social inequality (pp. 355-400). New York, NY: Russell Sage
Foundation.

Dunleavy, P., Margetts, H., Bastow, S., & Tinkler, J. (2006). Digital era governance: IT corporations, the
state, and e-government. New York, NY: Oxford University Press.

Elwood, S. (2006). Critical issues in participatory GIS: Deconstructions, reconstructions, and new research
directions. Transactions in GIS, 10, 693-708.

Foot, K., & Schneider, S. (2006). Web campaigning. Cambridge: MIT Press.

Fountain, J. E. (2001). Building the virtual state: Information technology and institutional change.
Washington, DC: Brookings.

Frederickson, G. H. (1982). The recovery of civism in public administration. Public Administration Review,
42, 501-508.

Gibson, W. (1984). Neuromancer. New York, NY: Ace Books.

Gil-Garcia, R. J. (2012). Enacting electronic government success: An integrative study of government-wide
websites, organizational capabilities, and institutions. New York, NY: Springer.

Downloaded from arp.sagepub.com by guest on February 14, 2015
234 American Review of Public Administration 45 (2)

 

Goldfinch, S. (2007). Pessimism, computer failure, and information systems development in the public sec-
tor. Public Administration Review, 67, 917-929.

Green, J. D. (2003). From neural “is” to moral “ought” what are the moral implications of neuroscientific
moral psychology? Nature Reviews Neuroscience, 4, 847-850.

Green, J. D. (2009). Dual-process morality and the personal/impersonal distinction: A reply to McGuire,
Langdon, Coltheart, and Mackenzie. Journal of Experimental Social Psychology, 45, 581-584.

Green, J. D., Morelli, S., Lowenber, K., Nystrom, L., & Cohen, J. (2008). Cognitive load selectively inter-
feres with utilitarian moral judgment. Cognition, 107, 1144-1154.

Green, J. D., Nystrom, L. E., Engell, A. D., Darley, J. M., & Cohen J. D. (2004). The neural bases of cogni-
tive conflict and control on moral judgment. Neuron, 44, 389-400.

Green, J. D., Sommerville, R. B., Nystrom, L. E., Darley, J. M., & Cohen, J. D. (2001). An {MRI investiga-
tion of emotional engagement in moral judgment. Science, 293, 2105-2108.

Grénlund, A. (2005). State of the art in e-gov research: Surveying conference publications. International
Journal of Electronic Government Research, 1, 1-25.

Grénlund, A., & Andersson, A. (2006). E-Gov research quality improvements since 2003: More rigor, but
research (perhaps) redefined. Lecture Notes in Computer Science, 4084, 1-12.

Gutmann, A., & Thompson, D. (2004). Why deliberative democracy. Princeton, NJ: Princeton University
Press.

Haidt, J. (2001). The emotional dog and its rational tail: A social intuitionists approach to moral judgment.
Psychological Review, 108, 814-834.

Hardy, C. A., & Williams, S. P. (2011). Assembling e-government research designs: A transdisciplinary
view and interactive approach. Public Administration Review, 71, 405-413.

Harmon, M. (1974). Social equity and organizational man: Motivation and organizational democracy.
Public Administration Review, 34(Special Issue), 11-18.

Hart, D. K. (1984). The virtuous citizen, the honorable bureaucrat, and “public” administration. Public
Administration Review, 44(Special Issue), 111-120.

Heeks, R., & Bailur, S. (2007). Analyzing e-government research: Perspectives, philosophies, theories,
methods, and practice. Government Information Quarterly, 24, 243-265.

Hindman, M. (2009). The myth of digital democracy. Princeton, NJ: Princeton University Press.

Im, T. (2011). Information technology and organizational morphology: The case of the Korean central
government. Public Administration Review, 71, 435-543.

Johnson, D. G. (1997). Ethics online. Communications of the ACM, 40(1), 60-65.

Johnson, D. G., & Miller, K. W. (2009). Computer ethics: Analyzing information technology (4th Ed.).
Upper Saddle River, NJ: Prentice Hall.

Kakabadse, A., Kakabadse, N., & Kouzmin, A. (2003). Reinventing the democratic governance project through
information technology? A growing agenda or debate. Public Administration Review, 63, 44-60.

Kerwin, C. M., & Furlong, S. R. (2011). Rulemaking: How government agencies write law and make policy
(4th ed.). Washington, DC: CQ Press.

Kettl, D. (2002). The transformation of governance: Public administration for the twenty-first century.
Baltimore, MD: Johns Hopkins University Press.

Kettl, D. (2009). The next government of the United States: Why our institutions fail us and how to fix them.
New York, NY: W.W. Norton.

Karlin, J. J. (1996). The big questions of public administration in a democracy. Public Administration
Review, 56, 416-423.

Karlin, J. J. (2001). Big questions for a significant public administration. Public Administration Review, 61,
140-143.

Lenk, K., & Traunmiiller, R. (2002). Electronic government: Where are we heading? In R. Lenk &
B. Traunmiiller (Eds.), Electronic government: First international conference, EGOV 2002 (pp. 173-199).
New York, NY: Springer.

Lessig, L. (2006). Code: Version 2.0. New York, NY: Basic Books.

Lipsky, M. (1980). Street-level bureaucracy: Dilemmas of the individual in public services. New York,
NY: Sage.

Lyytinen, K., & Robey, D. (1999). Learning failure in information systems development. Jnformation
Systems Journal, 9, 85-101.

Downloaded from arp.sagepub.com by guest on February 14, 2015
Roman 235

 

Macedo, S., Alex-Assensoh, Y., Berry, J. M., Brintanall, M., Campbell, D. E., Fraga, L. R., & Walsh, K.C.
(2005). Democracy at risk: How political choices undermine citizen participation and what we can do
about it. Washington, DC: Brookings.

Maureen Brown, M. (2007). Understanding e-government benefits. American Review of Public
Administration, 37, 178-197.

Maynard-Moody, S., & Musheno, M. (2006). Cops teachers, counselors: Stories from the front lines of
public service. Ann Arbor: University of Michigan Press.

McCall, M. K. (2003). Seeking good governance in participatory-GIS: A review of processes and gover-
nance dimensions in applying GIS to participatory spatial planning. Habitat International, 27, 549-573.

Milakovich, M. E. (2012). Digital governance: New technologies for improving public service and partici-
pation. New York, NY: Routledge.

Milgram, S. (1974). Obedience to authority. New York, NY: Harper and Row.

Moor, J. H. (1985). What is computer ethics? Metaphilosophy, 16, 226-275.

Moor, J. H. (1998). Reason, relativity, and responsibility in computer ethics. Computers and Society, 28,
14-21.

Moore, A., Clark, B., & Kane, M. (2008). Who shalt not kill? Individual differences in working memory
capacity, executive control, and moral judgment. Psychological Science, 19, 549-557.

Mossberger, K., Tolbert, C. J., & Stansbury, M. (2003). Virtual inequality: Beyond the digital divide.
Washington, DC: Georgetown University Press.

Mullen, H., & Horner, D. S. (2004). Ethical problems for e-government: An evaluative framework.
Electronic Journal of e-Government, 2, 187-196.

Neumann, F. X., Jr. (1996). What makes public administration a science? Or are its “big questions” really
big? Public Administration Review, 56, 409-415.

Norris, D. F., & Moon, M. J. (2005). Advancing e-government at the grassroots: Tortoise or hare? Public
Administration Review, 65, 424-433.

O’ Leary, R. (2006). The ethics of dissent: Managing guerrilla government. Washington, DC: CQ Press.

Orlikowski, W. J. (1992). The duality of technology: Rethinking the concept of technology in organizations.
Organization Science, 3, 398-427.

Orlikowski, W. J. (2000). Using technology and constituting structures: A practice lens for studying tech-
nology in organizations. Organization Science, 11, 404-428.

Prior, M. (2007). Post-broadcast democracy. New York, NY: Cambridge University Press.

Ramadhan, A., Sensuse, D. I., & Arymurthy, A. M. (2011). E-government ethics: A synergy of computer
ethics, information ethics, and cyber ethics. International Journal of Advanced Computer Science and
Applications, 2, 82-86.

Rethemeyer, K. R. (2007). The empires strike back: Is the Internet corporatizing rather than democratizing
policy processes? Public Administration Review, 67, 199-215.

Rohr, J. A. (1989). Ethics for bureaucrats: An essay on law and values (2nd ed.). New York, NY: Marcel
Dekker. (Original work published 1978)

Roman, A. V. (2013). Delineating three dimensions of e-government success: Security, functionality and
transformation. In J. Ramon Gil-Garcia (Ed.), E-Government success factors and measures: Concepts,
theories, experiences, and practical recommendations. Hershey, PA: IGI Global.

Roman, A. V., & Miller, H. (2013). New questions for e-government: Efficiency but not (yet?) democracy.
International Journal of Electronic Government Research, 9(1), 65-81.

Romzek, B. S., & Johnston, J. M. (2005). State social services contracting: Exploring the determinants of
effective contract accountability. Public Administration Review, 65, 436-449.

Rose, N. (1999). Powers of freedom: Reframing political thought. New York, NY: Cambridge University
Press.

Sachs, J. D. (2011). The price of civilization: Reawakening American virtue and prosperity. New York,
NY: Random House.

Sandel, M. J. (2009). Justice: What's the right thing do to? New York, NY: Farrar, Straus and Giroux.

Schlossberg, M., & Shuford, E. (2005). Delineating “public” and “participation” in PPGIS. URISA Journal,
16, 15-26.

Scholl, H. J. (2009). Profiling the EG research community and its core. In M. A. Wimmer, H. J. Scholl,
M. Janssen, & R. Traunmiiller (Eds.), Electronic government: 8th international conference, EGOV

Downloaded from arp.sagepub.com by guest on February 14, 2015
236 American Review of Public Administration 45 (2)

 

2009. Linz, Austria, August/September 2009 proceedings (pp. 1-12). Berlin, Germany: Springer-
Verlag Berlin Heidelberg.

Sennett, R. (1998). The corrosion of character: The personal consequences of work in the new capitalism.
New York, NY: W. W. Norton.

Stahl, C.B. (2005). The ethical problem of framing e-government in terms of e-commerce. The Electronic
Journal of e-Government, 3, 77-86.

Tapia, A. H., & Ortiz, J. A. (2010). Network hopes: Municipalities deploying wireless Internet to increase
civic engagement. Social Science Computer Review, 28, 93-117.

Thompson, D. F. (1980). Moral responsibility of public officials: The problem of many hands. The American
Political Science Review, 74, 905-916.

Thompson, D. F. (1987). Political ethics and public office. Cambridge, MA: Harvard University Press.

Torres, L., Pina, V., & Royo, S. (2005). E-government ant the transformation of public administration in EU
countries: Beyond NPM or just a second wave of reforms? Online Information Review, 29, 531-553.

Van den Hoven, J., (2000). The Internet and varieties of moral wrong doing. In D. Langford (Ed.), Internet
ethics (pp. 127-153). London, UK: Palgrave Macmillan.

van den Hooven, J. (1998). Moral responsibility, public office and information technology. In I. Snellen,
W. van & de Donk (Eds.), Public administration in an in-formation age: A handbook (pp. 97-111).
Amsterdam, The Netherlands: IOS Press.

Verbeek, P. (2011). Moralizing technology: Understanding and designing the morality of things. Chicago,
IL: University of Chicago Press.

Waldo, D. (1988). The enterprise of public administration. Novato, CA: Chandler and Sharp.

West, D. M. (2005). Digital government: Technology and public sector performance. Princeton, NJ:
Princeton University Press.

Author Biography

Alexandru V. Roman is an assistant professor of public administration at California State University, San
Bernardino. His current research focuses on questions of public management, e-government, and public
finance.

Downloaded from arp.sagepub.com by guest on February 14, 2015
The Anarchist Library
Anti-Copyright

 

David Berry
For a dialectic of homosexuality and revolution
2003

Retrieved on September 26, 2010 from raforum.info
Conference on “Socialism and Sexuality. Past and present of
radical sexual politics”, Amsterdam, 3-4 October 2003.

theanarchistlibrary.org

For a dialectic of
homosexuality and revolution

David Berry

2003
day means to struggle againt this capitalist and phallocratic society
(the mode of intersection of these three remains to be discussed).

Oppressed as we are, our relation to the world is not only dif-
ferent, it incriminates this world which excludes us and it raises
questions which, whilst specific, in no way concern exclusively ho-
mosexuals and lesbians. For example:

a. Since the labour movement promotes a bourgeois morality
which entails the oppression of the homosexuals (and
women) within it, we are led to ask ourselves questions
about the existing organizations of the working class and
about the nature of the society they hope to build.

b. What of the attitude towards so-called marginal struggles,
considered as, at best, secondary to the central political strug-
gle and implying the non-recognition of social movements
(women, homosexuals, regionalism, etc.) which are seen ei-
ther as factors of division or potential allies?

c. And despite the speeches, despite the struggles fought pri-
marily by women, the reduction of the social revolution to
the question of the relations of production.

43
Pierre Hahn, a leading left-winger in the gay lib movement and a
founder member of the FHAR:

More than to any other, homosexuals are grateful to
you — and I more than anyone — for everything you
have done for them, and that at a time when to speak
out in such a way brought with it great disrepute. [...]
But your most valuable contribution is a life's work
which is at once political (in the traditional sense of
the word) and sexological: it is La Peste brune plus Kin-
sey; it is Fourier and the texts against colonialism; it is,
above all, you yourself.!°

APPENDIX

“Behind the masks (Manifesto of Masques, a review of homosex-
ualities)” 1°

Composed of homosexuals, men and women, who have expe-
rience both of political activism and homosexual activism, the
Masques collective was born of our rejection of the separation of
these two practices, a rejection of the limitations imposed by such
a separation, of the ossification which it induces, and which we
are convinced, after two years of debate and of practice, we must
attempt to transcend.

This separation has it roots in the 80 years of struggle waged by
homosexuals and lesbians harassed by bourgeois repression with
the complicity, the silence, even the support of the organizations
claiming to represent the working class. Rejection, but also the de-

sire to go beyond: for us, to struggle against heterosexist society to-
15 Une lettre du regretté Pierre Hahn’, in Homosexualité et Révolution, pp.42-
44, quote p.43.
1° From Masques. Revue des homosexualités no.1 (May 1979), pp.2-3. The ed-
itorial committee seems to have consisted of five men (including P. Hahn) and
one woman. Guérin was a contributor and was interviewed for the first issue.

42

Contents

Living two lives: homophobia in the socialist and labour
movements... 6.
“The disalienation of each individual” For a dialectic of
homosexuality and revolution. ............
The left and homosexuality: acritique ...........
For (homo)sexual liberation: Guérin’s critical engage-
ment wth “sexology? ........-....20004
Kinsey...
Anarchist individualism: Stirner and Armand. ......
Fourier 2... ee
The revolutionary potential of psychoanalysis: the early
Freud 2... ee
For a synthesis of Marxism and psychoanalysis: Reich . .
Women and patriarchy .. 2... 0.2. ..0..20002..
Androgyny and bisexualism ................
Homophobia asracism 2.2... ...2....0.00.0.
Fora total revolution .......... 0.0... 0008.
The gay liberation movement: a critique. .........
Conclusion... ee
APPENDIX 2...

15
16
17
18

20
commitment to sexual and especially homosexual liberation, and
an attempt both on a theoretical and on a practical, organizational
level to bring these two aspects of total social revolution together.

The issue of homosexuality acquired ever greater importance in
Guérin’s life, and, in an interview he gave at the age of 75, he made
the following remarks about a collection of essays, which he evi-
dently thought might be his last, entitled Son testament:

I may well not live many more years, and as a pre-
caution I have been keen to let it be known that I
would like my last publication, my last thoughts, to
focus on my love of boys. Having already written
books on a great number of different subjects, having
a great deal of experience of political activism and
having very strong political views, I could have pro-
duced a synthesis of my thoughts about revolution,
antimilitarism, anticolonialism, etc. If I was insistent
that my last book should be called His Testament, it is
because I think that homosexuality has played such a
primordial réle in my life, that it has haunted me day
and night from the age of 15, that that is the message
I wish to leave behind. The fact that I am married,
a father, a grandfather, bisexual, homosexual, [...] it
seems to me that this is what I must leave behind as
the final expression of my life as a writer and as a

man.!*4

Finally, to conclude, I can do no better that to quote an assess-
ment of Guérin’s contribution in the form of a letter to him from

104 Thterview a la revue Homo 2000, 1979’ extracted in Homosexualité et Révo-
lution, pp.64—-5, quote p.65. Son testament (Paris: Encre, 1979) is divided into two
parts, the first bringing together a selection of autobiographical texts, the second
a set of texts by or about ‘precursors’: Plutarch, Shakespeare, Fourier, Balzac’s
Vautrin, Sacher-Masoch and Gauguin.

41
and Owen who have rejected the class-struggle in industry, or from
a women’s movement which from the nineteenth century to the
present has been seldom entirely happy with the definition of rad-
ical priorities offered by even the most revolutionary of males?!”
Quite apart from the fact that this claim is at least in part ques-
tionable, Sedgwick also seems to have been ignorant of Guérin’s
writings on sexuality other than the autobiographical texts. For,
although Guérin adhered to the orthodox Marxist argument, as ex-
pounded by Engels, according to which the patriarchal family, pri-
vate property and the state were both coterminous and historically
determined, it is precisely in the Utopian Fourier, in the individ-
ualist anarchists Armand and Stirner, in Reichian psychoanalysis
and in the liberal sexologist Kinsey that Guérin found the ideas
he needed to produce a critique of labour movement homophobia
and to tie this up with a socialist critique of bourgeois patriarchy.
Sedgwick concluded his analysis of the contradictions in Guérin by
arguing that:

In his more personal, experiential writing, Guérin is
unwittingly correcting the entire theoretical orienta-
tion of his public socialism: his oscillation between
a masculine public sphere of production and a quasi-
feminine world of the heart is the penalty of the
double life forced on him by society’s ban.!%

This is doubtless true of an earlier period in Guérin’s life.
But surely what characterizes Guérin’s activism and his non-
autobiographical writings from the 1950s onwards, and par-
ticularly after his coming out in 1965, is his move away from
Marxism-Leninism and towards anarchism, away from the point
of production and towards a breaking down of the artificial
barrier between the “public” and the “private”, towards a growing

102 Sedgwick, p.210-11
103 Sedgwick, p.217

40

Daniel Guerin’s engagement with “sexology” from the 1950’s and
his contribution to the theorization of sexuality and gender from a
historical materialist perspective.

Only a true libertarian communism, antiauthoritarian
and antistatist, would be capable of promoting the
definitive and concomitant emancipation both of
the homosexual and of the individual exploited or
alienated by capitalism.

[Je] me définirais, s'il fallait absolument se définir, un
marxiste libertaire qui n’ai cessé, depuis des années, de
soutenir la nécessité d’une synth ése entre marxisme,
anarchisme, psychanalyse.’

As the French revolutionary Daniel Guérin (1904-1988) once re-
marked, the European labour movement's record with regard to
homosexuality has not, on the whole, been positive.? Nor have the
marxist tradition’s attempts to theorize sexuality and heterosexism
and their relationship to class and class conflict been entirely satis-
factory. Guérin is a rare example of a marxist revolutionary and a
bisexual who dared to address these problems rigorously and very
publicly at a time when to do so was to invite opprobrium from
all quarters — including most of his supposed comrades. Although
by 1968 he could be seen as the “grandfather of the French homo-
sexual movement”, Daniel Guérin has always been better know
outside gay circles for his réle in the revolutionary movement. On
the revolutionary left of the Socialist Party in the 1930s, he was

' Homosexualité et révolution (Paris: Le Vent du ch’min, 1983), p.25

* “Wilhelm Reich aujourd’hui” (1969), p.28.

3 “Le mouvement ouvrier et ’homosexualité”. Guérin made similar remarks
in an interview with the same title published in Gérard Bach, Homosexualités:
Expression/Répression (Paris: Le Sycomore, 1982), pp.99-102.

* Frédéric Martel, Le rose et le noir. Les homosexuels en France depuis 1968
(Paris: Seuil, 2000), pp.46
later heavily influenced by Trotsky, before becoming attracted to
the libertarian communist wing of the anarchist movement. Af-
ter 1968, he became increasingly interested in Rosa Luxemburg
and councilism, and argued for a synthesis of marxism and anar-
chism. Guérin’s engagement with “sexology”, however, has been
relatively neglected (other than in the work of the French histo-
rian, Sylvie Chaperon?. Similarly, his active commitment to homo-
sexual liberation (especially after he came out in 1965) remains lit-
tle known beyond gay circles. Jean Maitron’s entry on Guérin in
the Dictionnaire biographique du mouvement ouvrier fran ais, for
instance, does not even mention homosexuality; and the obituary
by a close associate of Guérin’s, Daniel Guerrier, ironically enti-
tled “Un militant sans frontié res” (“An activist without borders”)
mentions it in one short sentence.° This doubtless reflects the en-
demic — if nowadays more carefully hidden — homophobia of the
left and the labour movement; and also the persistent reluctance on
the part of many historians of the left and of labour, even today and
perhaps particularly in France (relative to, say, Britain and the US),
to attach importance to forms of social inequality and oppression
linked to gender and sexuality.’ Guérin himself, in both his histori-
cal and theoretical writings and his political activism — whether it
be in the context of antifascism, anticolonialism or homosexual lib-
eration — adopted a consistently historical materialist, class-based

° Sylvie Chaperon, “Le fonds Daniel Guérin et I“histoire de la sexualité” in
Journal de la BDIC no.5 (Juin 2002), p.10; “Kinsey en France: les sexualités mas-
culine et féminine en débat” in Mouvement social no.198 (January-March 2002),
pp-91-110

° Jean Maitron, in Jean Maitron (ed.), Dictionnaire Biographique du Mouve-
ment Ouvrier Frang ais (Paris: Edns. ouvié res), vol_XXXI (1988), pp.33-5; Daniel
Guerrier, “Daniel Guérin. Un militant sans frontié res” in Le Monde libertaire
no.705 (April 1988).

7 On the place of feminism within the history of revolutionary movements,
see, for example, the “Tribune” piece by Anne-Lise Melquiond, “Le féminisme est-
il soluble dans le BLEMR?”, in Bulletin de Liaison des Etudes sur les Mouvements
Révolutionnaires no. 4 (December 1999), p.31.

Guérin’s desires have always been framed less in
terms of a body than of an embodiment: the lovers
pass as successor-incarnations of an active, questing
proletariat, a mass of privacies summating through
their plenitude and their sameness into a collec-
tive public subject. It is a myth of working-class
virility which yokes Guérin’s syndicalism with his
sexual nature, in an idealisation which echoes the
less erotic (but equally ethereal) mythology of the
proletariat-as-agent heralded by a Sorel or a Lukacs.””

Guérin also tended (particularly through his masochism, his
fetishism and his adherence to somewhat stereotypical, reduction-
ist representations of physical beauty) to reproduce exploitative
relations similar to those which have been much targeted by
feminism.

To some extent, Guérin was aware of these contradictions — the
contradictions, in Sedgwick’s words, “between the egalitarian and
emancipatory values which the Left canvasses for the reform of
society, and the metaphysics of abasement, domination or objec-
tivation which seem to characterise sexual relations of a certain
intensity”!°° — and in Eux et lui, notably, he submitted himself to
a public and painfully honest autocritique.

Sedgwick argues, quite rightly, that Guérin’s linking of his ho-
mosexual proclivities with the proletariat seen as social vanguard
“does not establish the radicalism of Guérin’s sexual choice within
the terms of sexual politics itself”!°! And he goes on to claim that
our modern awareness of sexual politics has tended historically
to derive not from the class-struggle-oriented Marxists and anar-
chists, but “from liberal feminists, or from Utopians like Fourier

” Peter Sedgwick, ‘Out Of Hiding: The Comradeships of Daniel Guérin’ in
Salmagundi, vol.58, part 9 (1982), pp.197-220, quote p.210

10 Sedgwick, p.211.

1°! Sedgwick, p.210.

39
struggle for “economic” liberation, but also the struggle for sex-
ual liberation. “We must not wait for the Revolution, we must not
wait for the proletariat to have taken power, and assume that this
will automatically bring about sexual liberation” It was exactly the
same, Guérin argued, with religion: “No! We must fight obscuran-
tist fanaticism now.*°All these struggles were “parallel” struggles
within total social revolution.

Conclusion

Guérin commented once that “the driving force of my life has
been love”.”” Perhaps this provides the unifying principle underly-
ing all of Guérin’s work. As he wrote in 1959 in the foreword to an
essay about the censorship of homosexual writers:

The problem in reality is not homosexuality. It is,
above and beyond that, the problem of sexual liber-
ation, or rather, more generally even than that, it
is the problem of freedom. Eroticism is one of the
instruments of freedom. There is within it, in the
words of Simone de Beauvoir, a principle which is
hostile to society, or, more precisely, hostile to a
society in which man oppresses man [sic], hostile to
the authoritarian society. In Carmen, the song goes:
Love is a gypsy child./It has never, ever obeyed laws.”®

There are nevertheless clearly some aspects of Guérin’s sexual
attitudes or practices which are not unproblematic, notably his ten-
dency to objectify his sexual partners and to idealize working-class
youth. As Sedgwick very eloquently put it:

*® Dela répression sexuelle a la Révolution’, from le Point, Brussels, Decem-

ber 1968, in Homosexualité et Révolution, p.34.
*7 ‘Géographie passionnelle’, p.6
8 Missing footnote

38

perspective. This paper aims to explore his attempts from the 1950s
onwards to analyze the nature of sexuality and the reasons for the
oppression of homosexuality, to promote (homo)sexual liberation,
and his insistence that such sexual liberation could only be fully
achieved as part of a social revolution. To what extent did Guérin
succeed in applying a marxist analysis to these problems?

Living two lives: homophobia in the socialist
and labour movements

For many years, Guérin lived what he referred to as a “cruel di-
chotomy”.’ With close friends and comrades, in whom he was able
to confide as far as other things were concerned, Guérin neverthe-
less felt obliged to refrain from raising anything to do with sexual-
ity, and it was certainly inconceivable that he should ever attempt
to defend “a non-orthodox version of love”’, even from a detached
point of view. Add to this the workerist and anti-intellectual tra-
ditions of French syndicalism, and Guérin was doubly damned. In-
deed the two sins — his homosexuality and his class background
— were of course linked, in that it was a common misconception
that homosexuality was a “bourgeois vice”, similarly to the way
in which it would in later years be seen as being in some way
intrinsically linked to fascism. This is doubtless why Guérin put
some effort into disseminating research as early as the 1950s which
demonstrated that homosexuality was just as common among the
working class as any other class, but which also highlighted the dif-
ferential experiences of working-class and bourgeois homosexuals
— both in terms of the conditions that working-class homosexuals
were forced to endure in their attempts to meet other homosexu-

® Homosexualité et Révolution, p.11. “I felt as if I was cut in two, speaking
out loud about my convictions as an activist and, by force of circumstance, feeling
obliged to hide my sexual inclinations”.

° Homosexualité et Révolution, p.11.
als (public urinals as opposed to private clubs and salons) and in
terms of harassment by the authorities (as contrasted with the rela-
tive tolerance of homosexuality in bourgeois and artistic circles).!°

“The disalienation of each individual.’ For a
dialectic of homosexuality and revolution.

In the 1950s, Guérin moved closer to anarchism — both on a
practical, campaigning level (to some extent by force of circum-
stance), and on a theoretical level, as his research on the European
revolutionary movement since 1789 forced him to become increas-
ingly critical of leninism. Still a historical materialist, he was active
on the revolutionary anti-stalinist left; he was heavily involved in
anti-colonial campaigns and worked to support the black liberation
movement in the United States (he was the first French publisher
of Malcolm X, for example). But starting in 1954, he began to write
more and more about sexuality, and he finally came out, no longer
able to bear the schizophrenic split between the two parts of his
life, in 1965, with the publication of his first autobiography, Un je-
une homme excentrique."! By the time he produced Homosexualité

10 See, for instance, “La répression de [homosexualité en France”, La Nef,
mars 1958, and “Pour le droit d’aimer un mineur”, Marge no.4, November-
December 1974. “Contrary to myth, homosexuality is not a ‘rich man’s vice’”.

" Un jeune homme excentrique. Essai d’autobiographie (Paris: Julliard, 1965).
The 1972 Autobiographie de jeunesse was a later, unexpurgated version of this.
It is true that Guérin had come out a few years earlier with the publication of
a shorter and more poetic work entitled “Eux et lui” (published in Les Lettres
nouvelles no.26, 21 October 1959, pp.28-39, and as a book in 1962 by Editions
du Rocher, Monaco, with illustrations by André Masson), but the readership was
so small it passed unnoticed by most. Guérin’s archives contain congratulatory
letters on “Eux et lui” from, amongst others, Aimé Césaire, Samuel Beckett, Franc
ois Mauriac, Michel Leiris and André Baudry (Fonds Guérin, BDIC, F° A 721/8). A
recent republication contains both the original 1962 version and a 1979 version
of Eux et lui and Commentaires, plus Guérin’s marginalia (Lille: GaiKitschCamp,
2000). For a bibliography, see my web page.

as homosexuals, have a r™le to play in the class
struggle.4

In Homosexualité et Révolution, Guérin summarized his strategy,
uniting short-term reforms favouring the civil liberties of homosex-
uals, women and ethnic minorities with the broader and long-term
aims of revolutionary socialism:

In any case, the gains won against homophobia by its
victims can only be limited and fragile. On the other
hand, the crushing of class tyranny would open the
way to the total liberation of every human being, in-
cluding homosexuals.

The task therefore is to ensure that there is as great a
convergence as possible between homosexuality and
revolution.

The proletarian revolutionary must understand, or
must be convinced, that, even if he does not see
himself as directly implicated, the emancipation of
the homosexual concerns him just as much as, for
example, the emancipation of women and of people
of colour. As for the homosexual, he must understand
that his liberation can be total and irreversible only if
it is achieved within the context of social revolution,
in other words, only if the human race succeeds not
just in liberalizing attitudes, but far more than this, in
transforming everyday life.

2

If, on another occasion, he conceded that the “essential struggle
was that against capitalism and for the liberation of the oppressed
proletariat, he nevetherless insisted that this meant not only the

** Le Droit a la caresse, also quoted in ‘Libertaires et gais’.
°S Homosexualité et Révolution, p.25.

37
provocations”®? had produced “defensive reactions and repulsion”
amongst young straight men who might otherwise have been more
open sexually.”

As has already been commented, despite his repeated assertion
that “thanks to the revolution of May 68, homosexuality finally
gained acceptance?*! and despite the fact that in theory at least
the FHAR and the GLPHQ (Groupe de libération homosexuelle poli-
tique et quotidien) put the seal on the rapprochement between ho-
mosexuality and Revolution, Guérin only found an organisation
which fully lived up to his expectations concerning the dialectic of
(homo)sexual liberation and social revolution with the creation of
the UTCL in 1978. Invited to write a regular column for Gai Pied
Hebdo in the early 80s, Guérin felt obliged to check with the UTCL
before agreeing: “Total and unreserved approval”, was the Union’s
response” The UTCL itself published a pamphlet, Le Droit a la ca-
resse, written by a gay activist:”

There can be no liberation of homosexuality other
than on the basis of new social relations, in other
words other than in a new society, which is why we
are allies with the labour movement in its struggle,
the labour movement beng the only force capable
of bringing about the necessary social change. So,
if socialism is not to be a caricature of itself, we,

® Homosexualité et Révolution, p.23.

” Paris Gay 1925, p.54.

°°. Homosexualité et Révolution, p.23.

*» 4 ibertaires et gais’, Gai Pied Hebdo no.52 (15-21 January 1983), p.15.

°3 Union des Travailleurs Communistes Libertaires, Le Droit 4 la caresse: Les
homosexualités et le combat homosexuel (Paris: Editions «L», n.d.; Supplement to
Tout le Pouvoir aux Travailleurs no.27 and to Lutter), 32 pp. 1 am grateful to com-
rades of the Centre International de Recherches sur l’Anarchisme in Marseille for
unearthing a copy of this for me. The title of the pamphlet is evidently a pun on
Paul Lafargue’s The Right to be Lazy, which translates into French as Le Droit a la
paresse. It includes a useful summary of the history of the homosexual liberation
movement and its connections with the left and the trade union movement

36

et Révolution — a collection of previously published essays, inter-
views and extracts from longer works — in 1983, just five years
before his death, the definition he provided of Revolution reflected
not only the traditional, more or less apocalyptic vision of the ris-
ing up and self-emancipation of the oppressed masses, but spoke
of “the disalienation of each individual”, and he went on: “Hence
the need to establish a dialectical relationship between the words
homosexuality and Revolution” How was this dialectic to be estab-
lished, and what critique of the existing revolutionary movement
(and of the homosexual movement) did it imply?

The left and homosexuality: a critique

“Not so many years ago, to declare oneself a revolutionary and to
confess to being homosexual were incompatible?” Guérin wrote in
1975.!2 All in all, Guérin did not have a positive opinion of the Eu-
ropean labour movement’s record on homosexuality: “the record
is very poor”, beginning with Engels, whose study of the origins of
the family discussed the possible causes of homosexuality before
dismissing it as a degrading.’°

Guérin pointed out that in the beginning, at least, revolutionary
Russia adopted an exemplary attitude to sexual and homosexual
liberation, but he was scathing about the USSR under Stalin and the
post-1945 socialist states in Eastern Europe and Cuba. One of the
reasons why the post-war generations of gays were distrustful of

” “Etre homosexuel et révolutionnaire”, p.36

8 “Le mouvement ouvrier et ’homosexualité’. Guérin made similar remarks
in an interview with the same title published in Gérard Bach, Homosexualités:
Expression/Répression (Paris: Le Sycomore, 1982), pp.99-102. Engels refers to the
“degradation” caused by “the perversion of boy-love” — Friedrich Engels, The Ori-
gin of the Family, Private Property and the State (New York: Pathfinder Press, 1972),
p.93. Guérin nevertheless thought The Origin of the Family “a great book”, “un-
justly depreciated today by a certain school of thought” — ‘Wilhelm Reich au-
jour@ hui’, p.22.
revolutionary politics, according to Guérin, was the abject failure
in this regard of “actually existing socialism’:

The intransigence of the so-called “communist”
regimes in this regard takes much more shocking
forms than that of the capitalist countries. It is para-
doxical and scandalous that the zealots of so-called
“scientific” socialism should display such crass ig-
norance of scientific facts. It is tragic that a morbid
puritanism be allowed to so disfigure the natural and
polymorphous eroticism of an entire generation.4

But why were homosexuals persecuted under stalinism?

The reason is that the homosexual, whether he knows
or wishes it or not, is potentially asocial, an outsider,
and therefore a virtual subversive. And as these
totalitarian regimes have consolidated themselves by
ressuscitating traditional family values, he who loves
boys is considered a danger to society.*

As for the French left, the PCF was “hysterically intransigent
as far as ’moral behaviour’ was concerned”; the trotskyist Pierre
Lambert’s OCI was “completely hysterical with regard to homosex-
uality”; Lutte ouvri re was theoretically opposed to homosexuality;

14 “Sur le racisme anti-homosexuel’, Masques. Revue des homosexualités no.6

(Autumn 1980), pp.49-52, quote p.52.

15 Homosexualité et Révolution, p.17. According to Jean-Louis Franc, a FAHR
activist at the same time as Guérin, the Lambertists were violent towards homo-
sexuals and the Moists even more so, whereas Lutte ouvriére activists, although
the party was programmatically opposed to homosexuality, in practice behaved
quite normally towards homosexuals. In conversation with the author, Linz, 14
September 2002.

16 ‘Aragon, victime et profiteur du tabou’ in Gai Pied Hebdo, 4 June 1983,
reproduced in Homosexualité et Révolution, pp.62-3, quote p.63.

10

homme excentrique, he claimed that he had wanted to present ho-
mosexuality in as “natural” a way as possible, as being part of the
life of a “normal”, healthy person, “carefully avoiding the postur-
ing dear to someone such as Jean Genet, for example, that is to
say the pose of the ‘outcast’, the "damned’. To pose as someone
exceptional, in my opinion, is to isolate oneself from common mor-
tals, and gives the heterosexual majority sticks with which to beat
us.”®> Elsewhere, he commented that although homosexuals must
have their own specific organization, they must also be integrated
within a broader movement for change, like black sections within
trade unions: “those who content themselves with the ghetto are
making a big mistake”®°

By the 1980s, Guérin’s assessment of the state of the gay libera-
tion movement and the gay “scene” was pretty negative:

The recent emancipation, the commercialisation of
homosexuality, the superficial pursuit of pleasure
for pleasure’s sake have created a whole generation
of “gay” young men, profoundly apolitical, obsessed
with gadgets, frivolous, characterless, incapable of
any serious reflexion, uncultured, good for nothing
but “cruising”, corrupted by the specialist press, the
mushrooming of gay bars and so on, and by the
libidinous small ads, in a word a million miles from
any conception of class struggle.®”

Guérin argued that the movement’s ghettoization went against
the “breaking down of social barriers” and against “universal bisex-
uality”®’, and that its “public excesses, sometimes even its pointless

*5 ‘Commentaires trés libres sur les Mémoires d’un jeune homme excen-
trique’, 17 February 1965 (unpublished bound TS held in Bibliothéque Nationale).

*¢ Te mouvement ouvrier et ’homosexualité’.

8” Homosexualité et Révolution, p.17.

83 Homosexualité et Révolution, p.23.

35
When the FHAR (Front homosexuel d’action révolutionnaire)
appeared in 1971, Guérin was enthusiastic, seeing the new group
as the revolutionary homosexual organisation — bringing together
revolutionary politics and a concern with homosexual liberation —
he had always longed to see. He was, however, soon disappointed,
and found it to be even worse than Arcadie: “Some completely
unaware and often very stupid people — except, of course, for
a few intelligent young boys such as Guy Maes and Guy Hoc-
quenghem”.** Guérin was particularly horrified when, at the
funeral of Pierre Overney (a Maoist militant killed by security men
at Renault-Billancourt), some of the more provocative members of
the FHAR exposed their buttocks.°3

Although it is apparently the case that he stood on a table at the
front of the hall and stripped naked with Fran coise d’Eaubonne
during a general assembly of the FHAR (to reinforce a point be-
ing made about the liberation of the body)*4, Guérin was in other
circumstances not a believer in provocation. Explaining once in
a talk to fellow Arcadians his intentions in publishing Un Jeune

®2 Elsewhere, however, Guérin was less complimentary about Hoc-
quenghem. Asked by L’Etincelle in 1977 what he thought of other “écrivains de
Vhomosexualité”, ‘homosexual writers’, he replied: “Hocquenghem writes in an
incomprehensible gobbledygook, Bory overdoes it, he’s the clown of homosexu-
ality, and what’s more he’s turning into a reactionary, he went to dinner with
Giscard; as for Peyrefitte, he is despicable and odious.” On Hocquenghem, Bory
and Peyrefitte, see Marcel, Le Rose et le Noir. On Hocquenghem, see also Bill Mar-
shall, Guy Hocquenghem. Theorising the Gay Nation (London: Pluto, 1996). I have
as yet been unable to establish who Maes was

8 Along with a few copies of the FHAR’s paper, L’Antinorm, Guérin’s
archives also contain a 2 pp. TS document, ‘Pour la constitution et organisation
d'une tendance «politique» au sein du FHAR’ (‘For the constitution and organi-
zation of a ‘political’ tendency within the FHAR’), but I have found no further
evidence of Guérin’s having been involved in any attempt to reform the FHAR.
He seems to have had closer links with David Thorstad, leader of the Gay Activist
Movement, the left-wing tendency within the American gay lib movement. Fonds
Guérin, BDIC, F° A 721/15.

54 See Martel, Le rose et le noir, pp.45-6.

34

as was the Ligue communiste, despite their belatedly paying lip ser-
vice to gay lib.!” Together, Guérin argued, such groups bore a great
deal of responsiblity for fostering homophobic attitudes among the
working class as late as the 1970s. Their attitude was “the most
blinkered, the most reactionary, the most antiscientific”.!®

In an appendix of his pioneering 1955 study of Kinsey (on the
persecution of homosexuals in France), Guérin took the opportu-

nity to argue for a change of attitude:

Revolutionaries have proven themselves to be no more
tolerant than the bourgeois with regard to homosexu-
ality. They have, it is true, an excuse: they distrust the
homosexuals in their ranks because the latter are re-
puted to be vulnerable to blackmail and to pressure
from the police, and are therefore “dangerous” for the
movement which, in the eyes of such activists, is more
important than respect for the human indivdual. But
they do not realize that their intolerance itself con-
tributes to perpetuating the state of affairs which is at
the root of their concern: by virtue of the fact that they
also cast their stone at homosexuals, they are helping
to consolidate the very taboo which makes homosex-
uals easy prey for the blackmailers and for the police.
The vicious circle will only be broken when progres-
sive workers adopt both a more scientific and a more
humane attitude towards homosexuality.”

It is not asurprising that Guérin should have been attacked by
the Catholic Church, but he also came under fire from the left,
and in particular the French Communist Party. The trotskyist

1” See ‘Daniel Guérin «A confesse»’, p11.

18 “Etre homosexuel et révolutionnaire”, p.10.

° Kinsey et la sexualité (Paris: Julliard, 1954), appendix II: “La persécution
des homosexuels en France”, pp.180-6, quotation pp.184-5.

11
Michel Raptis (Pablo) also apparently complained in his review
of the Kinsey book of an over-concentration on homosexuality.”
Even France Observateur, which had first published Guérin’s work
on Kinsey in article form, published only hostile readers’ letters,
refusing to print those expressing gratitude to Guérin. As Guérin
wrote of his critics in a letter to the libertarian sexologist René
Guyon, whose work he much admired:

The harshest [criticisms] came from marxists, who
tend seriously to underestimate the form of oppres-
sion which is antisexual terrorism. I expected it, of
course, and I knew that in publishing my book I was
running the risk of being attacked by those to whom

I feel closest on a political level.?!

Eventually, Guérin had had enough, and he finally came out with
the publication of Un jeune homme excentrique, in 1965:

These guardians of society’s “morals” have inadver-
tently done me a favour: they have made me face up
to them without false shame and come to terms with
myself more fully. Gone are the days of the fruitless
and absurd split between two halves of myself: one
half which was seen and another which had to remain
hidden. Totality has been re-established.””

However, his attempt to explain the relationship between his
discovery of the working class, his sexuality and his socialism,
shocked and was misunderstood by many on the left:

°° Etre homosexuel et révolutionnaire’, p.10. I have yet to trace this review.

21 Letter of 27 May 1955, Fonds Guérin, BDIC, F° A 721/carton 12/4, quoted.
in Chaperon, ‘Le fonds Daniel Guérin et l’histoire de la sexualité’ in Journal de la
BDIC, no.5 (June 2002), p.10

2 Foreword to Autobiographie de jeunesse, p.9

12

they are the object, and who are haunted by the idea of
suicide. I have received some deeply distressing letters
from such people. The most urgent thing, since we are
not going to transform the world tomorrow, is to help
such unfortunate people rediscover a taste for life.””

The gay liberation movement: a critique

Guérin was personally never attracted to what he called “effem-
inate” gay men, and had an “absolute, physical horror” of cross-
dressing.”* In his Essai sur la révolution sexuelle, he argued that “Les
‘tantes-filles’ [...] ne se font pas ‘femmes’ comme dit Sartre, elles
se font plus exactement, telles qu’elles se repésentent les femmes,
c’est-a-dire ‘poupées et putains’.” In the 1920s, most of his sex-
ual partners were heterosexuals — or at least they saw themselves
as such, and rejected the homosexual label. For these reasons, al-
though Martel asserts that he was in a sense “the grandfather of the
French homosexual movement”, Guérin had never actually mixed
a great deal with other declared homosexuals, other than through
his association with Arcadie from 1954 and with its review of the
same name, to which he contributed from 1956.°° Although he was,
as he put it, “very well regarded” within Arcadie, he found the or-
ganisation complacent, petit-bourgeois and reactionary, not least
because its founder, André Baudry, maintained close links with the
police and the clergy, and was determined not to “politicize” his
campaign for the tolerance of “homophilia”.®! Guérin left in 1968.

™ Plexus, no.26 (July 1969), pp.123-4.

® Daniel Guérin «A confesse»’, p.14.

” Essai sur la révolution sexuelle, p.65, note 1.

8° Erédéric Martel, Le rose et le noir. Les homosexuels en France depuis 1968
(Paris: Seuil, 2000), pp.46. Arcadie had about 10,000 members at the end of the
1960s. See Martel, pp.98-117.

8! Te mouvement ouvrier et ’homosexualité’.

33
homosexuality. [...]. The bourgeoisie as a whole will
never entirely lift its ban on dissident sexualities. The
whole edifice will have to be swept away in order to
achieve the complete liberation of man in general (a
generic term which includes both sexes), and of the
homosexual in particuliar.”

Having said that, Guérin was not dismissive of partial gains. In-
terviewed in 1969, he said:

Even at the present time, in capitalist societies, partial
victories over obscurantism should not be under-
estimated, far from it. I see no difference between
wage increases, improvements in prison regimes
and in civil rights (the emancipation of women, for
example) and the struggle against the repression
of homosexuals, a struggle which must be fought
straightaway.’°

This acceptance of partial reforms, in a spirit similar to that of
Amiens” assertion of the CGT’s “double task”, was motivated by
his personal experience of suffering and the knowledge he had of
others’ suffering, particularly in the villages and small towns of “la
France profonde”:

I am thinking above all of those who are imprisoned
as “common criminals” for having tried to satisfy their
sexuality by an act which was an expression of their
true selves. I am also thinking of all those homosexu-
als who find great difficulty in coming to terms with
themselves, in bearing the social reprobation of which

* Homosexualité et Révolution, pp.15-16.
% Interview with Pierre Hahn, Plexus, no.26 (July 1969), pp.123-4, quote
p.123. Extracts also in Homosexualité et Révolution, pp.56-59.

32

My background had enclosed me within the opaque
barriers of social segregation; homosexuality, by mak-
ing me intimately familiar with young workers, by en-
abling me to discover and share their life of exploita-
tion, led me to join the class enslaved by the class I
was leaving behind. This simple explanation, perhaps
too simple, was not to the liking of everybody.”

He was accused of dishonouring not only himself, but the whole
of the left, by suggesting that one had to be a “pédé” (queer) to be
a socialist: “Thanks to me, people might have suspected all “left-
ists” of siding with the labour movement for the pleasure of “a
bit of rough”!” Jean Daniel, editor of Le Nouvel observateur, organ-
ised a boycott, actively discouraging colleagues from reviewing the

book.”4 Guérin found few defenders, and even someone such as the

left-wing, gay novelist Jean-Louis Bory remained silent.” Guérin

reported that one reader and admirer of his celebrated study of an-
archism was profoundly disappointed that the author of such a “se-
rious” work could also have penned Un jeune homme excentrique.”®
Indeed, Guérin’s readers seem to have always fallen into one of two
kinds: “I have two publics: some people buy all my books on polit-

ical and social questions, whilst others are only interested in my

literary and homosexual writings”””

3 “Etre homosexuel et révolutionnaire’, p.10. For more detail on Guérin’s

‘discovery’ of the working class and its relation to his politicization, see my ““Pro-
létaires de tous les pays, caressez-vous!’: Guérin, the Labor Movement, and Ho-
mosexuality,’ in Gabriella Hauch, ed., Sexuality, the Working Classes, and Labor
Movements (forthcoming).

*4 See ‘Daniel Guérin «A confesse»’, pp.10-14.

°5 Ftre homosexuel et révolutionnaire’, p.10.
Etre homosexuel et révolutionnaire’, p.10. Guérin’s L’Anarchisme, de la
doctrine 4 la pratique (Paris: Gallimard, 1°" edition 1965) is widely regarded as one
of the best short introductions to anarchism. The English translation (New York:
Monthly Review Press, 1970) was given a preface by Noam Chomsky.

27 aniel Guérin: d’une dissidence sexuelle A la révolution’, p.42.

26 «©

13
Even the organisations of which Guérin was actually a member
were not beyond criticism. In 1958, before he had come out as a
homosexual, but at a time when he was concerning himself more
and mote in his writings with questions of sexuality, material sub-
mitted both to France-Observateur and to Perspectives socialistes —
the latter being the organ of the Union de la gauche socialiste, of
which he was a member — was censored without his being told:

Thus, in two papers to which I contribute and whose
political positions are close to my own, it is impossi-
ble for me to raise issues of sexuality without being
gagged. But the battle for the emancipation of man
[sic] on all levels continues, and we shall, in the end,
triumph.”*

He commented resignedly in an interview for Masques that the
OCL (Organisation Communiste Libertaire), of which he had been
a member in the early 1970s, had simply never mentioned sexual-
ity: “It’s not hostility, but they forgive me some deviations because
I’ve written books about anarchism.” Things would only change for
Guérin with the appearance of the UTCL (Union des Travailleurs
Communistes Libertaires), of which Guérin would remain a mem-
ber from its creation in 1978 until his death ten years later.”°

°8 From a letter attached by Guérin to an off-print of a journal article of
his held in the Bibliothéque Nationale: ‘André Gide et l'amour’, Arcadie no.49
(January 1958), pp.3-8.

°° The UTCL was to transform itself into the present-day Alternative Lib-
ertaire in 1991. See Georges Fontenis, Changer le monde. Histoire du mouvement
communiste libertaire, 1945-1997 (Editions Le Coquelicot/Alternative Libertaire,
2000), pp.171-5

14

The subjects dealt with make a whole. The libertarian
critique of the bourgeois regime is not possible with-
out a critique of bourgeois mores. The revolution can-
not be simply political. It must be, at the same time,
both cultural and sexual and thus transform every as-
pect of life and of society. [...] 1am against any society,
even a socialist one, which maintains sexual tabous.
The revolt of the spring of 68 rejected all the faces of
subjugation. If the generation of May discovered Re-
ich, it was because he campaigned at one and the same
time for the social revolution and the sexual revolu-
tion.”

Given Guérin’s belief that attitudes towards homosexuality were
intrinsically linked with the réle of the authoritarian family and of
patriarchal gender réles, he was convinced that it was unrealistic
to expect to be able to eradicate homophobia without attacking the
rest:

To my mind, the homophobic prejudice, in all its
hideousness, will not be countered only by means
which I would call ‘reformist’, by persuasion, by
concessions to our heterosexual enemies; it will be
possible to eradicate it definitively, as with racial
prejudice, only through an antiauthoritarian social
revolution. Indeed despite its liberal mask, the bour-
geoisie has too great a need, in order to perpetuate
its hegemony, of the domestic values of the family,
cornerstone of the social order. It cannot deprive
itself of the help provided for it by, on the one hand,
the glorification of marriage and the cult of procre-
ation, and on the other, the support given it by the
Churches, determined adversaries of free love and of

™ Le Monde, 15 November 1969.

31
at one of the meetings concerned who had responded to comments
Guérin had made about “psychological minorities”:

One will never denounce enough the good conscience,
the mental comfort, the contradiction, the hypocrisy of
almost all of the “people of the Left” and their pseudo-
racism. For, if racism is disdain for a community differ-
ent from us, the disdain for a human category because
of a particularity, racism in the full meaning of the
word is not only or necessarily directed at people of
another colour. We must speak out against these peo-
ple who believe themselves to be “generous”, who are
opposed to the racism of others, who are adamant that
they do not look down on blacks, but who never tire of
talking or writing of their disdain for alcoholics, prosti-
tutes, homosexuals, etc... who therefore fulfil for these
“anti-racists” the réle of substitute Jew, of replacement

nigger.”

For a total revolution

In 1969, Guérin was interviewed by Francois Bott for Le Monde.
Guérin’s Essai sur la révolution sexuelle aprés Reich et Kinsey and his
Pour un marxisme libertaire had both just appeared.”*. In later years
Guérin would talk of ‘libertarian communism’ rather than ‘libertar-
ian marxism’, in order not to offend his new anarchist friends, but it
was only a change of label. He remained faithful to historical mate-
rialism and to class analysis all his life.]] Asked if this simultaneity
was a coincidence, he replied emphatically “Non”:

” 4 ettre d’un auditeur’, in Daniel Guérin, Cuba-Paris (Chez auteur: 13 rue

des Marronniers, Paris 16e, Mai 1968), pp.29-30, quote p.29.
73 Essai sur la révolution sexuelle aprés Reich et Kinsey (Paris: Belfond, 1969);
Pour un marxisme libertaire (Paris: Laffont, 1969)

30

For (homo)sexual liberation: Guérin’s critical
engagement wth “sexology”

For Guérin, the revolutionary movement needed to concern it-
self not just with homosexuality, but with sexuality in general, the
libido:

The problem which confronts us, therefore, is know-
ing whether the free exercice of the sexual instinct is
compatible with the contingencies and demands of the
revolutionary struggle.°°

Some, like Proudhon, Robespierre and Lenin saw “virtue” as the
basis of revolutionary activism and emphasised the need for conti-
nence and self-repression in the struggle against the existing order.
Others, notably in 1968, argued on the contrary that “orgasm goes
along with the revolutionary’s furia”*! Reich, 30 years earlier, had
declared:

On croit gagner des forces en éliminant totalement la
vie sexuelle. C’est une erreur, une lourde erreur que
d’exclure la sexualité comme quelque chose de “bour-
geois”.

What was necessary, on the contrary, was to “transformer la ré-
bellion sexuelle de Ja jeunesse en un lutte révolutionnaire contre
Yordre social capitaliste”** Clearly, Guérin argues, excess is not
conducive to effective revolutionary struggle, it is a question of
balance, and this is as true of homosexuality as of any other form
of sexuality:

3° Homosexualité et Révolution, p.9.
31 Homosexualité et Révolution, p.10.
® Wilhelm Reich aujourd’hui’, p.24.

15
Whatever some class-struggle prudes may say,
homosexuality [...] has never diminished the revolu-
tionary’s commitment and combativity, on condition,
of course, that excess and promiscuity are avoided.

Kinsey

The groundbreaking work of Alfred Kinsey (published in French
translation in 1948 and 19543“) was without doubt the most impor-
tant influence on Guérin in his attempts from the 1950s to formu-
late a critique of homophobia and put forward an argument for a
more general sexual liberation. Serious studies of sexuality were
few and far between in France between the 1930s and the 1950s,
and the PCF’s position on sexology and psychoanalysis was as re-
pressive as that of the Catholic Church. Guérin’s study of Kinsey
was thus at once groundbreaking and controversial.*° It was pub-
lished first as a series of articles in the left-wing weekly, France
Observateur, in 1954, then in book form the following year.** It
represented for Guérin a major step forward in that he was able to

3 Homosexualité et Révolution, pp.10-11. This is reminiscent of Guérin’s re-
peated expressions (in his autobiographies) of feelings of guilt at his bouts of
(homo)sexual self-indulgence. Is this because of his determination in 1930 to ‘sub-
limate’ his sexual drive through devotion to the revolution? The assertions of the
need to control his sexual drive is reminiscent of Baudry’s invocations to Arcadie
members.

4 A.C. Kinsey et al, Le comportement sexuel de ’ homme (Paris: Editions du
Pavois, 1948); Le comportement sexuel de la femme (Paris: Le Livre contemporain
Amiot-Dumont, 1954). Sexual Behavior in the Human Male appeared in 1948 in
the US, Sexual Behavior in the Human female in 1953.

*5 See Sylvie Chaperon, ‘Kinsey en France: les sexualités masculine et fémi-
nine en débat’ in Mouvement social no.198 (January-March 2002), pp.91-110, and.
‘Le fonds Daniel Guérin’.

3° France Observateur, 23 September, 7, 22 & 29 October, 4 November 1954;
Kinsey et la sexualité (Paris: Julliard, 1955; EDI, 1967). It would be republished
again as part of Essai sur la révolution sexuelle aprés Reich et Kinsey (Paris: Belfond,
1969). The book was dedicated to Guérin’s father Marcel, “who was one day taken

16

one occasion, he expressed satisfaction at recent cultural trends
which seemed to some extent to represent a reversal of the process
of differentiation of the sexes, and he was positively delighted that,
as he put it, it was sometimes difficult to tell the difference between
young men and women in the street. But he was also aware that
such trends were limited: “We are still a long way from a symbio-
sis, something which, it would seem, only the Social Revolution,
thanks to its equalizing and reconciling aspect, would be able to

achieve””°

Homophobia as racism

As well as seeing parallels between the situation of women and
homosexuals, Guérin argued that homophobia was akin to racism,
and that in terms of the situation in which they found themselves
in their everyday life, the suffering of homosexuals could be com-
pared to that of blacks or Jews:

One only has to read the admirable analysis offered
by Frantz Fanon, in his Black Skin, White Masks, of the
permanent dread of the Black in the face of the White’s
racial prejudice to understand to what extent the fate
of the homosexual resembles that of the man [sic] of
colour. The writer Richard Wright, as heterosexual as
they come, sympathized equally with the comparable

condition of the Black, the Jew and the “queer”.”!

Guérin’s homosexual encounters in the colonies in the late 1920s
undoubtedly played a réle here,a la Genet.

Interestingly, Guérin chose to include in a short collection of
speeches, published in 1968, a letter from a member of the audience

 Homosexualité et Révolution, p.16.
7 “Sur le racisme anti-homosexuel’, p.50.

29
and his melancholy at not being able to choose be-
tween the two poles was inconsolable. He had a foot
in both camps. He dreamed of being the ram with the
ewe and of being the ewe with the ram. Being both
ram and ewe, he was neither ram nor ewe.”

Guérin was convinced that homophobes were in many cases
repressed homosexuals: “Many intolerant and aggressive homo-
phobes are nothing more than homosexuals who have painfully
repressed their natural tendencies and secretly envy those who
have chosen to give their own desire free rein”! He also talked
of the “bisexual universality’®, claiming that bisexuality was the
natural human state:

It certainly seems that [...] heterosexuals, conditioned
by society, are bisexuals without realizing it or who
censor themselves, or who, quite simply, only allow
the heterosexual aspect of their lovemaking to show.

Elsewhere, he clearly agreed with de Beauvoir’s interpretation,
namely that “la différenciation psychologique des sexes est, pour
une large part, artificielle et conditionnée socialement.” He be-
lieved there was “a tendency towards unification, towards a recon-
ciliation of the sexes, through sensitivity, creativity, intelligence. I
think the society of the future will be a bisexual society”®® And
again: “The time will come [...] when women and men will no
longer form two opposed species, when love of both sexes will be
recognized as the most natural form of love [...]”° On more than

® Bux et lui (Lille: GayKitschCamp, 2000), pp.23-24.
° Homosexualité et Révolution, p.20.

® Homosexualité et Révolution, p.23

°° Homosexualité et Révolution, p.8.

°” Essai sur la révolution sexuelle, p61.

°§ ‘Géographie passionnelle d’une époque’, p.6.

° Eux et lui (2000), p. 52.

28

use the opportunity to present a public defence of homosexuality.
Guérin argued that if, before Kinsey, it might have been possible
for socialists and communists, eager for the overthrow of capital-
ist exploitation, to join with Lenin in considering the sexual ques-
tion of secondary importance, or as an adjunct of the central strug-
gle, such an attitude was no longer tenable after the publication of
the Kinsey Report. The puritanism attacked by Kinsey was nothing
other than “a defence mechanism designed to protect a conception
of bourgeois private property thanks to which the bourgeoisie was
able to gain economic power, and then political power?*” Kinsey,
therefore:

encourages us to pursue simultaneously both the so-
cial revolution and the sexual revolution, until human
beings are liberated completely from the two crushing
burdens of capitalism and puritanism.**

Anarchist individualism: Stirner and
Armand

There were other influences on Guérin’s thinking about sexual
liberation, notably among the anarchists. In his youth, Guérin read
E. Armand’s individualist anarchist organ L’en dehors, which used

to campaign for complete sexual freedom, and for which homo-

sexuality was regarded as an entirely valid form of “free love”.*”

to task (by an over-watchful mother) for reading Havelock Ellis without hiding
the fact from his children.” Marcel Guérin was also bisexual.

*” Kinsey et la sexualité, p.118.

38 ‘Kinsey et la sexualité, 1955’ in Homosexualité et Révolution, pp.32-34,
quote p.32.

» Ten dehors appeared weekly, 1922-39. Armand was, however, quite iso-
lated within the French anarchist movement and his concern with sexual freedom
(and in particular his willingness to accept homosexuality) were not, I believe,
widespread among French anarchists. See René Bianco, “Un siécle de presse anar-

17
Much later, Guérin discovered the German individualist anarchist,
Max Stirner. If some anarchist-communists have been a little puz-
zled by Guérin’s interest in Stirner — generally anathema to the
non-individualist wing of the movement — the answer lies in what
Guérin perceived to be Stirner’s latent homosexuality, his concern
with sexual liberation and his determination to attack bourgeois

prejudice and puritanism: “Stirner was a precursor of May 68”."°

Fourier

Guérin was also a great admirer of Fourier, at least in so far as his
arguments in favour of sexual liberation and tolerance were con-
cerned: “I was as one with the genial Fourier when he ennobled and
sanctified all sexual acts, including those he termed “ambiguous”
[ie. homosexual].”4! Fourier himself was the victim of censorship
on the part of his own disciples, and his Nouveau monde amoureux,
written in 1816-18 but suppressed by the Phalansterians on the
grounds that it was immoral, was only published in 1967. Guérin
was delighted at its appearance:

‘The great utopian wants to see no form of attraction re-
pressed for, an ancestor of Freud, he is too well aware
of the psychological damage done by the constriction
of the instincts and how unhappy we are when we are
struggling against ourselves. Even more serious than

chiste d’expression francaise dans le monde, 1880-1983’ (Doctorat d’Etat, Univer-
sity of Provence, 1988), 7 vols; and my A History of the French Anarchist Movement,
1917-1945 (Westport, CT: Greenwood Press, 2002).

* Guérin, Ni Dieu ni Maitre, Anthologie de 'anarchisme (Paris: La Décou-
verte, 1999), voLI, p.12. Guérin began his anthology of anarchist texts — first pub-
lished in 1965 — with the ‘precursor’ Stirner; he also added an appendix on Stirner
to the 1981 edition of his short exposé, L’anarchisme: De la doctrine a la pratique.
See also Homosexualité et Révolution, p.12; and ‘Stirner, «Pére de l’anarchisme»?’
in La Rue no.26 (ler et 2éme trimestre 1979), pp.76

4. Homosexualité et Révolution, p.15.

18

are two separate sexes. For me, it is quite incomprehen-
sible and it seems to me that this is a result of a kind
of amputation carried out on this original being.

This “amputation” was something he felt in his own emotional
life. In the 1982 foreword for his 1929 novel La Vie selon la chair
[Life according to the flesh], Guérin spoke of the lead female char-
acter Hélé ne as representing “my own feminine side”. Of Hél ne
and her rival Hubert — rival for the affections of another man —
he wrote “I was at the same time Hélé ne and Hubert”. In the self-
questioning, self-critical text Eux et lui, he wrote — in the third
person — of the deep contradictions which he discerned in many
aspects of his personality:

His eroticism was no less contradictory [...]. He was
annoyed with girls for not having a phallus and with
boys for having no breasts. He resented girls for steal-
ing boys from him and boys for stealing girls from him.
The division of the sexes caused him a malaise which
was enough to destroy his joie de vivre and to alien-
ate him from the world. He tried to persuade himself
that this division was less definitive in nature than in
civilisation, that custom and fashion exagerated it, that
human emancipation was tending to reduce it, that
man is in woman and woman in man. He even tried
to savour the contrast and the diversity which are its
products and which ought to have consoled him for
the strange schism. But, the time not yet having come
for a certain reunification of the sexes, he was tired of
always hearing people talk of “man” when in fact he
very clearly had before his eyes two different species,

° ‘Géographie passionnelle d’une époque’, p.6. See also Homosexualité et
Révolution, p.16, note 2.

27
less his femininity, his betrayal of virility, supposedly
superior, for which the invert is not forgiven.*”

And asked by an interviewer for the gay magazine Homo 2000
why he thought there was so much hostility towards gay men, he
replied:

We live in post-patriarchal societies in which virility is
valued more highly than femininity. One could almost
say that the more heterosexual a man is, the more he
despises women. Certain men are not forgiven for be-
traying masculinity by desiring boys; I believe that is
the most fundamental reason.°°

Pursuing a similar argument in Eux et Lui, he concluded:

“woman had become my companion in adversity, my ally”!

Androgyny and bisexualism

The idea of some kind of original, pre-lapsarian androgyny was
one which interested and appealed to Guérin:

The Ancients believed in the myth according to which,
in the beginning, there existed a bisexual being who
was cut into two halves, each half corresponding to
one of the sexes. This image has always remained very
strong with me, and today, at the age of 74, I have still
not been able to come to terms with the idea that there

» ‘Kinsey’ in Homosexualité et Révolution, p.33.

® Entretiens avec Daniel Guérin’, Homo 2000 no.4, 3e trimestre 1979. A cor-
rected TS of the text of this interview can be found in Fonds Guérin, BDIC, F° A
721/15.

°! Fux et Lui’, in Son Testament (Paris: Encre, 1979), quoted in Homosexualité

et Révolution, pp.33-34

26

the individual suffering causing by the repression of
the passions are the effects on society. If they are held
in check, they immediately reappear in a more harm-
ful form which Fourier called “recurrent”, and it is then
and only then that they create disorder: “Any dammed
up passion produces its counter-passion which is as
harmful as the natural passion would have been bene-
ficial”?

Fourier thus lends support to Guérin’s critique of Proudhon’s
puritanism:

Thus the curse which Proudhon was to put on Eros
on the pretext of protecting industry had been refuted
in advance: in Harmony, the more each individual’s
tastes are satisfied, the better the community will be
served.¥

In 1975, Guérin published an anthology of Fourier’s texts on sex-
ual liberation, Vers la liberté en amour, witha lengthy preface which
included a detailed analysis of Fourier’s scattered and sketchy ref-
erences to homosexuality.“ Guérin was probably largely responsi-
ble for the new-found popularity of Fourier among the generation
of 68, and the same can be said to some extent of Wilhelm Reich,
with whom Guérin shared a taste for syntheses and the experience
of being condemned as a heretic simultaneously by the defenders
of two offended orthodoxies.

* Te nouveau monde amoureux de Fourier’ in Arcadie nos. 168 & 169 (1967
& 1968), pp.554-60 & 16-23, quote p.554.

*8 Te nouveau monde amoureux de Fourier’, p.560. ‘Harmonie’ was the
name given by Fourier to his ideal society. On Proudhon, see ‘Proudhon et l’amour
«unisexuel»’ in Arcadie nos.133 (January 1965) & 134 (February 1965), and Proud-
hon oui et non (Paris: Gallimard, 1978).

“4 Charles Fourier, Vers la liberté en amour (Paris: Gallimard, 1975); preface
by Guérin, pp.13-47.

19
The revolutionary potential of
psychoanalysis: the early Freud

For a synthesis of Marxism and
psychoanalysis: Reich

When Reich died in 1957, the event almost passed unnoticed in
France, and as Guérin remarked, “when I published his obituary,
those who learned nothing could have been counted on the fingers
of one hand’? Only two of Reich’s books had been translated into
French by the time of his death, so Guérin’s knowledge of German
gave him an advantage over most of his compatriots here as it did
in the study of Marxism. Guérin was particularly impressed by Re-
ich’s “Dialectical Materialism and Psychoanalysis” (first published
in 1934) and The Invasion of Compulsory Sex-Morality (1931).° Re-
ich was for Guérin the direct heir of an early, revolutionary Freud,
inspired by the 1907 essay, “Civilized” sexual morality and modern
nervous illness. What Guérin admired in Reich was his attack on
the socially conservative aspects of Freud’s theories, notably, again,
the notion of “sublimation”, that suppression of the sexual instinct
was necessary for civilisation, and Reich’s emphasis on “antisexual”
attitudes as being historically determined:

In his opinion, the repression of sexuality has social
and economic origins, not biological ones. Sexual
repressiveness appeared with the beginnings of class
society and the institution of private property and
patriarchy. It was installed by a particular social

* Wilhelm Reich aujourd’hui’ in Essai sur la révolution sexuelle, p.19.

“© In his 1968 talk on Reich — published as ‘Wilhelm Reich aujourd’hui’
— Guérin compared contemporary psychoanalysts’ distaste for ‘Civilized’ sex-
ual morality and modern nervous illness to contemporary trotskyists’ distaste
for Trotsky’s 1904 critique of Lenin’s organizational theses.- “Wilhelm Reich au-
jour@’hui’, p.21

20

So, argued Guérin, it was not only article 331 of the Penal Code
which must be attacked, but also all those concerning patriarchy:
the authority of the “head of the family”, divorce, contraception,
artifical insemination, abortion, prostitution and so on. The geneal-
ogy of the existing legal situation was clear: De Gaulle in Febru-
ary 1945 had perpetuated Pétain’s law of 1942, which itself must
be seen in the context of the reactionary Code de la Famille intro-
duced by decree in July 1939 and which attacked all sexual activity
outside of the family “where, according to our monogamous civili-
sation, sexual life must be enclosed*®

For Guérin, it was bourgeois society which was responsible for
the “detestable division of the sexes”, for pushing to an excessive
extent the differentiation between the sexes: “It has been happy to
reduce woman to the level of a doll, a “bimba”, a sexual object, a pin-
up girl, whilst simultaneously accentuating the opposite traits in
the male — macho, conceited, boorish and tyrannical”*” “Bourgeois
society, built on the family, will not readily give up on one of its
last ramparts.”°*

There were thus clear connections between patriarchal society’s
oppression of women and its oppression of homosexual men:

Patriarchal society, resting on the dual authority of the
man over the woman and of the father over the chil-
dren, accords primacy to the attributes and modes of
behaviour associated with virility. Homosexuality is
persecuted to the extent that it undermines this con-
struction. The disdain of which woman is the object
in patriarchal societies is not without correlation with
the shame attached to the homosexual act. It is doubt-

°° ‘La répression de l’homosexualité en France’, pp.1-2.

5” Homosexualité et Révolution, p.16.
38 Homosexualité et Révolution, p.8.

25
me sexe (published in 1949 and attacked by both communists and
catholics), and, as was made explicit in his 1969 essay on the sexual
revolution, he was clearly in favour of women’s sexual liberation:

La femme qui pendant des si écles, a été soumise”
a lesclavage du patriarcat, condamnée” a subir le
male,’ étre l’objet passif de son désir et de son choix,
privée par lui de la liberté sexuelle dont il s’arrogeait
le monopole, est en train de s’affranchir des dernié
res entraves psychologiques qui dénaturaient et
emprisonnaient sa sexualité. Elle sera (elle est déja)
tout aussi précoce dans sa vie sexuelle que homme,
tout aussi polyandre que homme est polygyne, tout
aussi capable que ’homme de s’intéresser” 4 la beauté
plastique du sexe opposé.*4

Several of Guérin’s later writings on sexuality and homosexual-
ity also raise the linked questions of gender identity and patriarchy.
In 1958, Guérin argued in a discussion of the repression of homo-
sexuality in France that the question had to be seen as just part of
a much broader set of issues:

I insist on maintaining that the homosexual cannot
and must not be seen as a separate problem, and that
the liberation of the homosexual must not be seen
as the egoistic demand of a minority. Homosexuality
is just a particular form, a variation, of sexuality
and must be considered in the broadest context. [...]
The prejudice with which this mode of behaviour is
besmirched derives, in large part, from patriarchal
society’s depreciation of femininity, considered as “in-
ferior”. Seen in this way, the cause of the homosexual

is the cause of woman.”

54 Essai sur la révolution sexuelle, p.79.
* ‘La répression de I’homosexualité en France’, p.1.

24

group, that of polygamous chiefs, in whose hands,
thanks to the accumulation of dowries paid by their
wives, economic power now resided. In modern times,
such repression remains indispensable in order to
safeguard the two essential institutions of society:
monogamous marriage and the family. It constitutes
one of the means of economic enslavement. The
sexual revolution is only possible through social
revolution.”

For Reich, Freud’s early theory of the libido and his courageous
attacks on antisexual oppression had been played down, sanitized
and rendered acceptable to the bourgeoisie — his clientele — such
that Reich could draw a parallel between the fate of psychoanal-
ysis at the hands of Freud and his successors and Marxism at the
hands of the reformist socialists and stalinist reaction. The erec-
tion of the “reality principle” into an absolute simply enabled it to
be used as a tool by the ruling class to maintain its domination
and to negate the revolutionary potential of psychoanalysis. Simi-
larly, the -dipus complex was seen as a biological given by Freud,
whereas for Reich it was the product of particular historically deter-
mined forms of society and the family: “Dans une société socialiste,
le complexe d’CEdipe doit disparaitre du fait mé me que sa base so-
ciale, la famille patriarcale, s’effondrera.”** Similarly, the theory of
the original murder of the father, reinforced the notion that patri-
archy and its antisexual ethic were part of human nature, rather
than historically determined.

Although Reich had, in his time, been attacked by both Marxists
and psychoanalysts, Guérin would insist in a debate in 1969 that it
was precisely this uncomfortable position astride both schools of
thought which was now his strong point. Psychoanalysis destroyed
the bases of religion and of bourgeois sexual morality in the same

“” Hommage A Wilhelm Reich’, pp.15-16.
“8 Hommage a Wilhelm Reich’, p.15.

21
way that Marxism destroyed outdated values through its material-
ist philosophy and through a revolution in the economic system:

A Marxism which sought to emancipate man [sic:
Phomme] without including sexuality in its analysis
and liberating man on the sexual level as well would
be disfiguring itself, it would be incomplete. A purely
biological or purely clinical sexology which paid
no attention to the social context and to dialectical
materialist analysis would produce only half-truths.”

Guérin’s only serious criticism of Reich was his relatively con-
servative position on homosexuality, namely that homosexuality
was an aberration caused by restrictions imposed on “normal” het-
erosexual relations. Here Guérin preferred the more libertarian im-
plications of Kinsey’s findings — although Kinsey himself was no
apologist for homosexuality and was criticized by Guérin for not
taking sufficient account of the socio-historical aspect of the ques-
tion. As for Reich’s later work, Guérin commented in an interview
for the first issue of the French journal Sexpol:

Jai admiré beucoup le Reich marxiste se dégageant
de lemprise stalinienne et, tout en restant marxiste,
développant ses idées et son action autour de Sexpol.
Par contre, partir de sa rupture comple te avec toute
notion marxiste, j'avoue que je me méfie, surout
parce que je n’ai pas les connaissances scientifiques
nécessaires pour émettre un jugement sur la théorie
de l’orgone.°°

* Wilhelm Reich aujourd’hui’ in Essai sur la révolution sexuelle, pp.17-28,

quote p.21. This is the text of an introduction to a debate organised in Brussels by
‘Liaison 20° on 29 November 1968.

°° Daniel Guérin «A confesse»’, interview with Gérard Ponthieu in La revue
Sexpol no.1 (20 January 1975), pp.10-14, quote p.14.

22

Women and patriarchy

Guérin has been criticized for neglecting the question of
women’s oppression:

The most serious difficulty raised in Guérin’s combi-
nation of radicalism and gayness is hardly touched on
in his memoirs. This is the simple issue of whether the
celebration of male homosexuality is supportive, or on
the contrary obstructive, in that larger question of sex-
ual politics: women’s emancipation.*!

It is certainly true that women are strikingly absent from
Guérin’s autobiographical writings, and that his representations
of the working class and of the world of work tend to be male-
centred and focussed on the point of production. Nor have I as yet
found any evidence in Guérin’s archives of any links with femi-
nists, apart from some brief correspondence with the American
trotskyist and feminist anthropologist, Evelyn Reed.*?

Nevertheless, as a historian of the French revolution Guérin did
resurrect the Société des femmes républicaines révolutionnaires, and
interpreted its destruction by the Robespierrists as a clear indicator
of reaction.*3 He was also, of course, a great popularizer of Fourier,
for whom, famously, the progress made by any society could be
measured in terms of the degree of emancipation of the women in
that society. He was an admirer of Simone de Beauvoir’s Le deuxié

*! Peter Sedgwick, ‘Out of hiding: the Comradeships of Daniel Guérin’ in
Robert Boyer, & Georges Steiner (eds.), Salmagundi: A quarterly journal of the
humanities and social sciences, 58-9, special issue on homosexualism (June 1982),
pp.197-220, quote p.215.

** Reed’s publications (all with Pathfinder, New York) include Woman’s Evo-
lution from Matriarchal Clan to Patriarchal Family (1992), Problems of Women’s
Liberation: A Marxist Approach (1972) and an introduction to a 1972 edition of
Engels’ The Origin of the Family.

53 La lutte de classes sous la Premiére République, 1793-1797 (Paris: Gallimard,
1968), vol, pp.271-8

23
﻿Minds and Machines (2020) 30:99–120
https://doi.org/10.1007/s11023-020-09517-8

The Ethics of AI Ethics: An Evaluation of Guidelines

Thilo Hagendorff1 

Received: 1 October 2019 / Accepted: 21 January 2020 / Published online: 1 February 2020 
© The Author(s) 2020

Abstract
Current advances in research, development and application of artificial intelligence 
(AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a 
number of ethics guidelines have been released in recent years. These guidelines 
comprise normative principles and recommendations aimed to harness the “disrup-
tive” potentials of new AI technologies. Designed as a semi-systematic evaluation, 
this paper analyzes and compares 22 guidelines, highlighting overlaps but also omis-
sions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also 
examine to what extent the respective ethical principles and values are implemented 
in the practice of research, development and application of AI systems—and how 
the effectiveness in the demands of AI ethics can be improved.

Keywords Artificial intelligence · Machine learning · Ethics · Guidelines · 
Implementation

1 Introduction

The current AI boom is accompanied by constant calls for applied ethics, which are 
meant to harness the “disruptive” potentials of new AI technologies. As a result, a 
whole body of ethical guidelines has been developed in recent years collecting prin-
ciples, which technology developers should adhere to as far as possible. However, 
the critical question arises: Do those ethical guidelines have an actual impact on 
human decision-making in the field of AI and machine learning? The short answer 
is: No, most often not. This paper analyzes 22 of the major AI ethics guidelines and 
issues recommendations on how to overcome the relative ineffectiveness of these 
guidelines.

AI ethics—or ethics in general—lacks mechanisms to reinforce its own nor-
mative claims. Of course, the enforcement of ethical principles may involve 

*  Thilo Hagendorff 
 thilo.hagendorff@uni-tuebingen.de

1 Cluster of Excellence “Machine Learning: New Perspectives for Science”, University 
of Tuebingen, Tübingen, Germany

Vol.:(012134 56789)



 100 T. Hagendorff 

reputational losses in the case of misconduct, or restrictions on memberships in 
certain professional bodies. Yet altogether, these mechanisms are rather weak and 
pose no eminent threat. Researchers, politicians, consultants, managers and activ-
ists have to deal with this essential weakness of ethics. However, it is also a reason 
why ethics is so appealing to many AI companies and institutions. When companies 
or research institutes formulate their own ethical guidelines, regularly incorporate 
ethical considerations into their public relations work, or adopt ethically motivated 
“self-commitments”, efforts to create a truly binding legal framework are continu-
ously discouraged. Ethics guidelines of the AI industry serve to suggest to legisla-
tors that internal self-governance in science and industry is sufficient, and that no 
specific laws are necessary to mitigate possible technological risks and to eliminate 
scenarios of abuse (Calo 2017). And even when more concrete laws concerning AI 
systems are demanded, as recently done by Google (2019), these demands remain 
relatively vague and superficial.

Science- or industry-led ethics guidelines, as well as other concepts of self-gov-
ernance, may serve to pretend that accountability can be devolved from state author-
ities and democratic institutions upon the respective sectors of science or industry. 
Moreover, ethics can also simply serve the purpose of calming critical voices from 
the public, while simultaneously the criticized practices are maintained within the 
organization. The association “Partnership on AI” (2018) which brings together 
companies such as Amazon, Apple, Baidu, Facebook, Google, IBM and Intel is 
exemplary in this context. Companies can highlight their membership in such asso-
ciations whenever the notion of serious commitment to legal regulation of business 
activities needs to be stifled.

This prompts the question as to what extent ethical objectives are actually imple-
mented and embedded in the development and application of AI, or whether merely 
good intentions are deployed. So far, some papers have been published on the sub-
ject of teaching ethics to data scientists (Garzcarek and Steuer 2019; Burton et al. 
2017; Goldsmith and Burton 2017; Johnson 2017) but by and large very little to 
nothing has been written about the tangible implementation of ethical goals and val-
ues. In this paper, I address this question from a theoretical perspective. In a first 
step, 22 of the major guidelines of AI ethics will be analyzed and compared. I will 
also describe which issues they omit to mention. In a second step, I compare the 
principles formulated in the guidelines with the concrete practice of research and 
development of AI systems. In particular, I critically examine to what extent the 
principles have an effect. In a third and final step, I will work out ideas on how AI 
ethics can be transformed from a merely discursive phenomenon into concrete direc-
tions for action.

2  Guidelines in AI Ethics

2.1  Method

Research in the field of AI ethics ranges from reflections on how ethical principles 
can be implemented in decision routines of autonomous machines (Anderson and 

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 101

Anderson 2015; Etzioni and Etzioni 2017; Yu et al. 2018) over meta-studies about 
AI ethics (Vakkuri and Abrahamsson 2018; Prates et  al. 2018; Boddington 2017; 
Greene et al. 2019; Goldsmith and Burton 2017) or the empirical analysis on how 
trolley problems are solved (Awad et al. 2018) to reflections on specific problems 
(Eckersley 2018) and comprehensive AI guidelines (The IEEE Global Initiative 
on Ethics of Autonomous and Intelligent Systems 2019). This paper mainly deals 
with the latter issue. The list of ethics guidelines considered in this article therefore 
includes compilations that cover the field of AI ethics as comprehensively as pos-
sible. To the best of my knowledge, a few preprints and papers are currently avail-
able, which also deal with the comparison of different ethical guidelines (Zeng et al. 
2018; Fjeld et  al. 2019; Jobin et  al. 2019). While especially the paper from Jobin 
et al. (2019) is a systematic scoping review of all the existing literature on AI eth-
ics, this paper does not aim at a full analysis of every available soft-law or non-legal 
norm document on AI, algorithm, robot, or data ethics, but rather a semi-systematic 
overview of issues and normative stances in the field, demonstrating how the details 
of AI ethics relate to a bigger picture.

The selection and compilation of 22 major ethical guidelines were based on a 
literature analysis. This selection was undertaken in two phases. In the first phase, 
I searched different databases, namely Google, Google Scholar, Web of Science, 
ACM Digital Library, arXiv, and SSRN for hits or articles on “AI ethics”, “arti-
ficial intelligence ethics”, “AI principles”, “artificial intelligence principles”, “AI 
guidelines”, and “artificial intelligence guidelines, following every link in the first 
25 search results, while at the same time ignoring duplicates in the search process. 
During the analysis of the search results, I also sifted through the references in order 
to manually find further relevant guidelines. Furthermore, I used Algorithm Watch’s 
AI Ethics Guidelines Global Inventory, a crowdsourced, comprehensive list of ethics 
guidelines, to check whether I missed relevant guidelines. Via the list, I found three 
further guidelines that meet the criteria for the selection. In this context, a shortcom-
ing one has to consider is that my selection is biased towards documents which are 
western/northern in nature, excluding guidelines which are not written in English.

I rejected all documents older than 5  years in order to only take guidelines 
into account that are relatively new. Documents that only refer to a national con-
text—such as for instance position papers of national interest groups (Smart 
Dubai Smart Dubai 2018), the report of the British House of Lords (Bakewell 
et al. 2018), or the Nordic engineers’ stand on Artificial Intelligence and Ethics 
(Podgaiska and Shklovski)—were excluded from the compilation. Nevertheless, 
I included the European Commission’s “Ethics Guidelines for Trustworthy AI” 
(Pekka et al. 2018), the Obama administration’s “Report on the Future of Artifi-
cial Intelligence” (Holdren et al. 2016), and the “Beijing AI Principles” (Beijing 
Academy of Artificial Intelligence 2019), which are backed by the Chinese Min-
istry of Science and Technology. I have included these three guidelines because 
they represent the three largest AI “superpowers”. Furthermore, I included the 
“OECD Principles on AI” (Organisation for Economic Co-operation and Devel-
opment 2019) due to their supranational character. Scientific papers or texts that 
fall into the category of AI ethics but focus on one or more specific aspects of 
the topic were not considered either. The same applies to guidelines or toolkits, 

1 3



 102 T. Hagendorff 

which are not specifically about AI but rather about big data, algorithms or robot-
ics (Anderson et al. 2018; Anderson and Anderson 2011). I further excluded cor-
porate policies, with the exception of the “Information Technology Industry AI 
Policy Principles” (2017), the principles of the “Partnership on AI” (2018), the 
IEEE first and second version of the document on “Ethically Aligned Design” 
(The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems 
2016, 2019), as well as the brief principle lists of Google (2018), Microsoft 
(2019), DeepMind (DeepMind), OpenAI (2018), and IBM (Cutler et  al.  2018) 
which have become well-known through media coverage. Other large companies 
such as Facebook or Twitter have not yet published any systematic AI guidelines, 
but only isolated statements of good conduct. Paula Boddington’s book on ethical 
guidelines (2017) funded by the Future of Life Institute was also not considered 
as it merely repeats the Asilomar principles (2017).

The decisive factor for the selection of ethics guidelines was not the depth of 
detail of the individual document, but the discernible intention of a comprehen-
sive mapping and categorization of normative claims with regard to the field of 
AI ethics. In Table 1, I only inserted green markers if the corresponding issues 
were explicitly discussed in one or more paragraphs. Isolated mentions without 
further explanations were not considered, unless the analyzed guideline is so 
short that it consists entirely of brief mentions altogether.

Table 1  Overview of AI ethics guidelines and the different issues they cover

 

(The IEEE (The IEEE 
(Organisao Global Global 

(Beijing n for Iniave on Iniave on (Informaon 
Academy of Economic Co- (Future of Ethics of Ethics of Technology (Microso 

(Pekka et al. (Holdren et (Brundage et (Floridi et al. (Crawford et (Campolo et (Whiaker (Crawford et (Diakopoulo (Abrassart (OpenAI (Google (Cutler et al. (Partnership 
authors Arficial operaon Life Instute Autonomus Autonomus Industry Corporaon (DeepMind)

2018) al. 2016) al. 2018) 2018) al. 2016) al. 2017) et al. 2018) al. 2019) s et al.) et al. 2018) 2018) 2018) 2018) on AI 2018)
Intelligence and 2017) and and Council 2019)

2019) Developmen Intelligent Intelligent 2017)
t 2019) Systems Systems 

2016) 2019)
meta- principles of 

code of several detailed detailed several several IBM’s short 
analysis brief short list of an 

analysis of large statements statements statements statements ethics short descripon descripon short short list of 
about principles of guideline keywords associaon 

AI principles AI principles AI prinicples AI principles abuse collecon of on social on social on social on social released by principles of ethical of ethical principles principles keywords 
key issue principles the FAT ML about basic for the between 

of the EU of the US of China of the OECD scenarios of different implicaons implicaons implicaons implicaons the for the aspects in aspects in for the for the for the 
for the community ethical ethical use several 

AI principles of AI of AI of AI of AI Université ethical use the context the context ethical use ethical use ethical use 
beneficial principles of AI industry 

de Montréal of AI of AI of AI of AI of AI of AI
use of AI leaders

privacy protecon x x x x x x x x x x x x x x x x x x 18

fairness, non-discriminaon, jusce x x x x x x x x x x x x x x x x x x 18

accountability x x x x x x x x x x x x x x x x x 17

transparency, openness x x x x x x x x x x x x x x x x 16

safety, cybersecurity x x x x x x x x x x x x x x x x 16

common good, sustainability, well-being x x x x x x x x x x x x x x x x 16

human oversight, control, auding x x x x x x x x x x x x 12

solidarity, inclusion, social cohesion x x x x x x x x x x x 11

explainability, interpretabiliy x x x x x x x x x x 10

science-policy link x x x x x x x x x x 10

legislave framework, legal status of AI systems x x x x x x x x x x 10

future of employment/worker rights x x x x x x x x x 9

responsible/intensified research funding x x x x x x x x 8

public awareness, educaon about AI and its risks x x x x x x x x 8

dual-use problem, military, AI arms race x x x x x x x x 8

field-specific deliberaons (health, military, mobility etc.) x x x x x x x x 8

human autonomy x x x x x x x 7

diversity in the field of AI x x x x x x x 7

cerficaon for AI products x x x x 4

protecon of whistleblowers x x x 3

cultural differences in the ethically aligned design of AI 
x x 2

systems
hidden costs (labeling, clickwork, contend moderaon, 

x x 2
energy, resources)

yes, but very 
notes on technical implementaons none none none yes none none none none none none none none none none none none none none none none none

few

varies in varies in 
proporon of women among authors (f/m) (8/10) (2/3) ns ns (5/21) (5/8) ns (4/2) (3/1) (6/4) (12/4) (1/12) (8/10) ns ns ns ns ns (1/2) ns (55/77)

each chapter each chapter

length (number of words) 16546 22787 766 3249 34017 8609 646 11530 18273 25759 38970 1359 4754 441 40915 108.092 2272 75 417 882 4488 1481
science/

affiliaon (government, industry, science) government government government science science science science science science science science science non-profit industry industry industry industry industry industry industry industry
gov./ind.

number of ethical aspects 9 12 13 12 8 14 12 13 9 12 13 5 11 4 14 18 9 6 6 6 6 8

1 3

number of menons

Partnership on AI

Everyday Ethics for Arficial Intelligence

Arficial Intelligence at Google

DeepMind Ethics & Society Principles

Microso AI principles

ITI AI Policy Principles

Ethically Aligned Design: A Vision for Priorizing 
Human Well-being with Autonomous and 

Intelligent Systems (First Edion)

Ethically Aligned Design: A Vision for Priorizing 
Human Well-being with Autonomous and 

Intelligent Systems (Version for Public Discussion)

OpenAI Charter

Montréal Declaraon for Responsible 
Development of Arficial Intelligence

Principles for Accountable Algorithms and a Social 
Impact Statement for Algorithms

AI Now 2019 Report

AI Now 2018 Report

AI Now 2017 Report

AI Now 2016 Report

The Asilomar AI Principles

AI4People

The Malicious Use of Arficial Intelligence

OECD Recommendaon of the Council on Arficial 
Intelligence

Beijing AI Principles

Report on the Future of Arficial Intelligence

The European Commission's High-Level Expert 
Group on Arficial Intelligence



The Ethics of AI Ethics: An Evaluation of Guidelines 103

2.2 M ultiple Entries

As shown in Table  1, several issues are unsurprisingly recurring across vari-
ous guidelines. Especially the aspects of accountability, privacy or fairness 
appear altogether in about 80% of all guidelines and seem to provide the mini-
mal requirements for building and using an “ethically sound” AI system. What is 
striking here is the fact that the most frequently mentioned aspects are those for 
which technical fixes can be or have already been developed. Enormous technical 
efforts are undertaken to meet ethical targets in the fields of accountability and 
explainable AI (Mittelstadt et al. 2019), fairness and discrimination aware data 
mining (Gebru et al. 2018), as well as privacy (Baron and Musolesi 2017). Many 
of those endeavors are unified under the FAT ML or XAI community (Veale and 
Binns 2017; Selbst et  al. 2018). Several tech-companies already offer tools for 
bias mitigation and fairness in machine learning. In this context, Google, Micro-
soft and Facebook have issued the “AI Fairness 360” tool kit, the “What-If Tool”, 
“Facets”, “fairlern.py” and “Fairness Flow”, respectively (Whittaker et al. 2018).

Accountability, explainability, privacy, justice, but also other values such as 
robustness or safety are most easily operationalized mathematically and thus tend 
to be implemented in terms of technical solutions. With reference to the find-
ings of psychologist Carol Gilligan, one could argue at this point that the way AI 
ethics is performed and structured constitutes a typical instantiation of a male-
dominated justice ethics (Gilligan 1982). In the 1980s, Gilligan demonstrated in 
empirical studies that women do not, as men typically do, address moral problems 
primarily through a “calculating”, “rational”, “logic-oriented” ethics of justice, 
but rather interpret them within a wider framework of an “empathic”, “emotion-
oriented” ethics of care. In fact, no different from other parts of AI research, the 
discourse on AI ethics is also primarily shaped by men. My analysis of the distri-
bution of female and male authors of the guidelines, as far as authors were indi-
cated in the documents, showed that the proportion of women was 41.7%. This 
ratio appears to be close to balance. However, it should be considered that the 
ratio of female to male authors is reduced to a less balanced 31.3% if the four AI 
Now Reports are discarded, which come from an organization that is deliberately 
led by women. The proportion of women is lowest at 7.7% in the FAT ML com-
munity’s guidelines which are focused predominantly on technical solutions (Dia-
kopoulos et al.). Accordingly, the “male way” of thinking about ethical problems 
is reflected in almost all ethical guidelines by way of mentioning aspects such as 
accountability, privacy or fairness. In contrast, almost no guideline talks about 
AI in contexts of care, nurture, help, welfare, social responsibility or ecological 
networks. In AI ethics, technical artefacts are primarily seen as isolated entities 
that can be optimized by experts so as to find technical solutions for technical 
problems. What is often lacking is a consideration of the wider contexts and the 
comprehensive relationship networks in which technical systems are embedded. 
In accordance with that, it turns out that precisely the reports of AI Now (Craw-
ford et al. 2016, 2019; Whittaker et al. 2018; Campolo et al. 2017), an organiza-
tion primarily led by women, do not conceive AI applications in isolation, but 
within a larger network of social and ecological dependencies and relationships 

1 3



 104 T. Hagendorff 

(Crawford and Joler 2018), corresponding most closely with the ideas and tenets 
of an ethics of care (Held 2013).

What are further insights from my analysis of the ethics guidelines, as summa-
rized in Table 1? On the one hand, it is noticeable that guidelines from industrial 
contexts name on average 9.1 distinctly separated ethical aspects, whereas the aver-
age for ethics codes from science is 10.8. The principles of Microsoft’s AI ethics are 
the most brief and minimalistic (Microsoft Corporation 2019). The OpenAI Charta 
names only four points and is thus situated at the bottom of the list (OpenAI 2018). 
Conversely, the IEEE guideline contains the largest volume with more than 100.000 
words (The IEEE Global Initiative on Ethics of Autonomous and Intelligent Sys-
tems 2019). Finally, yet importantly, it is noteworthy that almost all guidelines sug-
gest that technical solutions exist for many of the problems described. Neverthe-
less, there are only two guidelines which contain genuinely technical explanations at 
all—albeit only very sparsely. The authors of the guideline on the “Malicious Use of 
AI” provide the most extensive commentary here (Brundage et al. 2018).

2.3 O missions

Despite the fact that the guidelines contain various parallels and several recurring 
topics, what are issues the guidelines do not discuss at all or only very occasion-
ally? Here, I want to give a (non-exhaustive) overview of issues that are missing. 
Two things should be considered in this context. First, the sampling method used 
to select the AI ethics guidelines has an effect on the list of issues and omissions. 
When deliberately excluding for instance robot ethics guidelines, this has the effect 
that the list of entries lacks issues that are connected with robotics. Second, not all 
omissions can be treated equally. There are omissions which are missing or severely 
underrepresented without any good reason—for instance the aspect of politi-
cal abuse or “hidden” social and ecological costs of AI systems—, and omissions 
that can be justified—for instance deliberations on artificial general intelligence or 
machine consciousness, since those technologies are purely speculative.

Nevertheless, in view of the fact that significant parts of the AI community 
see the emergence of artificial general intelligence as well as associated dangers 
for humanity or existential threats as a likely scenario (Müller and Bostrom 2016; 
Bostrom 2014; Tegmark 2017; Omohundro 2014), one could argue that those top-
ics could be discussed in ethics guidelines under the umbrella of potential prohibi-
tions to pursue certain research strands in this area (Hagendorff 2019). The fact that 
artificial general intelligence is not discussed in the guidelines may be due to the 
fact that most of the guidelines are not written by research groups from philosophy 
or other speculative disciplines, but by researchers with a background directly in 
computer science or its application. In this context, it is noteworthy that the fear 
of the emergence of superintelligence is more frequently expressed by people who 
lack technical experience in the field of AI—one just has to think of people like 
Stephen Hawking, Elon Musk or Bill Gates—while “real” experts generally regard 
the idea of a strong AI as rather absurd (Calo 2017, 26). Perhaps the same holds true 
for the question of machine consciousness and the ethical problems associated with 

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 105

it (Lyons 2018), as this topic is also omitted from all examined ethical guidelines. 
What is also striking is the fact that only the Montréal Declaration for Responsible 
Development of Artificial Intelligence (2018) as well as the AI Now 2019 Report 
(2019) explicitly addresses the aspect of democratic control, governance and politi-
cal deliberation of AI systems. The mentioned documents are also the only guide-
lines that explicitly prohibits imposing certain lifestyles or concepts of “good living” 
on people by AI systems, as it is for example demonstrated in the Chinese scoring 
system (Engelmann et al. 2019). The former document further criticizes the appli-
cation of AI systems for the reduction of social cohesion, for example by isolating 
people in echo chambers (Flaxman et  al. 2016). In addition, hardly any guideline 
discusses the possibility for political abuse of AI systems in the context of automated 
propaganda, bots, fake news, deepfakes, micro targeting, election fraud, and the like. 
What is also largely absent from most guidelines is the issue of a lack in diversity 
within the AI community. This lack of diversity is prevailing in the field of artificial 
intelligence research and development, as well as in the workplace cultures shaping 
the technology industry. In the end, a relatively small group of predominantly white 
men determines how AI systems are designed, for what purposes they are optimized, 
what is attempted to realize technically, etc. The famous AI startup “nnaisense” run 
by Jürgen Schmidhuber, which aims at generating an artificial general intelligence, 
to name just one example, employs only two women—one scientist and one office 
manager—in its team, but 21 men. Another matter, which is not covered at all or 
only very rarely mentioned in the guidelines, are aspects of robot ethics. As men-
tioned in the methods chapter, specific guidelines for robot ethics exist, most prom-
inently represented by Asimov’s three laws of robotics (Asimov 2004), but those 
guidelines were intentionally excluded from the analysis. Nonetheless, advances 
in AI research contribute, for instance, to increasingly anthropomorphized techni-
cal devices. The ethical question that arises in this context echoes Immanuel Kant’s 
“brutalization argument” and states that the abuse of anthropomorphized agents—
as, for example, is the case with language assistants (Brahnam 2006)—also pro-
motes the likelihood of violent actions between people (Darling 2016). Apart from 
that, the examined ethics guidelines pay little attention to the rather popular trolley 
problems (Awad et al. 2018) and their alleged relation to ethical questions surround-
ing self-driving cars or other autonomous vehicles. In connection to this, no guide-
line deals in detail with the obvious question where systems of algorithmic deci-
sion making are superior or inferior, respectively, to human decision routines. And 
finally, virtually no guideline deals with the “hidden” social and ecological costs 
of AI systems. At several points in the guidelines, the importance of AI systems for 
approaching a sustainable society is emphasized (Rolnick et al. 2019). However, it 
is omitted—with the exception of the AI Now 2019 Report (2019)—that producer 
and consumer practices in the context of AI technologies may in themselves contra-
dict sustainability goals. Issues such as lithium mining, e-waste, the one-way use of 
rare earth minerals, energy consumption, low-wage “clickworkers” creating labels 
for data sets or doing content moderation are of relevance here (Crawford and Joler 
2018; Irani 2016; Veglis 2014; Fang 2019; Casilli 2017). Although “clickwork” is a 
necessary prerequisite for the application of methods of supervised machine learn-
ing, it is associated with numerous social problems (Silberman et  al. 2018; Irani 

1 3



 106 T. Hagendorff 

2015; Graham et al. 2017), such as low wages, work conditions and psychological 
work consequences, which tend to be ignored by the AI community. Finally, yet 
importantly, not a single guideline raises the issue of public–private partnerships 
and industry-funded research in the field of AI. Despite the massive lack of trans-
parency regarding the allocation of research funds, it is no secret that large parts of 
university AI research are financed by corporate partners. In light of this, it remains 
questionable to what extent the ideal of freedom of research can be upheld—or 
whether there will be a gradual “buyout” of research institutes.

3  AI in Practice

3.1 B usiness Versus Ethics

The close link between business and science is not only revealed by the fact that all 
of the major AI conferences are sponsored by industry partners. The link between 
business and science is also well illustrated by the AI Index 2018 (Shoham et  al. 
2018). Statistics show that, for example, the number of corporate-affiliated AI papers 
has grown significantly in recent years. Furthermore, there is a huge growth in the 
number of active AI startups, each supported by huge amounts of annual funding 
from Venture Capital firms. Tens of thousands of AI-related patents are registered 
each year. Different industries are incorporating AI applications in a broad variety of 
fields, ranging from manufacturing, supply-chain management, and service develop-
ment, to marketing and risk assessment. All in all, the global AI market comprises 
more than 7 billion dollars (Wiggers 2019).

A critical look at this global AI market and the use of AI systems in the econ-
omy and other social systems sheds light primarily on unwanted side effects of the 
use of AI, as well as on directly malevolent contexts of use. These occur in various 
areas (Pistono and Yampolskiy 2016; Amodei et  al. 2017). Leading, of course, is 
the military use of AI in cyber warfare or regarding weaponized unmanned vehicles 
or drones (Ernest and Carroll 2016; Anderson and Waxman 2013). According to 
media reports, the US government alone intends to invest two billion dollars in mili-
tary AI projects over the next 5 years (Fryer-Biggs 2018). Moreover, governments 
can use AI applications for automated propaganda and disinformation campaigns 
(Lazer et  al. 2018), social control (Engelmann et  al. 2019), surveillance (Helbing 
2019), face recognition or sentiment analysis (Introna and Wood 2004), social sort-
ing (Lyon 2003), or improved interrogation techniques (McAllister 2017). Notwith-
standing the above, companies can cause massive job losses due to AI implementa-
tion (Frey and Osborne 2013), conduct unmonitored forms of AI experiments on 
society without informed consent (Kramer et  al. 2014), suffer from data breaches 
(Schneier 2018), use unfair, biased algorithms (Eubanks 2018), provide unsafe AI 
products (Sitawarin et al. 2018), use trade secrets to disguise harmful or flawed AI 
functionalities (Whittaker et al. 2018), rush to integrate and put immature AI appli-
cations on the market and many more. Furthermore, criminal or black-hat hackers 
can use AI to tailor cyberattacks, steal information, attack IT infrastructures, rig 
elections, spread misinformation for example through deepfakes, use voice synthesis 

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 107

technologies for fraud or social engineering (Bendel 2017), or disclose personal 
traits that are actually secret or private via machine learning applications (Kosinski 
and Wang 2018; Kosinski et al. 2013, 2015). All in all, only a very small number of 
papers is published about the misuse of AI systems, even though they impressively 
show what massive damage can be done with those systems (Brundage et al. 2018; 
King et al. 2019; O’Neil 2016).

3.2 A I Race

While the United States currently has the largest number of start-ups, China claims 
to be the “world leader in AI” in 2030 (Abacus 2018). This claim is supported by 
the sheer amount of data that China has at its disposal to train its own AI systems, 
as well as by the large label companies that take over the manual preparation of 
data sets for supervised machine learning (Yuan 2018). Conversely, China is seen 
to have a weakness vis-à-vis the USA in that the investments of the market lead-
ers Baidu, Alibaba and Tencent are too application-oriented comprising areas such 
as autonomous driving, finance or home appliances, while important basic research 
on algorithm development, chip production or sensor technology is neglected (Hao 
2019). The constant comparison between China, the USA and Europe renders the 
fear of being inferior to each other an essential motive for efforts in the research and 
development of artificial intelligence.

Another justification for competitive thinking is provided by the military context. 
If the own “team”, framed in a nationalist way, does not keep pace, so the consid-
eration, it will simply be overrun by the opposing “team” with superior AI military 
technology. In fact, potential risks emerge from the AI race narrative, as well as 
from an actual competitive race to develop AI systems for technological superior-
ity (Cave and ÓhÉigeartaigh 2018). One risk of this rhetoric is that “impediments” 
in the form of ethical considerations will be eliminated completely from research, 
development and implementation. AI research is not framed as a cooperative global 
project, but as a fierce competition. This competition affects the actions of individu-
als and promotes a climate of recklessness, repression, and thinking in hierarchies, 
victory and defeat. The race for the best AI, whether a mere narrative or a harsh 
reality, reduces the likelihood of the establishment of technical precaution measures 
as well as of the development of benevolent AI systems, cooperation, and dialogue 
between research groups and companies. Thus, the AI race stands in stark contrast to 
the idea of developing an “AI4people” (Floridi et al. 2018). The same holds true for 
the idea of an “AI for Global Good”, as was proposed at the 2017’s ITU summit, or 
the large number of leading AI researchers who signed the open letter of the “Future 
of Life Institute”, embracing the norm that AI should be used for prosocial purposes.

Despite the downsides, in less public discourses and in concrete practice, an AI 
race has long since established itself. Along with that development, in- and out-
group-thinking has intensified. Competitors are seen more or less as enemies or at 
least as threats against which one has to defend oneself. Ethics, on the other hand, in 
its considerations and theories always stresses the danger of an artificial differentia-
tion between in- and outgroups (Derrida 1997). Constructed outgroups are subject 

1 3



 108 T. Hagendorff 

to devaluation, are perceived de-individualized and in the worst case can become 
victims of violence simply because of their status as “others” (Mullen and Hu 1989; 
Vaes et al. 2014). I argue that only by abandoning such thinking in- and outgroups 
may the AI race be reframed into a global cooperation for beneficial and safe AI.

3.3  Ethics in Practice

Do ethical guidelines bring about a change in individual decision-making regard-
less of the larger social context? In a recent controlled study, researchers critically 
reviewed the idea that ethical guidelines serve as a basis for ethical decision-making 
for software engineers (McNamara et al. 2018). In brief, their main finding was that 
the effectiveness of guidelines or ethical codes is almost zero and that they do not 
change the behavior of professionals from the tech community. In the survey, 63 
software engineering students and 105 professional software developers were scruti-
nized. They were presented with eleven software-related ethical decision scenarios, 
testing whether the influence of the ethics guideline of the Association for Comput-
ing Machinery (ACM) (Gotterbarn et al. 2018) in fact influences ethical decision-
making in six vignettes, ranging from responsibility to report, user data collection, 
intellectual property, code quality, honesty to customer to time and personnel man-
agement. The results are disillusioning: “No statistically significant difference in the 
responses for any vignette were found across individuals who did and did not see the 
code of ethics, either for students or for professionals.” (McNamara et al. 2018, 4).

Irrespective of such considerations on the microsociological level, the rela-
tive ineffectiveness of ethics can also be explained at the macrosociological level. 
Countless companies are eager to monetize AI in a huge variety of applications. 
This strive for a profitable use of machine learning systems is not primarily framed 
by value- or principle-based ethics, but obviously by an economic logic. Engineers 
and developers are neither systematically educated about ethical issues, nor are they 
empowered, for example by organizational structures, to raise ethical concerns. In 
business contexts, speed is everything in many cases and skipping ethical considera-
tions is equivalent to the path of least resistance. Thus, the practice of development, 
implementation and use of AI applications has very often little to do with the values 
and principles postulated by ethics. The German sociologist Ulrich Beck once stated 
that ethics nowadays “plays the role of a bicycle brake on an intercontinental air-
plane” (Beck 1988, 194). This metaphor proves to be particularly true in the context 
of AI, where huge sums of money are invested in the development and commercial 
utilization of systems based on machine learning (Rosenberg 2017), while ethical 
considerations are mainly used for public relations purposes (Boddington 2017, 56).

In their AI Now 2017 Report, Kate Crawford and her team state that ethics and 
forms of soft governance “face real challenges” (Campolo et al. 2017, 5). This is mainly 
due to the fact that ethics has no enforcement mechanisms reaching beyond a voluntary 
and non-binding cooperation between ethicists and individuals working in research and 
industry. So what happens is that AI research and development takes place in “closed-
door industry settings”, where “user consent, privacy and transparency are often over-
looked in favor of frictionless functionality that supports profit-driven business models” 

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 109

(Campolo et al. 2017, 31 f.). Despite this dispensation of ethical principles, AI systems 
are used in areas of high societal significance such as health, police, mobility or edu-
cation. Thus, in the AI Now Report 2018, it is repeated that the AI industry “urgently 
needs new approaches to governance”, since, “internal governance structures at most 
technology companies are failing to ensure accountability for AI systems” (Whittaker 
et al. 2018, 4). Thus, ethics guidelines often fall into the category of a “’trust us’ form 
of [non-binding] corporate self-governance” (Whittaker et  al. 2018, 30) and people 
should “be wary of relying on companies to implement ethical practices voluntarily” 
(Whittaker et al. 2018, 32).

The tension between ethical principles and wider societal interests on the one hand, 
and research, industry, and business objectives on the other can be explained with 
recourse to sociological theories. Especially on the basis of system theory it can be 
shown that modern societies differ in their social systems, each working with their own 
codes and communication media (Luhmann 1984, 1997, 1988). Structural couplings 
can lead decisions in one social system to influence other social systems. Such cou-
plings, however, are limited and do not change the overall autonomy of social systems. 
This autonomy, which must be understood as an exclusive, functionalist orientation 
towards the system’s own codes is also manifested in the AI industry, business and sci-
ence. All these systems have their own codes, their own target values, and their own 
types of economic or symbolic capital via which they are structured and based upon 
which decisions are made (Bourdieu 1984). Ethical intervention in those systems is 
only possible to a very limited extent (Hagendorff 2016). A certain hesitance exists 
towards every kind of intervention as long as these lie beyond the functional laws of 
the respective systems. Despite that, unethical behavior or unethical intentions are not 
solely caused by economic incentives. Rather, individual character traits like cognitive 
moral development, idealism, or job satisfaction play a role, let alone organizational 
environment characteristics like an egoistic work climate or (non-existent) mechanisms 
for the enforcement of ethical codes (Kish-Gephart et  al. 2010). Nevertheless, many 
of these factors are heavily influenced by the overall economic system logic. Ethics is 
then, so to speak, “operationally effectless” (Luhmann 2008).

And yet, such system-theoretical considerations apply only on a macro level of 
observation and must not be generalized. Deviations from purely economic behavioral 
logics in the tech industry occur as well, for example when Google withdrew from the 
military project “Maven” after protests from employees (Statt 2018) or when people at 
Microsoft protested against the company’s cooperation with Immigration and Customs 
Enforcement (ICE) (Lecher 2018). Nevertheless, it must also be kept in mind here that, 
in addition to genuine ethical motives, the significance of economically relevant reputa-
tion losses should not be underestimated. Hence, the protest against unethical AI pro-
jects can in turn be interpreted in an economic logic, too.

3.4  Loyalty to Guidelines

As indicated in the previous sections, the practice of using AI systems is poor in terms 
of compliance with the principles set out in the various ethical guidelines. Great pro-
gress has been made in the areas of privacy, fairness or explainability. For example, 

1 3



 110 T. Hagendorff 

many privacy-friendly techniques for the use of data sets and learning algorithms have 
been developed, using methods where AI systems’ “sight” is “darkened” via cryptogra-
phy, differential or stochastic privacy (Ekstrand et al. 2018; Baron and Musolesi 2017; 
Duchi et al. 2013; Singla et al. 2014). Nevertheless, this contradicts the observation that 
AI has been making such massive progress for several years precisely because of the 
large amounts of (personal) data available. Those data are collected by privacy-invasive 
social media platforms, smartphone apps, as well as Internet of Things devices with its 
countless sensors. In the end, I would argue that the current AI boom coincides with 
the emergence of a post-privacy society. In many respects, however, this post-privacy 
society is also a black box society (Pasquale 2015), in which, despite technical and 
organizational efforts to improve explainability, transparency and accountability, mas-
sive zones of non-transparency remain, caused both by the sheer complexity of techno-
logical systems and by strategic organizational decisions.

For many of the issues mentioned in the guidelines, it is difficult to assess the extent 
to which efforts to meet the set objectives are successful or whether conflicting trends 
prevail. This is the case in the areas of safety and cybersecurity, the science-policy link, 
future of employment, public awareness about AI risks, or human oversight. In other 
areas, including the issue of hidden costs and sustainability, the protection of whistle-
blowers, diversity in the field of AI, the fostering of solidarity and social cohesion, the 
respect for human autonomy, the use of AI for the common good or the military AI 
arms race, it can certainly be stated that the ethical goals are being massively undera-
chieved. One only has to think of the aspect of gender diversity: Even though ethical 
guidelines clearly demand its improvement, the state of affairs is that on average 80% 
of the professors at the world’s leading universities such as Stanford, Oxford, Berkeley 
or the ETH are male (Shoham et al. 2018). Furthermore, men make up more than 70% 
of applicants for AI jobs in the U.S. (Shoham et al. 2018). Alternatively, one can take 
human autonomy: As repeatedly demanded in various ethical guidelines, people should 
not be treated as mere data subjects, but as individuals. In fact, however, countless 
examples show that computer decisions, regardless of their susceptibility to error, are 
attributed a strong authority which results in the ignorance of individual circumstances 
and fates (Eubanks 2018). Furthermore, countless companies strive for the opposite of 
human autonomy, employing more and more subtle techniques for manipulating user 
behavior via micro targeting, nudging, UX-design and so on (Fogg 2003; Matz et al. 
2017). Another example is that of cohesion: Many of the major scandals of the last 
years would have been unthinkable without the use of AI. From echo chamber effects 
(Pariser 2011) to the use of propaganda bots (Howard and Kollanyi 2016), or the spread 
of fake-news (Vosoughi et al. 2018), AI always played a key role to the effect of dimin-
ishing social cohesion, fostering instead radicalization, the decline of reason in public 
discourse and social divides (Tufekci 2018; Brady et al. 2017).

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 111

4  Advances in AI Ethics

4.1 T echnical Instructions

Given the relative lack of tangible impact of the normative objectives set out in 
the guidelines, the question arises as to how the guidelines could be improved to 
make them more effective. At first glance, the most obvious potential for improve-
ment of the guidelines is probably to supplement them with more detailed tech-
nical explanations—if such explanations can be found. Ultimately, it is a major 
problem to deduce concrete technological implementations from the very abstract 
ethical values and principles. What does it mean to implement justice or trans-
parency in AI-systems? What does a “human-centered” AI look like? How can 
human oversight be obtained? The list of questions could easily be continued.

The ethics guidelines examined refer exclusively to the term “AI”. They never or 
very seldom use more specific terminology. However, “AI” is just a collective term 
for a wide range of technologies or an abstract large-scale phenomenon. The fact that 
not a single prominent ethical guideline goes into greater technical detail shows how 
deep the gap is between concrete contexts of research, development, and application 
on the one side, and ethical thinking on the other. Ethicists must partly be capable 
of grasping technical details with their intellectual framework. That means reflecting 
on the ways data are generated, recorded, curated, processed, disseminated, shared, 
and used (Bruin and Floridi 2017), on the ways of designing algorithms and code, 
respectively (Kitchin 2017; Kitchin and Dodge 2011), or on the ways training data 
sets are selected (Gebru et al. 2018). In order to analyze all this in sufficient depth, 
ethics has to partially transform to “microethics”. This means that at certain points, 
a substantial change in the level of abstraction has to happen insofar as ethics aims 
to have a certain impact and influence in the technical disciplines and the practice of 
research and development of artificial intelligence (Morley et al. 2019). On the way 
from ethics to “microethics”, a transformation from ethics to technology ethics, to 
machine ethics, to computer ethics, to information ethics, to data ethics has to take 
place. As long as ethicists refrain from doing so, they will remain visible in a gen-
eral public, but not in professional communities.

A good example of such a microethical work which can be implemented eas-
ily and concretely in practice is the paper by Gebru et al. (2018). The research-
ers propose the introduction of standardized datasheets listing the properties of 
different training data sets, so that machine learning-practitioners can check to 
what extent certain data sets are best suitable for their purposes, what the origi-
nal intention was when the data set was created, what data the data set is com-
posed of, how the data was collected and pre-processed, etc. The paper by Gebru 
et al. makes it possible for practitioners to obtain a more informed decision on the 
selection of certain training data sets, so that supervised machine learning ulti-
mately becomes fairer, and more transparent, and avoids cases of algorithmic dis-
crimination (Buolamwini and Gebru 2018). Such work is, however, an exception.

In general, ethical guidelines postulate very broad, overarching princi-
ples which are then supposed to be implemented in a widely diversified set of 

1 3



 112 T. Hagendorff 

scientific, technical and economic practices, and in sometimes geographically 
dispersed groups of researchers and developers with different priorities, tasks and 
fragmental responsibilities. Ethics thus operates at a maximum distance from the 
practices it actually seeks to govern. Of course, this does not remain unnoticed 
among technology developers. In consequence, the generality and superficiality 
of ethical guidelines in many cases not only prevents actors from bringing their 
own practice into line with them, but rather encourages the devolution of ethical 
responsibility to others.

4.2  Virtue Ethics

Regardless of the fact that normative guidelines should be accompanied by in-depth 
technical instructions—as far as they can reasonably be identified—, the question 
still arises how the precarious situation regarding the application and fulfillment of 
AI ethics guidelines can be improved. To address this question, one needs to take a 
step back and look at ethical theories in general. In ethics, several major strands of 
theories were created and shaped by various philosophical traditions. Those theories 
range from deontological to contractualistic, utilitarian, or virtue ethical approaches 
(Kant 1827; Rawls 1975; Bentham 1838; Hursthouse 2001). In the following, two of 
these approaches—deontology and virtue ethics—will be selected to illustrate dif-
ferent approaches in AI ethics. The deontological approach is based on strict rules, 
duties or imperatives. The virtue ethics approach, on the other hand, is based on 
character dispositions, moral intuitions or virtues—especially “technomoral virtues” 
(Vallor 2016). In the light of these two approaches, the traditional type of AI ethics 
can be assigned to the deontological concept (Mittelstadt 2019). Ethics guidelines 
postulate a fixed set of universal principles and maxims which technology devel-
opers should adhere to (Ananny 2016). The virtue ethics approach, on the other 
hand, focuses more on “deeper-lying” structures and situation-specific deliberations, 
on addressing personality traits and behavioral dispositions on the part of technol-
ogy developers (Leonelli 2016). Virtue ethics does not define codes of conduct but 
focusses on the individual level. The technologists or software engineers and their 
social context are the primary addressees of such an ethics (Ananny 2016), not tech-
nology itself.

I argue that the prevalent approach of deontological AI ethics should be aug-
mented with an approach oriented towards virtue ethics aiming at values and char-
acter dispositions. Ethics is then no longer understood as a deontologically inspired 
tick-box exercise, but as a project of advancing personalities, changing attitudes, 
strengthen responsibilities and gaining courage to refrain from certain actions, 
which are deemed unethical. When following the path of virtue ethics, ethics as a 
scientific discipline must refrain from wanting to limit, control, or steer (Luke 1995). 
Very often, ethics or ethical guidelines are perceived as something whose purpose is 
to stop or prohibit activity, to hamper valuable research and economic endeavors 
(Boddington 2017, 8). I want to resign this negative notion of ethics. It should not be 
the objective of ethics to stifle activity, but to do the exact opposite, i.e. broadening 

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 113

the scope of action, uncovering blind spots, promoting autonomy and freedom, and 
fostering self-responsibility.

In view of AI ethics, approaches that focus on virtues aim at cultivating a moral 
character, expressing technomoral virtues such as honesty, justice, courage, empa-
thy, care, civility, or magnanimity, to name just a few (Vallor 2016). Those virtues 
are supposed to raise the likelihood of ethical decision-making practices in organi-
zations that develop and deploy AI applications. Cultivating a moral character, in 
terms of virtue ethics, means to educate virtues in families, schools, communities, 
as well as companies. At best, every individual, every member of a society should 
encourage this cultivation, by generating the motivation to adopt and habituate prac-
tices that influence technology development and use in a positive manner. Especially 
the subject of responsibility diffusion can only be circumvented when virtue ethics is 
adopted on a broad and collective level in communities of tech professionals. Simply 
every person involved in data science, data engineering and data economies related 
to applications of AI has to take at least some responsibility for the implications 
of their actions (Leonelli 2016). This is why researchers such as Floridi argue that 
every actor who is causally relevant for bringing about the collective consequence or 
impacts in question, has to be held accountable (Floridi 2016). Interestingly, Floridi 
uses the backpropagation method known from Deep Learning to describe the way 
in which responsibilities can be assigned, except that here backpropagation is used 
in networks of distributed responsibility. When working in groups, actions that are 
on first glance allegedly morally neutral can nevertheless have consequences or 
impacts—intended or non-intended—that are morally wrong. This means that prac-
titioners from AI communities always need to discern the overarching, short- and 
long-term consequences of the technical artefacts they are building or maintaining, 
as well as to explore alternative ways of developing software or using data, includ-
ing the option of completely refraining from carrying out particular tasks, which are 
considered unethical.

In addition to the endorsement of virtue ethics in tech communities, several insti-
tutional changes should take place. They include the adoption of legal framework 
conditions, the establishment of mechanisms for an independent auditing of tech-
nologies, the establishment of institutions for complaints, which also compensate for 
harms caused by AI systems, and the expansion of university curricula in particu-
lar through content from ethics of technology, media, and information (Floridi et al. 
2018; Cowls and Floridi 2018; Eaton et al. 2017; Goldsmith and Burton 2017). So 
far, however, hardly any of these demands have been met.

5 C onclusion

Currently, AI ethics is failing in many cases. Ethics lacks a reinforcement mech-
anism. Deviations from the various codes of ethics have no consequences. And 
in cases where ethics is integrated into institutions, it mainly serves as a market-
ing strategy. Furthermore, empirical experiments show that reading ethics guide-
lines has no significant influence on the decision-making of software developers. 
In practice, AI ethics is often considered as extraneous, as surplus or some kind 

1 3



 114 T. Hagendorff 

of “add-on” to technical concerns, as unbinding framework that is imposed from 
institutions “outside” of the technical community. Distributed responsibility in con-
junction with a lack of knowledge about long-term or broader societal technologi-
cal consequences causes software developers to lack a feeling of accountability or 
a view of the moral significance of their work. Especially economic incentives are 
easily overriding commitment to ethical principles and values. This implies that the 
purposes for which AI systems are developed and applied are not in accordance with 
societal values or fundamental rights such as beneficence, non-maleficence, justice, 
and explicability (Taddeo and Floridi 2018; Pekka et al. 2018).

Nevertheless, in several areas ethically motivated efforts are undertaken to 
improve AI systems. This is particularly the case in fields where technical “fixes” 
can be found for specific problems, such as accountability, privacy protection, anti-
discrimination, safety, or explainability. However, there is also a wide range of ethi-
cal aspects that are significantly related to the research, development and applica-
tion of AI systems, but are not or very seldomly mentioned in the guidelines. Those 
omissions range from aspects like the danger of a malevolent artificial general intel-
ligence, machine consciousness, the reduction of social cohesion by AI ranking 
and filtering systems on social networking sites, the political abuse of AI systems, 
a lack of diversity in the AI community, links to robot ethics, the dealing with trol-
ley problems, the weighting between algorithmic or human decision routines, “hid-
den” social and ecological costs of AI, to the problem of public–private-partnerships 
and industry-funded research. Again, as mentioned earlier, the list of omissions is 
not exhaustive and not all omissions can be justified equally. Some omissions, like 
deliberations on artificial general intelligence, can be justified by pointing at their 
purely speculative nature, while other omissions are less valid and should be a rea-
son to update or improve existing and upcoming guidelines.

Checkbox guidelines must not be the only “instruments” of AI ethics. A transition 
is required from a more deontologically oriented, action-restricting ethic based on 
universal abidance of principles and rules, to a situation-sensitive ethical approach 
based on virtues and personality dispositions, knowledge expansions, responsible 
autonomy and freedom of action. Such an AI ethics does not seek to subsume as 
many cases as possible under individual principles in an overgeneralizing way, but 
behaves sensitively towards individual situations and specific technical assemblages. 
Further, AI ethics should not try to discipline moral actors to adhere to normative 
principles, but emancipate them from potential inabilities to act self-responsibly on 
the basis of comprehensive knowledge, as well as empathy in situations where mor-
ally relevant decisions have to be made.

These considerations have two consequences for AI ethics. On the one hand, a 
stronger focus on technological details of the various methods and technologies in 
the field of AI and machine learning is required. This should ultimately serve to 
close the gap between ethics and technical discourses. It is necessary to build tangi-
ble bridges between abstract values and technical implementations, as long as these 
bridges can be reasonably constructed. On the other hand, however, the consequence 
of the presented considerations is that AI ethics, conversely, turns away from the 
description of purely technological phenomena in order to focus more strongly on 
genuinely social and personality-related aspects. AI ethics then deals less with AI 

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 115

as such, than with ways of deviation or distancing oneself from problematic routines 
of action, with uncovering blind spots in knowledge, and of gaining individual self-
responsibility. Future AI ethics faces the challenge of achieving this balancing act 
between the two approaches.

Acknowledgements Open Access funding provided by Projekt DEAL.

Funding This research was supported by the Cluster of Excellence “Machine Learning – New Perspec-
tives for Science” funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) 
under Germany’s Excellence Strategy – Reference Number EXC 2064/1 – Project ID 390727645.

Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, 
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as 
you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-
mons licence, and indicate if changes were made. The images or other third party material in this article 
are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the 
material. If material is not included in the article’s Creative Commons licence and your intended use is 
not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission 
directly from the copyright holder. To view a copy of this licence, visit http://creati veco mmons. org/licen 
ses/by/4.0/.

References

Abacus. (2018). China internet report 2018. Retrieved July 13, 2018. https: //www.abacus news. com/china 
-intern et-repor t/china- intern et-2018.pdf.

Abrassart, C., Bengio, Y., Chicoisne, G., de Marcellis-Warin, N., Dilhac, M.-A., Gambs, S., Gautrais, V., 
et al. (2018). Montréal declaration for responsible development of artificial intelligence (pp. 1–21).

Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., Mané, D. (2017). Concrete problems in 
AI safety. arXiv (pp. 1–29).

Ananny, M. (2016). Toward an ethics of algorithms: Convening, observation, probability, and timeliness. 
Science, Technology, & Human Values, 41(1), 93–117.

Anderson, M., & Anderson, S. L. (Eds.). (2011). Machine ethics. Cambridge: Cambridge University 
Press.

Anderson, M., Anderson, S. L. (2015). Towards ensuring ethical behavior from autonomous systems: 
A case-supported principle-based paradigm. In Artificial intelligence and ethics: Papers from the 
2015 AAAI Workshop (pp. 1–10).

Anderson, D., Bonaguro, J., McKinney, M., Nicklin, A., Wiseman, J. (2018). Ethics & algorithms toolkit. 
Retrieved February 01, 2019. https: //ethic stool kit.ai/.

Anderson, K., Waxman, M. C. (2013). Law and ethics for autonomous weapon systems: Why a ban won’t 
work and how the laws of WAR can. SSRN Journal, 1–32.

Asimov, I. (2004). I, Robot. New York: Random House LLC.
Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., et  al. (2018). The moral machine 

experiment. Nature, 563(7729), 59–64. https ://doi.org/10.1038/s41586 -018-0637-6.
Bakewell, J. D., Clement-Jones, T. F., Giddens, A., Grender, R. M., Hollick, C. R., Holmes, C., Levene, 

P. K. et al. (2018). AI in the UK: Ready, willing and able?. Select committee on artificial intel-
ligence (pp. 1–183).

Baron, B., Musolesi, M. (2017). Interpretable machine learning for privacy-preserving pervasive systems. 
arXiv (pp. 1–10).

Beck, U. (1988). Gegengifte: Die organisierte Unverantwortlichkeit. Frankfurt am Main: Suhrkamp.
Beijing Academy of Artificial Intelligence. (2019). Beijing AI principles. Retrieved June 18, 2019. https ://

www.baai.ac.cn/blog/beiji ng-ai-princi ples .
Bendel, O. (2017). The synthetization of human voices. AI & SOCIETY - Journal of Knowledge, Culture 

and Communication, 82, 737.

1 3



 116 T. Hagendorff 

Bentham, J. (1838). The Works of Jeremy Bentham. With the assistance of J. Bowring. 11 vols. 1. Edin-
burgh: William Tait. Published under the Superintendence of his Executor.

Boddington, P. (2017). Towards a code of ethics for artificial intelligence. Cham: Springer.
Bostrom, N. (2014). Superintelligence: Paths, dangers, strategies. Oxford: Oxford University Press.
Bourdieu, P. (1984). Distinction: A social critique of the judgement of taste. Cambridge: Harvard Univer-

sity Press.
Brady, W. J., Wills, J. A., Jost, J. T., Tucker, J. A., & van Bavel, J. J. (2017). Emotion shapes the diffusion 

of moralized content in social networks. Proc Natl Acad Sci USA, 114(28), 7313–7318.
Brahnam, S. (2006). Gendered bots and bot abuse. In Antonella de Angeli, Sheryl Brahnam, Peter Wallis, 

& Peter Dix (Eds.), Misuse and abuse of interactive technologies (pp. 1–4). Montreal: ACM.
Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A. et  al. (2018). The 

malicious use of artificial intelligence: Forecasting, prevention, and mitigation. arXiv (pp. 1–101).
Buolamwini, J., Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gen-

der classification. In Sorelle and Wilson 2018 (pp. 1–15).
Burton, E., Goldsmith, J., Koening, S., Kuipers, B., Mattei, N., & Walsh, T. (2017). Ethical considera-

tions in artificial intelligence courses. Artificial Intelligence Magazine, 38(2), 22–36.
Calo, R. (2017). Artificial intelligence policy: a primer and roadmap. SSRN Journal, 1–28.
Campolo, A., Sanfilippo, M., Whittaker, M., Crawford, K. (2017). AI now 2017 report. Retrieved October 

02, 2018. https: //asset s.ctfas sets.net/8wprhh vnpf c0/1A9c3 ZTCZa2 KEYM 64Wsc 2a/86365 57c5f 
b14f2b 74b2 be64c3 ce0c7 8/_AI_Now_Instit ute_2017_Report _.pdf.

Casilli, A. A. (2017). Digital labor studies go global: Toward a digital decolonial turn. International 
Journal of Communication, 11, 1934–3954.

Cave, S., ÓhÉigeartaigh, S. S. (2018). An AI race for strategic advantage: Rhetoric and risks (pp. 1–5).
Cowls, J., Floridi, L., (2018). Prolegomena to a white paper on an ethical framework for a good AI soci-

ety. SSRN Journal, 1–14.
Crawford, K., Dobbe, R., Dryer, T., Fried, G., Green, B., Kaziunas, E., Kak, A. et al. (2019). AI now 2019 

report. Retrieved December 18, 2019. https ://ainow instit ute.org/AI_Now_2019_Repor t.pdf.
Crawford, K., Joler, V. (2018). Anatomy of an AI system. Retrieved February 06, 2019. https ://anatom yof.

ai/.
Crawford, K., Whittaker, M., Clare Elish, M., Barocas, S., Plasek, A., Ferryman, K. (2016). The AI 

now report: The social and economic implications of artificial intelligence technologies in the 
near-term.

Cutler, A., Pribić, M., Humphrey, L. (2018). Everyday ethics for artificial intelligence: A practical guide 
for designers & developers. Retrieved February 04, 2019. https: //www.ibm.com/watson /assets /duo/
pdf/every dayet hics.pdf: 1–18.

Darling, K. (2016). Extending legal protection to social robots: The effect of anthropomorphism, empa-
thy, and violent behavior towards robotic objects. In R. Calo, A. M. Froomkin, & I. Kerr (Eds.), 
Robot law (pp. 213–234). Cheltenham: Edward Elgar.

de Bruin, B., & Floridi, L. (2017). The ethics of cloud computing. Science and Engineering Ethics, 23(1), 
21–39.

DeepMind. DeepMind ethics & society principles. Retrieved July 17, 2019. https ://deepmi nd.com/appli 
ed/deepmi nd-ethic s-societ y/princ iples /.

Derrida, J. (1997). Of grammatology. Baltimore: Johns Hopkins Univ. Press.
Diakopoulos, N., Friedler, S. A., Arenas, M., Barocas, S., Hay, M., Howe, B., Jagadish, H. V. et al. Prin-

ciples for accountable algorithms and a social impact statement for algorithms. Retrieved July 31, 
2019. https ://www.fatml .org/resour ces/princ iples -for-accoun table -algori thms .

Duchi, J. C., Jordan, M. I., Wainwright, M. J. (2013). Privacy aware learning. arXiv (pp. 1–60).
Eaton, E., Koenig, S., Schulz, C., Maurelli, F., Lee, J., Eckroth, J., Crowley, M. et al. (2017). Blue sky 

ideas in artificial intelligence education from the EAAI 2017 new and future AI educator program. 
arXiv (pp. 1–5).

Eckersley, P. (2018). Impossibility and uncertainty theorems in AI value alignment or why your AGI 
should not have a utility function. arXiv (pp. 1–13).

Ekstrand, M. D., Joshaghani, R., Mehrpouyan, H. (2018). Privacy for all: Ensuring fair and equitable pri-
vacy protections. In Sorelle and Wilson 2018 (pp. 1–13).

Engelmann, S., Chen, M., Fischer, F., Kao, C., Grossklags, J. (2019). Clear sanctions, vague rewards: 
How China’s social credit system currently defines “Good” and “Bad” behavior. In Proceedings of 
the conference on fairness, accountability, and transparency—FAT* ‘19 (pp. 69–78).

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 117

Ernest, N., & Carroll, D. (2016). Genetic fuzzy based artificial intelligence for unmanned combat aerial 
vehicle control in simulated air combat missions. Journal of Defense Management. https: //doi.
org/10.4172/2167-0374.10001 44.

Etzioni, A., & Etzioni, O. (2017). Incorporating ethics into artificial intelligence. The Journal of Ethics, 
21(4), 403–418.

Eubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. 
New York: St. Marting’s Press.

Fang, L. (2019). Google hired gig economy workers to improve artificial intelligence in controversial 
drone-targeting project. Retrieved February 13, 2019. https ://thein terce pt.com/2019/02/04/googl 
e-ai-proje ct-maven -figure -eight/ .

Fjeld, J., Hilligoss, H., Achten, N., Daniel, M. L., Feldman, J., Kagay, S. (2019). Principled artificial 
intelligence: A map of ethical and rights-based approaches. Retrieved July 17, 2019. https: //ai-hr.
cyber .harvar d.edu/primp -viz.html.

Flaxman, S., Goel, S., & Rao, J. M. (2016). Filter bubbles, echo chambers, and online news consumption. 
PUBOPQ, 80(S1), 298–320.

Floridi, L. (2016). Faultless responsibility: On the nature and allocation of moral responsibility for dis-
tributed moral actions. Philosophical Transactions. Series A, Mathematical, Physical, and Engi-
neering Sciences, 374(2083), 1–13.

Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., et al. (2018). AI4People—
An ethical framework for a good AI society: Opportunities, risks, principles, and recommenda-
tions. Minds and Machines, 28(4), 689–707.

Fogg, B. J. (2003). Persuasive technology: Using computers to change what we think and do. San Fran-
cisco: Morgan Kaufmann Publishers.

Frey, C. B., Osborne, M. A. (2013). The future of employment: How susceptible are jobs to computerisa-
tion: Oxford Martin Programme on Technology and Employment (pp. 1–78).

Fryer-Biggs, Z. (2018). The pentagon plans to spend $2 billion to put more artificial intelligence into its 
weaponry. Retrieved January 25, 2019. https: //www.theve rge.com/2018/9/8/178331 60/pentag on-
darpa -artifi cial- intel ligenc e-ai-invest ment .

Future of Life Institute. (2017). Asilomar AI principles. Retrieved October 23, 2018. https: //future ofli 
fe.org/ai-princ iples/ .

Garzcarek, U., Steuer, D. (2019). Approaching ethical guidelines for data scientists. arXiv (pp. 1–18).
Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daumeé, III, H., Crawford, K. 

(2018). Datasheets for datasets. arXiv (pp. 1–17).
Gilligan, C. (1982). In a different voice: Psychological theory and women’s development. Cambridge: 

Harvard University Press.
Goldsmith, J., Burton, E. (2017). Why teaching ethics to AI practitioners is important. ACM SIGCAS 

Computers and Society (pp. 110–114).
Google. (2018). Artificial intelligence at Google: Our principles. Retrieved January 24, 2019. https ://

ai.google /princ iples /.
Google. (2019). Perspectives on issues in AI governance (pp. 1–34). Retrieved February 11, 2019. https ://

ai.google /stati c/docume nts/perspe ctiv es-on-issues -in-ai-gover nance .pdf.
Gotterbarn, D., Brinkman, B., Flick, C., Kirkpatrick, M. S., Miller, K., Vazansky, K., Wolf, M. J. (2018). 

ACM code of ethics and professional conduct: Affirming our obligation to use our skills to ben-
efit society (pp. 1–28). Retrieved February 01, 2019. https ://www.acm.org/binari es/conte nt/asset s/
about/ acm-code-of-ethic s-bookl et.pdf.

Graham, M., Hjorth, I., & Lehdonvirta, V. (2017). Digital labour and development: Impacts of global 
digital labour platforms and the gig economy on worker livelihoods. Transfer: European Review of 
Labour and Research, 23(2), 135–162.

Greene, D., Hoffman, A. L., Stark, L. (2019). Better, nicer, clearer, fairer: A critical assessment of the 
movement for ethical artificial intelligence and machine learning. In Hawaii international confer-
ence on system sciences (pp. 1–10).

Hagendorff, T. (2016). Wirksamkeitssteigerungen Gesellschaftskritischer Diskurse. Soziale Probleme. 
Zeitschrift für soziale Probleme und soziale Kontrolle, 27(1), 1–16.

Hagendorff, T. (2019). Forbidden knowledge in machine learning: Reflections on the limits of research 
and publication. arXiv (pp. 1–24).

Hao, K. (2019). Three charts show how China’s AI Industry is propped up by three companies. Retrieved 
January 25, 2019. https: //www.techno logyr eview .com/s/612813 /the-futur e-of-chinas -ai-indus 

1 3



 118 T. Hagendorff 

try-is-in-the-hands -of-just-three -compa nies/?utm_campa ign=Artifi cial %2BInt ellig ence%2BWee 
kly&utm_mediu m=email &utm_source =Artifi cial_ Intell igen ce_Weekly _95.

Helbing, D. (Ed.). (2019). Towards digital enlightment: Essays on the darf and light sides of the digital 
revolution. Cham: Springer.

Held, V. (2013). Non-contractual society: A feminist view. Canadian Journal of Philosophy, 17(Supple-
mentary Volume 13), 111–137.

Holdren, J. P., Bruce, A., Felten, E., Lyons, T., & Garris, M. (2016). Preparing for the future of artificial 
intelligence (pp. 1–58). Washington, D.C: Springer.

Howard, P. N., Kollanyi, B. (2016). Bots, #StrongerIn, and #Brexit: Computational propaganda during 
the UK-EU Referendum. arXiv (pp. 1–6).

Hursthouse, R. (2001). On virtue ethics. Oxford: Oxford University Press.
Information Technology Industry Council. (2017). ITI AI policy principles. Retrieved January 29, 2019. 

https ://www.itic.org/public -policy /ITIAI Polic yPrin ciples FINAL .pdf.
Introna, L. D., & Wood, D. (2004). Picturing algorithmic surveillance: The politics of facial recognition 

systems. Surveillance & Society, 2(2/3), 177–198.
Irani, L. (2015). The cultural work of microwork. New Media & Society, 17(5), 720–739.
Irani, L. (2016). The hidden faces of automation. XRDS, 23(2), 34–37.
Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine 

Intelligence, 1(9), 389–399.
Johnson, D. G. (2017). Can engineering ethics be taught? The Bridge, 47(1), 59–64.
Kant, I. (1827). Kritik Der Praktischen Vernunft. Leipzig: Hartknoch.
King, T. C., Aggarwal, N., Taddeo, M., & Floridi, L. (2019). Artificial intelligence crime: An interdisci-

plinary analysis of foreseeable threats and solutions. Science and Engineering Ethics, 26, 89–120.
Kish-Gephart, J. J., Harrison, D. A., & Treviño, L. K. (2010). Bad apples, bad cases, and bad barrels: 

Meta-analytic evidence about sources of unethical decisions at work. The Journal of Applied Psy-
chology, 95(1), 1–31.

Kitchin, R. (2017). Thinking critically about and researching algorithms. Information, Communication & 
Society, 20(1), 14–29.

Kitchin, R., & Dodge, M. (2011). Code/space: Software and everyday life. Cambridge: The MIT Press.
Kosinski, M., Matz, S. C., Gosling, S. D., Popov, V., & Stillwell, D. (2015). Facebook as a research tool 

for the social sciences: Opportunities, challenges, ethical considerations, and practical guidelines. 
American Psychologist, 70(6), 543–556.

Kosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital 
records of human behavior. Proceedings of the National Academy of Sciences of the United States 
of America, 110(15), 5802–5805.

Kosinski, M., & Wang, Y. (2018). Deep neural networks are more accurate than humans at detecting sex-
ual orientation from facial images. Journal of Personality and Social Psychology, 114(2), 246–257.

Kramer, A. D. I., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emo-
tional contagion through social networks. Proceedings of the National Academy of Sciences of the 
United States of America, 111(24), 8788–8790.

Lazer, D. M. J., Baum, M. A., Benkler, Y., Berinsky, A. J., Greenhill, K. M., Menczer, F., et al. (2018). 
The science of fake news. Science, 359(6380), 1094–1096.

Lecher, C. (2018). The employee letter denouncing Microsoft’s ICE contract now has over 300 signa-
tures. Retrieved February 11, 2019. https ://www.theve rge.com/2018/6/21/17488 328/micros oft-ice-
employ ees-signa tures -prote st.

Leonelli, S. (2016). Locating ethics in data science: Responsibility and accountability in global and 
distributed knowledge production systems. Philosophical Transactions. Series A, Mathematical, 
Physical, and Engineering Sciences, 374(2083), 1–12.

Luhmann, N. (1984). Soziale Systeme: Grundriß einer allgemeinen Theorie. Frankfurt A.M: Suhrkamp.
Luhmann, N. (1988). Die Wirtschaft der Gesellschaft. Frankfurt A.M: Suhrkamp.
Luhmann, N. (1997). Die Gesellschaft der Gesellschaft. Frankfurt am Main: Suhrkamp.
Luhmann, N. (2008). Die Moral der Gesellschaft. Frankfurt AM: Suhrkamp.
Luke, B. (1995). Taming ourselves or going Feral? Toward a nonpatriarchal metaethic of animal lib-

eration. In Carol J. Adams & Josephine Donovan (Eds.), Animals & women: Feminist theoretical 
explorations (pp. 290–319). Durham: Duke University Press.

Lyon, D. (2003). Surveillance as social sorting: Computer codes and mobile bodies. In David Lyon (Ed.), 
Surveillance as social sorting: Privacy, risk, and digital discrimination (pp. 13–30). London: 
Routledge.

1 3



The Ethics of AI Ethics: An Evaluation of Guidelines 119

Lyons, S. (2018). Death and the machine. Singapore: Palgrave Pivot.
Matz, S. C., Kosinski, M., Nave, G., & Stillwell, D. (2017). Psychological targeting as an effective 

approach to digital mass persuasion. Proceedings of the National Academy of Sciences of the 
United States of America, 114, 12714–12719.

McAllister, A. (2017). Stranger than science fiction: The rise of A.I. interrogation in the dawn of autono-
mous robots and the need for an additional protocol to the U.N. convention against torture. Minne-
sota Law Review, 101, 2527–2573.

McNamara, A., Smith, J., Murphy-Hill, E. (2018). Does ACM’s code of ethics change ethical decision 
making in software development?” In G. T. Leavens, A. Garcia, C. S. Păsăreanu (Eds.) Proceed-
ings of the 2018 26th ACM joint meeting on european software engineering conference and sym-
posium on the foundations of software engineering—ESEC/FSE 2018 (pp. 1–7). New York: ACM 
Press.

Microsoft Corporation. (2019). Microsoft AI principles. Retrieved February 01, 2019. https: //www.micro 
soft.com/en-us/ai/our-approa ch-to-ai.

Mittelstadt, B. (2019). Principles alone cannot guarantee ethical AI. Nature Machine Intelligence, 1(11), 
501–507.

Mittelstadt, B., Russell, C., Wachter, S. (2019). Explaining explanations in AI. In Proceedings of the con-
ference on fairness, accountability, and transparency—FAT* ‘19 (pp. 1–10).

Morley, J., Floridi, L., Kinsey, L., Elhalal, A. (2019). From what to how. An overview of AI ethics tools, 
methods and research to translate principles into practices. arXiv (pp. 1–21).

Mullen, B., & Hu, L.-T. (1989). Perceptions of ingroup and outgroup variability: A meta-analytic integra-
tion. Basic and Applied Social Psychology, 10(3), 233–252.

Müller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opin-
ion. In Vincent C. Müller (Ed.), Fundamental issues of artificial intelligence (pp. 555–572). Cham: 
Springer International Publishing.

Omohundro, S. (2014). Autonomous technology and the greater human good. Journal of Experimental & 
Theoretical Artificial Intelligence, 26(3), 303–315.

O’Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens 
democracy. New York: Crown Publishers.

OpenAI. (2018). OpenAI Charter. Retrieved July 17, 2019. https ://opena i.com/chart er/.
Organisation for Economic Co-operation and Development. (2019). Recommendation of the Council on 

Artificial Intelligence (pp. 1–12). Retrieved June 18, 2019. https ://legali nstru ment s.oecd.org/en/
instru ments /OECD-LEGAL -0449.

Pariser, E. (2011). The filter bubble: What the internet is hiding from you. New York: The Penguin Press.
Partnership on AI. (2018). About us. Retrieved January 25, 2019. https ://www.partne rshi ponai .org/about /.
Pasquale, F. (2015). The black box society: The secret algorithms that control money and information. 

Cambridge: Harvard University Press.
Pekka, A.-P., Bauer, W., Bergmann, U., Bieliková, M., Bonefeld-Dahl, C., Bonnet, Y., Bouarfa, L. et al. 

(2018). The European Commission’s high-level expert group on artificial intelligence: Ethics 
guidelines for trustworthy ai. Working Document for stakeholders’ consultation. Brussels (pp. 
1–37).

Pistono, F., Yampolskiy, R. (2016). Unethical research: How to create a malevolent artificial intelligence. 
arXiv (pp. 1–6).

Podgaiska, I., Shklovski, I. Nordic engineers’ stand on artificial intelligence and ethics: Policy recom-
mendations and guidelines (pp. 1–40).

Prates, M., Avelar, P., Lamb, L. C. (2018). On quantifying and understanding the role of ethics in AI 
research: A historical account of flagship conferences and journals. arXiv (pp. 1–13).

Rawls, J. (1975). Eine Theorie Der Gerechtigkeit. Frankfurt am Main: Suhrkamp.
Rolnick, D., Donti, P. L., Kaack, L. H., Kochanski, K., Lacoste, A., Sankaran, K., Ross, A. S. et  al. 

(2019). Tackling climate change with machine learning. arXiv (pp. 1–97).
Rosenberg, S. (2017) Why AI is still waiting for its ethics transplant.”Retrieved January 16, 2018. https: //

www.wired .com/story/ why-ai-is-still- waiti ng-for-its-ethic s-trans plant /.
Schneier, B. (2018). Click here to kill everybody. New York: W. W. Norton & Company.
Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., Vertesi, J. (2018). Fairness and abstrac-

tion in Sociotechnical Systems. In ACT conference on fairness, accountability, and transparency 
(FAT) (vol. 1, No. 1, pp. 1–17).

Shoham, Y., Perrault, R., Brynjolfsson, E., Clark, J., Manyika, J., Niebles, J. C., Lyons, T., Etchemendy, 
J., Grosz, B., Bauer, Z. (2018). The AI index 2018 annual report. Stanford, Kalifornien (pp. 1–94).

1 3



 120 T. Hagendorff 

Silberman, M. S., Tomlinson, B., LaPlante, R., Ross, J., Irani, L., & Zaldivar, A. (2018). Responsible 
research with crowds. Communications of the ACM, 61(3), 39–41.

Singla, A., Horvitz, E., Kamar, E., White, R. W. (2014). Stochastic Privacy. arXiv (pp. 1–10).
Sitawarin, C., Bhagoji, A. N., Mosenia, A., Chiang, M., Mittal, P. (2018). DARTS: Deceiving autono-

mous cars with toxic signs. arXiv (pp. 1–27).
Smart Dubai. 2018. AI ethics principles & guidelines. Retrieved February 01, 2019. https ://smart dubai 

.ae/pdfvi ewer/web/viewe r.html?file=https ://smart dubai .ae/docs/defau lt-sourc e/ai-princ iples -resou 
rces/ai-ethic s.pdf?Status =Master &sfvrsn =d4184f 8d_6.

Statt, N. (2018). Google reportedly leaving project maven military AI program after 2019. Retrieved Feb-
ruary 11, 2019. https ://www.thever ge.com/2018/6/1/174184 06/google -maven- drone- image ry-ai-
contr act-expir e.

Taddeo, M., & Floridi, L. (2018). How AI can be a force for good. Science, 361(6404), 751–752.
Tegmark, A. (2017). Life 3.0: Being human in the age of artificial intelligence. New York: Alfred A. 

Knopf.
The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. (2016). Ethically aligned 

design: A vision for prioritizing human well-being with artificial intelligence and autonomous sys-
tems (pp. 1–138).

The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. (2019). Ethically aligned 
design: A vision for prioritizing human well-being with autonomous and intelligent systems (pp. 
1–294).

Tufekci, Z. (2018). YouTube, the great Radicalizer. Retrieved March 19, 2018. https: //www.nytim 
es.com/2018/03/10/opini on/sunda y/youtu be-polit ics-radic al.html.

Vaes, J., Bain, P. G., & Bastian, B. (2014). Embracing humanity in the face of death: why do existential 
concerns moderate ingroup humanization? The Journal of Social Psychology, 154(6), 537–545.

Vakkuri, V., Abrahamsson, P. (2018). The key concepts of ethics of artificial intelligence. In Proceedings 
of the 2018 IEEE international conference on engineering, technology and innovation (pp. 1–6).

Vallor, S. (2016). Technology and the virtues: A philosophical guide to a future worth wanting. New 
York: Oxford University Press.

Veale, M., & Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination with-
out collecting sensitive data. Big Data & Society, 4(2), 1–17.

Veglis, A. (2014). Moderation techniques for social media content. In D. Hutchison, T. Kanade, J. Kittler, 
J. M. Kleinberg, A. Kobsa, F. Mattern, J. C. Mitchell, et al. (Eds.), Social computing and social 
media (pp. 137–148). Cham: Springer International Publishing.

Vosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. Science, 359(6380), 
1146–1151.

Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V., West, S. M., Richardson, R., 
Schultz, J., Schwartz, O. (2018). AI now report 2018 (pp. 1–62).

Wiggers, K. (2019). CB insights: Here are the top 100 AI companies in the world. Retrieved February 
11, 2019. https ://ventu rebea t.com/2019/02/06/cb-insigh ts-here-are-the-top-100-ai-compan ies-in-
the-world/ .

Yu, H., Shen, Z., Miao, C., Leung, C., Lesser, V. R., Yang, Q. (2018). Building ethics into artificial intel-
ligence. arXiv (pp. 1–8).

Yuan, L. (2018). How cheap labor drives China’s A.I. ambitions. Retrieved November 30, 2018. https: //
www.nytime s.com/2018/11/25/busin ess/china -artifi cial- intell igenc e-label ing.html.

Zeng, Y., Lu, E., Huangfu, C. (2018). Linking artificial intelligence principles. arXiv (pp. 1–4).

Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published 
maps and institutional affiliations.

1 3﻿Towards an Ethics of AI Assistants: An Initial 
Framework 

 
By John Danaher 

 
Penultimate draft of a paper in Philosophy and Technology 
DOI: 10.1007/s13347-018-0317-3 

 
 

Abstract 
 

Personal AI assistants are now nearly ubiquitous. Every leading smartphone operating system 
comes with a personal AI assistant that promises to help you with basic cognitive tasks: 
searching, planning, messaging, scheduling, and so on. Usage of such devices is effectively a 
form of algorithmic outsourcing: getting a ‘smart’ algorithm to do something on your behalf. 
Many have expressed concerns about this algorithmic outsourcing. They claim that it is 
dehumanising, leads to cognitive degeneration, and robs us of our freedom and autonomy. 
Some people have a more subtle view, arguing that it is problematic in those cases where its use 
may degrade important interpersonal virtues. In this article, I assess these objections to the use 
of AI assistants. I will argue that the ethics of their use is complex in the sense that there are no 
quick fixes or knockdown objections to the practice, but there are some legitimate concerns. By 
carefully analysing and evaluating some of the most typical objections that have been lodged to 
date, we can begin to articulate an ethics of personal AI use that navigates those concerns. In 
the process, we can locate some paradoxes in our thinking about outsourcing and technological 
dependence, and we can think more clearly about what it means to live a good life in the age of 
smart machines. 

 
 
 
 
Keywords: Artificial Intelligence; Degeneration; Cognitive Outsourcing; Embodied 
Cognition; Autonomy; Interpersonal Communications 
 
 
 
 
 

1 



 1. Introduction 

 Personal AI assistants are now almost ubiquitous. Every smartphone operating 
system comes with a personal AI assistant that promises to help you with basic 
cognitive tasks: searching, planning, messaging, scheduling, and so on. Google’s 
Assistant, Apple’s Siri and Microsoft’s Cortana are the obvious examples. But they are 
just the tip of a large and ever-growing iceberg. Specialised apps offer similar assistance 
for specific tasks. Although these AI assistants are currently in their infancy, we can 
expect the underlying technology to develop and for the usage of AI assistants to 
expand. 

 
This raises some interesting ethical and normative questions. As Selinger and 

Frischmann (2016) have recently noted, usage of AI assistance is effectively a new form 
of outsourcing. Humans have long outsourced the performance of cognitive tasks to 
others. I don’t do my tax returns; my accountant does. I don’t book my travel 
arrangements; my assistant does. Such humanistic outsourcing has its own ethical 
issues. If I get someone to do something on my behalf I need to ensure that they do so 
voluntarily, that they are fairly compensated, and that they are not exploited. On top of 
this, as Michael Sandel (2012) has argued, there are some tasks that seem to ethically 
demand my personal involvement. For instance, outsourcing the writing of a best man’s 
speech seems like a mark of disrespect and apathy, not a praiseworthy efficiency-
maximising way to fulfil one’s duties. 

 
If humanistic outsourcing demands its own ethical framework, then presumably AI 

outsourcing does too. But what might that ethical framework look like? There is no 
shortage of opinions on this matter. Some people think the answer is simple. They think 
that AI outsourcing is generally problematic. It is dehumanising (Dreyfus and Kelly 
2011; Frischmann 2014), leads to cognitive degeneration (Carr 2014) and robs us of our 
freedom and autonomy (Krakauer 2016; Crawford 2015). If it is to be done at all, it 
should be done sparingly and wisely. Some people have a more subtle view (Selinger 
2014a, 2014b, 2014c), arguing that it is problematic in those cases where its use may 
degrade important interpersonal virtues.  

 
In this article, I want to assess these objections to the use of AI assistants. Contrary 

to the view stated in the previous paragraph, I will argue that the ethics of AI 
outsourcing is complex in the sense that there are no quick fixes or knockdown 

2 



objections to the practice, although there are some legitimate concerns. By carefully 
analysing and evaluating those concerns we can begin to articulate an ethical framework 
for deciding when it is appropriate to make use of an AI assistant. In the process, we can 
locate some paradoxes in our thinking about outsourcing and technological dependence, 
and we can think more clearly about what it means to live a good life in the age of smart 
machines. 

 
I proceed in six parts. In section 2, I clarify the target for the remainder of the article 

and present a general theoretical model for thinking about the ethics of AI usage. I then 
proceed to examine various objections to the practice of AI outsourcing in descending 
order of generality. I start with the most common and general objection — the 
degeneration objection — in section 3. I then look at some freedom/autonomy related 
objections in section 4. In section 5 I turn to a more discrete objection, though still one 
with a degree of generality, focusing on personal virtues and interpersonal relationships. 
In section 6, I discuss the paradox of internal automaticity. And in section 7, I conclude 
by distilling the lessons and paradoxes of the preceding analysis. 
 

 2. What are Artificially Intelligent Personal Assistants? 

 Let me start by clarifying the phenomenon of interest. This is not an easy task. 
Although the term ‘artificial intelligence’ is widely used, its precise definition is 
contested. For example, Russell and Norvig, in their leading textbook on AI, discuss 
eight different definitions, divided into four main categories: thinking like a human, 
acting like a human, thinking rationally, and acting rationally (Russell & Norvig 2010, 2 
ff). Classically, following the work of Alan Turing, human-likeness was the operative 
standard in definitions of AI. A system could only be held to be intelligent if it could 
think or act like a human with respect to one or more tasks. Over time, rationality, 
conceived in instrumental/goal-directed forms has become the more preferred standard. 
Indeed, Russell and Norvig themselves opt for this definition, describing as an ‘AI’ any 
system that “acts so as to achieve the best outcome or, when there is uncertainty, the 
best expected outcome” (Russell & Norvig 2010, 3). A similar definition was favoured 
by John McCarthy – often described as the father of AI -- who described intelligence as 

3 



the ‘computational part of the ability to achieve goals in the world’ and AI as the 
science and engineering of creating intelligent systems.1  

  In this paper, I follow this ‘rationality’ standard in will define as personal AI 
assistant any computer-coded software system/program that can act in a goal-directed 
manner. In other words, it is program that can be set some target output (“find me the 
best restaurant in my local area”) and can select among a range of options that optimizes 
(according to specific metric) for that output. This definitional approach is generous, 
and includes broad and narrow forms of AI within its scope (i.e. AI capable of general 
problem-solving across multiple domains or AI capable of solving one or two problems 
in discrete domains). For the purposes of this article, I am concerned with the ethics of 
using such AI in our personal lives, not with the ethics of such AI in public (e.g. 
military or government) or commercial contexts. That is to say, I am interested in 
exploring the ethical dimension to the use of AI in personal decision-making, goal 
setting and goal achievement — the cases where you use an AI assistant to pick a movie 
to watch or a meal to cook, to plan your travel arrangements, to book appointments, to 
pay for services, to file taxes, to send messages, to perform household chores, and, more 
generally, to solve your personal problems. 

 
This narrow focus is not intended to ignore or downplay the public and commercial 

uses of AI. Most of the technologies I talk about are commercial in nature, and 
individuals might use them in furtherance of commercial or public aims (if those aims 
line up with their own personal goals). But I am ignoring the ethical questions that 
might arise from those public or commercial uses of the technology. I argue that this 
limitation of focus is both legitimate and appropriate. Distinctive ethical issues arise 
from the public and commercial use of AI (e.g. issues around democratic participation, 
privacy and public accountability, bias, procedural fairness and so on), and some of 
those issues have been analysed at considerable length already (e.g. Danaher 2016; 
Mittelstadt et al 2016). The ethics of AI assistance in the personal domain is relatively 
neglected by comparison. 

 

                                                            
1 The quotes come from John McCarthy ‘What is Artificial Intelligence? Basic Question’ 
available at http://www-formal. stanford.edu/jmc/whatisai/node1.html – note that this quote and 
the quote from Russell and Norvig was originally sourced through Scherer 2016 
 

4 



AI assistance in the personal domain can be viewed as a form of algorithmic 
cognitive outsourcing, i.e. the offloading of a cognitive task to a smart algorithm. This 
makes AI assistance a species of automation. Automation is the general phenomenon 
whereby once human (or animal) -performed tasks are performed by machines. Much 
historic automation has involved machines taking over the physical, non-cognitive 
elements of human or animal tasks. With the growth of AI we see automation creeping 
into the mental and cognitive elements of tasks. In many ways, this is what makes the 
rise of AI assistance so ethically contentious. In modern societies, it is to the mental and 
cognitive that we attach much of our self-worth and social value. This forms an 
essential underlying assumption for the ethical evaluation undertaken in the remainder 
of this paper. 

 
In order to think about the ethical significance of such cognitive outsourcing, it 

helps to draw upon the theoretical models proposed within the situated/embodied 
cognition literature (indeed, this literature is explicitly invoked in many of the 
arguments discussed below).2 The central thesis of the situated/embodied cognition 
school of thought is that cognition is not a purely brain-based phenomenon (Kirsh 2010 
& 1995; Norman 1991; Heersmink 2013 & 2015; Crawford 2015). We don’t just think 
inside our heads. Our bodies and environments shape the way we perceive and process 
cognitive tasks. Cognition is a distributed phenomenon not a localised one, i.e. the 
performance of a cognitive task is something that gets distributed across brains, bodies 
and environments. I am not going to defend this situated/embodied view of cognition in 
this article. I think it is right in its basic outline, but there are legitimate criticisms of 
particular positions taken up within the literature. I appeal to it here because I think it 
provides a useful model for understanding both the phenomenon of AI assistance and its 
discontents. 

 
One way that it helps us to do this is by identifying and explaining the long-standing 

importance of cognitive artifacts in human thinking. Cognitive artifacts can be defined 
as tools, objects or processes that assist in the performance of a cognitive task (Norman 
1991; Heersmink 2013). Artifacts of this sort are abundant: 

 

                                                            
2 Indeed, this literature is explicitly invoked by many of the critics of AI assistance e.g. Carr 2014, 
Krakauer 2016, and Crawford 2015. 

5 



“We use maps to navigate, notebooks to remember, rulers to measure, calculators to 
calculate, sketchpads to design, agendas to plan, textbooks to learn, and so on. Without 
such artifacts we would not be the same cognitive agents, as they allow us to perform 
cognitive tasks we would otherwise not be able to perform.”  
 
(Heersmink 2013, 465-466) 

 
AI assistants can be viewed as simply a new type of cognitive artifact. The crucial 

question is whether they have unique and distinctive ethical consequences. 
 
To answer that question, it helps to adopt two further ideas from the 

situated/embodied analysis of cognitive artifacts. The first is that although cognitive 
artifacts often enhance our performance of cognitive tasks — I am undoubtedly a better 
mathematician with a pen and paper than I am without— they do so in an 
underappreciated way. We can think about our interactions with cognitive artifacts at 
the system level (i.e. our brains/bodies plus the artifact) and the personal level (i.e. how 
we interact with the artifact). The distinction comes from Norman (1991). At the system 
level, the cognitive performance is often enhanced by the artifact: me-plus-pen-and-
paper is better at than me-without-pen-and-paper. But the system level enhancement is 
achieved by changing the cognitive task performed at the personal level: instead of 
imagining numbers in my head and adding and subtracting them using some mentally 
represented algorithm, I visually represent the numbers on a page, in a format that 
facilitates the easy application of an algorithm. The artifact changes one cognitive task 
into another (series) of cognitive tasks. 

 
Why is this important? The answer (and the second key idea) is that it encourages us 

to think about the effects of cognitive artifacts in an ecological mode. So that when we 
start using a new artifact to assist with the performance of a cognitive task, we shouldn’t 
think of this simply as a form of outsourcing. The artifact may share (or takeover) the 
cognitive burden, but in doing so it will also change the cognitive environment in which 
we operate. It will create new cognitive tasks for us to perform and open up new modes 
or styles of cognition. For example, by performing computations with a pen and paper, 
we will be able to do far more complex mathematical operations than we could without. 
Changing one aspect of the cognitive environment has knock-on effects on other aspects 
of the cognitive environment. 

 
6 



This ecological model helps us to understand why one of the most popular 
dismissals of technolo-pessimists (including critics of AI) is so attractive and so 
misleading. It is common, whenever someone expresses concerns about the cognitive 
consequences of a new technology like AI assistants, to bring up the Platonic dialogue 
The Phaedrus as a reductio of those concerns. In that dialogue, Socrates has a debate 
with Phaedrus about the merits of writing vis-a-vis oratory. Socrates insists that writing 
is bad for thinking. It weakens your innate cognitive capacities, makes you dependent 
on the artifact of the written word, and less able to remember and think complex 
thoughts for yourself. As he put it: 

 
“…Their trust in writing, produced by external characters which are no part of 
themselves, will discourage the use of their own memory within them. You have invented 
an elixir not of memory, but of reminding; and you offer your pupils the appearance of 
wisdom, not true wisdom, for they will read many things without instruction and will 
therefore seem to know many things, when they are for the most part ignorant and hard 
to get along with, since they are not wise, but only appear wise.”  
 
(Plato The Phaedrus 274d) 

 
The view that Socrates expresses seems quaint and silly to modern ears. It seems as 

though he did not appreciate all the advantages that writing could bring. By writing 
down our thoughts and ideas we allowed for their cultural transmission and 
preservation; we allowed for others to interpret, critique and build upon them. In short, 
we dramatically changed the cognitive ecology in which we live and breathe. These 
changes have enabled us to soar to new cognitive and civilisational heights. 

 
And yet what Socrates said seems quite similar to what modern-day technological 

naysayers have to say about AI. They too worry about the cognitive consequences of the 
latest technological aids; and they too worry that these technological aids will make us 
stupid, more dependent, and less able. So can we dismiss their concerns as easily as we 
dismiss Socrates’s? I think not. The Phaedrus is not a reductio of the modern-day AI 
naysayers. There are two reasons for this. First, doing so fails to engage with the 
particularities of their criticisms. The way in which writing changes our cognitive 
ecology is not necessarily the same as the way in which AI assistants change our 
cognitive ecology. Although they all belong to the general family of cognitive artifacts, 
the members of that family differ in their uses and their consequences. We need to 

7 



attend to those differences when developing an appropriate ethics of AI assistance. 
Second, The Phaedrus is only persuasive with the benefit of hindsight. It is only 
because we see the advantages that writing has wrought that we are able to dismiss 
Socrates’s view. We don’t have the benefit of hindsight when it comes to AI assistants. 
We can see some of the changes they may bring, but not all. This uncertainty needs to 
be factored into our ethical analysis and it makes it less easy to dismiss the concerns of 
the critics. 

 
Consequently, I think it is important to give the critics’ views a fair hearing and to 

assess the merits of those criticisms in light of the features of our cognitive ecology that 
they call out. With this goal in mind, in the remainder of this article I will assess three 
major objections to the use of personal AI assistants. Each of the objections has been 
defended by at least one sceptic of this technology. And they have been chosen for 
analysis because I believe them to be paradigmatic, though not exhaustive,3 of the kinds 
of concerns raised in both the academic literature and among the general public. 
Furthermore, each of the arguments identifies a set of ecological consequences arising 
from the use of an AI assistant, some of which are quite general, others more narrow. 
Assessing them will give good coverage of the sorts of effects that AI assistants might 
raise within our changing cognitive ecology.  
 
 
 3. The Degeneration Argument 

 With that said, I will to start by looking at the contemporary argument that is most 
similar to the one defended by Socrates in the Phaedrus and claims the most general 
ecological effect of the use of AI assistants. This is the degeneration argument. Though 
variants of this argument are commonplace, I will focus on the version presented in 
Nicholas Carr’s book The Glass Cage (2014). In this book he tries to marshal a number 
of philosophical and psychological ideas into a general critique of automation. The book 

                                                            
3 A reviewer wonders, for example, why I do not discuss the consequences of using AI 
assistants to outsource moral decision-making. There are several reasons for this. The most 
pertinent is that I have discussed moral outsourcing as a specific problem in another paper 
[reference omitted] and, as I point out in that paper, I suspect discussions of moral outsourcing 
to AI will raise similar issues to those already discussed in the expansive literature on the use of 
enhancement technologies to improve moral decision-making (for a similar analysis, coupled 
with a defence of the use of AI moral assistance, see Giublini and Savulescu 2018). That said, 
some of what I say below about degeneration, autonomy and interpersonal virtue will be also be 
relevant to debates about the use of moral AI assistance.  

8 



focuses on all forms of automation, but it is clear that AI-based automation is the major 
concern. And although some of Carr’s criticisms are concerned with the more social and 
political effects of automation, the degeneration argument is focused directly on the 
personal consequences of automation. 

 
Carr defends the degeneration argument indirectly by first considering a contrary 

point of view: that of Alfred North Whitehead. In his 1911 work An Introduction to 
Mathematics, Whitehead made a bold claim: 

 
“It is a profoundly erroneous truism, repeated by all copy-books and by eminent people 
when they are making speeches, that we should cultivate the habit of thinking of what we 
are doing. The precise opposite is the case. Civilization advances by extending the 
number of important operations which we can perform without thinking about them. 
Operations of thought are like cavalry charges in a battle — they are strictly limited in 
number, they require fresh horses, and must only be made at decisive moments.”  
 
(Whitehead 1911, 45-46) 

 
Though Whitehead was writing long before modern AI what he says has relevance 

to the debate about personal AI. He suggests that mental resources are limited and need 
to saved — like the cavalry charges in a battle — for the times that really matter. Such 
savings are exactly what AI assistants promise. AI assistants can reduce the amount of 
thinking we need to do by automating certain cognitive tasks and thereby freeing up our 
mental resources for the ‘decisive moments’.  

 
So it seems like we can co-opt Whitehead’s claims into the following defence of AI 

assistants (this argument is not intended to be formally valid): 
 

(1) Mental labour is difficult and finite: time spent thinking about trivial matters limits 
our ability to think about more important ones. 
 
(2) It is good when we have the time and ability to think the more important thoughts. 
 
(3) Therefore, it would be good if we could reduce the amount of mental labour expended 
on trivial matters and increase the amount spent on important ones. 
 

9 



(4) AI assistants help to reduce the amount of mental labour expended on trivial matters. 
 
(5) Therefore, it would be good if we could outsource more mental operations to AI 
assistants. 

 
Although he doesn’t present his analysis in these terms, Carr’s defence of the 

degeneration argument starts by highlighting the flaws in Whitehead’s inference from 
(3) and (4) to (5). The problem with this inference is that it assumes that if AI assistance 
saves mental labour, we will use the reserved mental labour to think more important 
thoughts. This ignores the potential knock-on effects of increased reliance on AI 
assistance. One of those knock-on effects, according to Carr, is that increased reliance 
on AI assistance will atrophy and degenerate our mental faculties. So, far from freeing 
up mental resources, increased reliance on AI assistance will deplete mental resources. 
We will no longer have the ability to think the important thoughts. This will in turn 
reduce the quality of our personal lives because the ability to engage in deep thinking is 
both intrinsically and instrumentally valuable: it results in a better immediate conscious 
experience and engagement with life, and it helps you to solve personal problems. 

 
So far, so Socrates. The novelty in Carr’s argument comes from the psychological 

evidence he uses to support it. In the 1970s, psychologists discovered something they 
called the generation effect (Slamecka and Graf 1978). The original experiments in 
which it was discovered had to do with memorisation and recall. The finding was that 
the more cognitive work you have to do during the memorisation phase, the better able 
you are to recall the information at a future date. Later studies revealed that this effect 
applied outside of memorisation: it helped with conceptual understanding, problem 
solving, and recall of more complex materials too. 

 
The generation effect has a corollary: the degeneration effect. If anything that forces 

us to use our own internal cognitive resources enhances our memory and understanding, 
then anything that takes away the need to exert those internal resources will reduce our 
memory and understanding. Carr cites the experimental work of Christof van Nimegen 
and his colleagues in support of this view (Van Nimwegen et al 2006; Burgos et al 
2007). They have worked on the role of assistive software in conceptual problem 
solving. In one of their studies (van Nimwegen et al 2006) they presented experimental 
subjects with a variation on the Missionaries and Cannibals game (a classic logic puzzle 

10 



about getting a group of missionaries across a river without being eaten by a cannibal). 
The game comes with a basic set of rules. You must get the missionaries across the river 
in the least number of trips while conforming to those rules. Van Nimwegen and his 
colleagues got one group of subjects to solve the puzzle with a simple software program 
that provided no assistance and a second group to solve it using a software program that 
offered on-screen prompts, including details as to which moves were permissible. 
People using the assistive software solved the puzzles more quickly than the others. But 
in the long-run, the second group emerged as the winners: they solved the puzzles more 
efficiently and with fewer wrong-moves. What’s more, in a follow up study performed 
8 months later, it was found that members of the second group were better able to recall 
how to solve the puzzle. Subsequent studies (Burgos et al 2007) showed that the effect 
was also found for other cognitive tasks.  

 
This adds a degree of evidential credibility to the Socratic-style dependency 

objection. But does it succeed in defeating Whitehead’s argument? Not necessarily. 
Remember, we need to think about the effects of AI assistance in an ecological mode. It 
is not enough to prove that relying on AI assistance for certain cognitive tasks will lead 
to the degeneration in our ability to perform those tasks. I am willing to grant that 
degeneration is a plausible consequence of reliance on AI: everything we know from the 
psychology and neuroscience of human performance suggests that the brain has some 
plasticity: if you stop performing certain tasks the relevant cognitive real estate can be 
redeveloped and used for other tasks. But degeneration of performance on certain tasks 
is not enough, in and of itself, to show that there is a problem. It could well be that tasks 
in which our performance degenerates are not that important in the first place, and 
freeing us from their performance might be a net benefit. Indeed, this is often provably 
the case at an individual level. For example, I am no good at tracking, recording, and 
budgeting my finances. I find the task mind-numbingly boring. I have tried to do it 
several times in the past and failed. Fortunately, my bank recently started offering a 
software service which automatically categorises my expenditure items (this was easy 
since I conduct nearly all transactions electronically), presented nice visual readouts of 
this information, and gave me budgeting and saving suggestions. This has had a 
dramatic and immediate effect on my behaviour. I am now more aware of what I am 
spending, and saving more money per month. It is a clear and simple objective metric of 
success. I have no doubt that this automated expenditure tracking and budgeting system 
is causing some cognitive degeneration (I no longer even try to keep a mental track of 

11 



my expenditure on a daily basis), but this degeneration is having net benefits. My 
overall cognitive ecology has changed for the better. I am better at solving the financial 
problems that life throws in my way.  

 
To claim that degeneration is a general reason to object to the use of AI assistants, 

Carkk will have to do more than prove the likelihood of degeneration for some specific 
task. He will have to show that the degeneration effect is either non-localised or that it is 
likely to affect some specific ability or set of abilities that is so intrinsically valuable 
that their degeneration would make life much worse. I consider the latter possibility in 
subsequent sections, particularly in section 5 when I look at the possible effects of AI 
assistants on interpersonal relationships. For now, I focus on the general case since it 
seems to be more in keeping with Carr’s aspirations. 

 
To succeed in making the general case, Carr will have to show that reliance on AI 

(i) degenerates cognition beyond some individual task and/or (ii) that the rest of our 
cognitive ecology will be changed so dramatically that it compounds the degeneration 
effect in the localised domain. To be fair, Carr has tried to do this. He himself (2011) 
along with several other authors (Crawford 2015; Newport 2016) have been arguing for 
some time now that ICT has dramatically changed our cognitive environment to the 
point that we are incapable of serious cognitive effort. The internet, they say, is giving 
birth to a generation of information junkies, addicted to a constantly-changing, 
constantly-updating soup of social media posts, fake news, and cat videos. People who 
live in such cognitive environments are never going to think the big thoughts that 
Whitehead revered. What’s more, there is evidence to suggest that this distraction-rich 
environment has non-localised effects on our cognition. Clifford Nass’s research, for 
example, suggests that people who constantly live and work in a distraction-rich 
environment are less able to focus on important tasks when the need arises (Nass 2013 
and Ophir, Nass and Wagner 2009). They are, in his own words, ‘chronically distracted’ 
and ‘suckers for irrelevancy’ (Nass/Flatow 2013). Incorporating this insight makes the 
degeneration argument more interesting and more challenging. The claim now is that 
the degenerating effect of AI comes on top of the pre-existing degeneration effect of our 
distraction-rich cognitive ecology. The result is that the degeneration effect spreads 
beyond the particular forms of assistance that AI provides. If we continue down the path 
to more AI assistance, we risk losing the ability to think deep thoughts by ourselves.  

 

12 



But even this does not provide a knockdown objection to the personal use of AI. All 
it does is force a nuanced and careful approach to the ethics of AI outsourcing. There 
are three reasons for this. 

 
First, the value assumption underlying this general version of the degeneration 

argument must be kept in mind. The assumption is that the capacity to think deep 
thoughts is both intrinsically and instrumentally valuable. There is credibility to this. 
Understanding something, or working out the solution to a problem, can be intrinsically 
rewarding. There is a degree of conscious satisfaction/achievement that comes from the 
personal achievement that is lost if you outsource a task to another person or thing. 
Also, solving a cognitive problem is clearly instrumentally rewarding: it enables us to 
get other good things (food, money, shelter, success, status etc.) that make life worth 
living. Indeed, according to one theory (Pinker 2010), the ability to solve cognitive 
problems is fundamental to why humans still exist to this day. But the intrinsic and 
instrumental rewards of thinking are dissociable and this has an important impact on the 
ethics of AI outsourcing. If the primary good of cognition is in the instrumental rewards 
it brings, then it is not clear that we lose anything by excessive AI outsourcing. Indeed, 
we may gain more than we lose. Take, once again, my example of the expenditure 
tracking and budgeting advice that is now automatically given out by my bank. This is 
instrumentally valuable to me. I am saving more money as a result of this cognitive 
outsourcing, and this enables me to get more of what I want in the future. Furthermore, 
since I did not enjoy the mental effort involved in solving this cognitive problem (it was 
not intrinsically rewarding to me), it seems like this particular instance of outsourcing is 
a win-win, despite the degenerating effects it may have. The same logic can apply 
across a whole suite of outsourcing options and degenerating effects. As long as I am 
embedded in a network of outsourcing devices that brings me the same (or better) 
instrumental rewards, and as long as I only rely on those outsourcing devices that solve 
problems I did not enjoy solving, there is little to lament. The only problem from an 
instrumental perspective is if I become decoupled from the network of outsourcing 
devices, or if the network of outsourcing devices breaks down. In those cases there will 
be knock-on negative effects to my well-being. Building up a degree of independent 
cognitive resilience could save me from these negative effects, but should I want to do 
this? That really depends on how likely decoupling and breakdown are, and how severe 
the effects will be. The reality is that in the modern era total disconnection is relatively 
rare and so you may waste mental effort by trying to build up cognitive resilience. 

13 



Furthermore, as Heersmink argues (2015), the existence of these networks changes the 
instrumental rewards that are available to us. It could be that I am rewarded for being an 
expert user and navigator of my AI-rich ecology. If so, voluntarily decoupling myself 
from the system could bring more costs than rewards. 

 
This brings me to the second point. One thing that is missing from Carr’s analysis 

(and passed over by Whitehead) is any discussion of the positive role that AI assistance 
could play in addressing other cognitive deficits that are induced by resource scarcity. 
The work of Sendhil Mullainathan and Eldar Shafir (2014a and 2014b) is instructive in 
this regard. It suggests that those who suffer from scarcity (income scarcity, food 
scarcity, time scarcity etc) also suffer knock-on cognitive deficits (Shah, Mullainathan 
and Shafir 2012). If a resource is scarce to you, you tend to focus all your cognitive 
energies on it. This has deleterious effects on your other cognitive abilities. For 
example, in one study people suffering from income scarcity were found to have 
associated deficits in fluid intelligence (up 15 IQ points) and executive control (impulse 
control, willpower etc). This suggests that people suffering from scarcity face increased 
cognitive burdens, which they often find difficult to discharge. One great promise of AI 
assistance is that it could help to shoulder some of that cognitive burden — creating 
what Mullainathan and Shafir call ‘slack’ — which could in turn put them in a better 
position to address their scarcity-related problems. In other words, cognitive 
outsourcing through AI could redress scarcity-induced cognitive imbalances within 
one’s larger cognitive ecology. This serves as a counterbalance to Carr’s concerns about 
degeneration.4 

 
Of course, none of this quite addresses the impact of cognitive degeneration on the 

intrinsic benefits of thinking deeply. There are certain cognitive tasks that I want to 
continue to perform (reading philosophical articles and critically reflecting on their 
contents being chief among them) and there are some legitimate concerns about the 
impact of the new cognitive ecology on my ability to do so. Here, the response to the 
degeneration argument should be one of cautious optimism. We need to be discerning in 
our approach to AI assistance. We shouldn’t embrace the latest form of AI assistance 
without first reflecting on the nature of the cognitive task with which it assists, the 
likely degenerating effects of that assistance, and the possible instrumental benefits. 
                                                            
4 I am indebted to Miles Brundage for suggesting this line of argument to me. We write about it in more 
detail in https://philosophicaldisquisitions.blogspot.com/2017/05/cognitive-scarcity-and-artificial.html  

14 



Fortunately there is a general principle we can apply: If the task itself is something that 
is intrinsically valuable, if the associated degenerating effects are likely to be 
widespread, and if the instrumental benefits are slight, it is probably best to avoid AI 
assistance. Beyond such a case, the negative consequences of degeneration can be 
overstated.  

 
Determining whether the use of an AI assistant will have a damaging widespread 

degenerating effect will need to be assessed by reference to the commonality of the 
outsourced task within the individual’s life, and the possible need for cognitive 
resiliency with respect to that task. For example, I know that budgeting and expenditure 
tracking is a relatively minimal part of my day-to-day life. It is not something for which 
I am paid or rewarded, or for which others rely on my guidance. Consequently, 
outsourcing the task is unlikely to have a damaging degenerating effect. is Determining 
whether or not an activity is sufficiently intrinsically valuable to warrant avoiding AI 
assistance, is something to be decided partly by the individual – What is most important 
to them? What do they find rewarding? – and partly by reference to general societal 
norms. I return to this latter point again in section 5 when I discuss one set of activities 
that might be high on the intrinsic value scale because of societal norms, and may raise 
particular concerns when it comes to the use of AI assistants. 
 

 4. The Autonomy Argument 

 The main flaw of the degeneration argument is its over-ambitious nature. It makes 
the assumption that degeneration in and of itself is a bad thing when that is not always 
true: you need to consider the impact of that degeneration in light of the broader 
cognitive ecology in which a person operates. A slightly more plausible, but still 
reasonably broad, variant on the argument focuses on the ill-effects of AI assistance on 
our capacity for autonomy and responsibility.  

 
Autonomy and responsibility are cherished values in modern liberal societies. Some 

of their value lies in the political and social realm: in the obligations the state (and other 
public actors) owe to us and that we owe to others. And some of their value lies in the 
personal realm. Personal goal-setting and goal-achievement depend on autonomy and 
responsibility. It is commonly believed that my happiness and self-fulfilment are best 
served when I pursue goals that are of my own choosing; and it is also commonly 

15 



believed that the achievement and meaning I derive from my goals is dependent on my 
being responsible for what I do (Luper 2014; Smuts 2013). If AI assistance threatened 
autonomy and responsibility, it could have an important knock-on effect on our 
personal happiness and fulfillment. 

 
Here is the argument someone might make: 
 

(6) Personal autonomy and responsibility are essential for meaning, happiness and self-
fulfilment. 
 
(7) Widespread use of AI assistance undermines autonomy and responsibility. 
 
(8) Therefore, widespread use of AI assistance undermines meaning, happiness and self-
fulfilment. 

 
Let us grant premise (6) for now. Attention then turns to premise (7): is there any 

reason to believe that AI assistance poses such a threat? Yes, there is. The threat comes 
from two directions. First, AI assistance threatens to sever the link between what we 
choose and desire to do and what happens in the world around us. This can undermine 
personal responsibility and hence achievement. And second, AI assistance threatens to 
manipulate, filter or otherwise structure our choices, meaning that we act for reasons or 
beliefs that are not necessarily our own. 

 
The first threat is easy to understand. Whenever AI assistance joins up with other 

forms of automation (e.g. the automation of physical labour) there is obviously an 
impact on the link between what an individual chooses or desires to do and what gets 
done. Suppose my partner comes home after a long day at work and asks me if I have 
done the vacuuming. I say ‘yes, of course, my dear’, beaming with pride at my 
industrious discharge of my household duties. It turns out that I didn’t really do the 
vacuuming. I purchased a Roomba robot that does the vacuuming for me on an 
automatic cycle. So when my partner asks whether I did the vacuuming, I’m misleading 
her when I claim the effort as my own. I am not really entitled to feel the sense of pride 
and achievement that I suggested I felt. At best, I can claim responsibility for the 
decision to purchase and use the robot, but this puts considerable distance between me 
and what happens in our house on a daily basis. This might be problematic, but we need 

16 



to bear in mind Whitehead’s point about saving one’s resources for the activities that 
really matter. Severing the link in certain cases seems perfectly acceptable.  

 
The second threat is more challenging. Although the link between AI assistance and 

other forms of automation is real, at the moment the primary role of AI assistance is not 
in replacing human agency but, rather, in replacing human cognitive effort. The AIs 
make sense of information for us: they perform basic computations on our behalf, and 
then issue recommendations and suggestions to us. We are still the ones that ultimately 
choose to act on those recommendations and suggestions, but we choose from the menu 
of options provided by the AI. Several authors have expressed concern about this 
dynamic, suggesting that it is tantamount to external manipulation or coercion, and that 
it will ultimately corrode our autonomy. 

 
Krakauer (2016) expresses the worry by distinguishing between two major types of 

cognitive artifact: the complementary and the competitive. He uses the abacus as an 
example of a complementary cognitive artifact. The abacus helps us to perform 
mathematical operations and but it does so by complementing our innate cognitive 
capacities. Studies of skilled abaci users show, according to Krakauer, that if you take 
away the abacus, their mathematical performance is not impaired. They replace the 
external artifact with an internal mental model. The artifact is effectively like a set of 
training wheels: it provides a short-term scaffold for learning. Contrast that with a 
competitive cognitive artifact like a digital calculator. This doesn’t complement our 
innate cognitive capacities, it replaces them. If you take away the calculator, the human 
agent is no better at performing the task. AI assistants are, according to Krakauer, more 
like calculators than abaci and hence are likely to have this capacity-reducing effect as 
their usage becomes more widespread. To this extent, Krakauer is an orthodox 
proponent of the degeneration thesis. His explicit concern, however, is that the 
competition between humans and AIs will corrode freedom and autonomy. He uses the 
Homeric tale of Odysseus and the Lotos Eaters to make his point: 

 
In Homer’s The Odyssey, Odysseus’s ship finds shelter from a storm on the land of the 
lotus eaters. Some crew members go ashore and eat the honey-sweet lotus, “which was 
so delicious that those [who ate it] left off caring about home, and did not even want to 
go back and say what happened to them.” Although the crewmen wept bitterly, Odysseus 
reports, “I forced them back to the ships…Then I told the rest to go on board at once, lest 
any of them should taste of the lotus and leave off wanting to get home.” In our own 

17 



times, it is the seductive taste of the algorithmic recommender system that saps our ability 
to explore options and exercise judgment. If we don’t exercise the wise counsel of 
Odysseus, our future won’t be the dystopia of Terminator but the pathetic death of the 
Lotus Eaters.  
 
(Krakauer 2016) 

 
In short, Krakauer worries that AI recommenders will make us lazy with respect to 

autonomy: their choices will replace our choices. 
 
The technology critic Evgeny Morozov (2013) expresses a very similar set of 

concerns. He writes in particular about data-mining and predictive analytics and the 
impact it will have on our ability to choose for ourselves. Like Krakauer, he uses an 
evocative metaphor to explain his fear: the metaphor of invisible-barbed wire: 

 
The invisible barbed wire of big data limits our lives to a space that might look quiet and 
enticing enough but is not of our own choosing and that we cannot rebuild or expand. 
The worst part is that we do not see it as such. Because we believe that we are free to go 
anywhere, the barbed wire remains invisible… 
 
Thanks to smartphones or Google Glass, we can now be pinged whenever we are about 
to do something stupid, unhealthy or unsound. We wouldn’t necessarily need to know why 
the action would be wrong: the system’s algorithms do the moral calculus on their own. 
Citizens take on the role of information machines that feed the techno-bureaucratic 
complex with our data. And why wouldn’t we, if we are promised slimmer waistlines, 
cleaner air, or longer (and safer) lives in return? 
 
(Morozov 2013) 

 
So here the concern is that the AI assistant will imprison us within a certain zone of 

agency. It will do all the hard cognitive work needed to come up with suggestions on 
what to do: we will be reduced to mere implementers of these suggestions, completely 
shut off from the rationale and reasons that underlie the AI’s recommendations. That 
certainly sounds like a threat to autonomy. 

 
But to establish whether or not it is a threat we need to consider what the conditions 

of autonomy actually are. This is, unsurprisingly, a contentious philosophical question. 
18 



Raz’s model of autonomy (Raz 1986) is reasonably broad, and highlights three general 
conditions that must satisfied if one is to exercise autonomous choice: rationality (i.e. 
the choice must be premised on some rationale that is comprehensible to you); 
optionality (you must be able to choose from an adequate range of valuable options); 
and independence (i.e. the choice must be free from coercion and manipulation). Other 
theories of autonomy point to consistency between lower-order and higher-order 
preferences in addition to independence from manipulation (Frankfurt 1971; Dworkin 
1988). Are any of these conditions threatened or undermined by the use of AI 
assistance? Let’s take them one by one. 

 
The rationality condition might be threatened, depending on how it is interpreted. 

One consistent complaint about AI systems, particularly when they rely on machine 
learning algorithms, is that they are black box systems. They produce certain outputs — 
recommendations and suggestions — in an opaque manner: we don’t know, or are 
unable to reverse engineer, the precise logic they use to come up with these 
recommendations (Burrell 2016; Danaher 2016). This might threaten comprehensibility 
but how serious a threat this is depends on how extensive and deep the 
comprehensibility needs to be for autonomy. If an AI assistant issues one 
recommendation and you unfailingly and unquestioningly implement it without 
understanding it, then maybe the rationality of your choice is compromised. But if it 
gives many recommendations, you still exercise some capacity for critical reflection on 
the reasons for your choices. Furthermore, even in the case where it issues one 
recommendation, you still have some control over the decision to defer to it in the first 
place. The rationale/reasons underlying that deference will be comprehensible to you. It 
is only if the deference becomes completely automatised within your own mind, that a 
serious problem emerges. But this then raises the paradox of internal automaticity, 
which I discuss in the penultimate section. 

 
What about the optionality condition? Here, AI assistance might be able to help, not 

hinder autonomy. It is widely known and widely appreciated that we have far too many 
options available to us now. Whether it is choosing books to buy, movies to watch or 
songs to stream, there is too much ‘stuff’ out there for any one human being to process 
and appreciate. We need some assistance when it comes to filtering and limiting our 
options. Schwartz’s work on the paradox of choice highlights the problem (2004). 
Having an adequate range of options increases well-being up to a certain point, but once 

19 



the range of options becomes too extensive people get ‘frozen’ in decision problems, 
unable to compute and process all the possible outcomes. This can increase feelings of 
regret and guilt when it comes to decision-making (Nagel 2010). Empirical evidence on 
the paradox of choice is somewhat mixed (Scheibehenne et al 2010) with some studies 
suggesting that individuals can avoid getting ‘stuck’ in decision problems by adopting 
heuristics and other rules for filtering information. Reliance on such heuristics does, 
however, increase the cognitive burden when it comes to individual choices. One major 
advantage of an AI assistant is that it can reduce this cognitive burden by doing this 
filtration for you. This suggests that AI assistance can help to ensure that your choices 
bring about that all-important harmony between lower and higher-order preferences. By 
filtering options according to what it learns about your preferences the assistant can 
ensure that the available options are consistent with your preferences, while at the same 
time reducing the opportunity for getting stuck in choices or feeling increased regret and 
guilt. 

 
What about independence and manipulation? This is probably the most challenging 

condition. AI recommendation systems will impact on our ability to process and sort 
through options on our own, and this may, in turn, make us more susceptible to 
manipulation. As we get comfortable with outsourcing the cognitive burden of choice to 
the AI, we may become more trustworthy and this trust can be abused. I would argue 
that it is unlikely that AIs will become overtly coercive — coercion requires some 
explicit or implied threat that if you don’t follow a recommended option you will be 
made worse off as a result (formally, coercion requires that you be threatened with 
being made worse off relative to some pre-existing baseline - see Wertheimer 1987). It 
is possible that AI systems will incorporate something akin to this dynamic — e.g. a 
health AI might threaten you with higher insurance premiums if you do not follow its 
recommendations — and that would clearly involve a breach of autonomy — but 
presumably those cases will be reasonably obvious when they arise. What is more 
interesting and potentially insidious — and what seems to really worry Krakauer and 
Morozov — is that the AI would gradually ‘nudge’ you into a set of preferences and 
beliefs about the world that are not of your own making or, as one reviewer to this paper 
put it, ‘guilt trip’ you into doing something that you would rather not do (think again of 
the health AI telling you that you are 2,000 steps short of your 10,000 a day target). The 
autonomy-undermining properties of these more subtle manipulations of choice are now 
widely debated. Nudging, in particular, has been subject to a lot of scrutiny, with 

20 



disagreement lingering as to whether it undermines one’s autonomy or not. Nudging 
occurs when a choice architect designs a decision-making environment in such as way 
as to bias or encourage people to select certain options, e.g. putting healthy foods at eye 
level in a canteen in order to get people to select them over unhealthy options (Thaler 
and Sunstein 2009). The original claim made by Thaler and Sunstein in defence of 
nudges was that they could be used to satisfy desirable policy goals and while 
protecting autonomy. They could do so because people would always still have choices 
within the relevant choice architecture, even if some choices were made less attractive 
or more difficult. But, as Sunstein acknowledges, there is clearly a dark side to nudging 
and the technique could be used for nefarious purposes (Sunstein 2016) Furthermore, as 
the regulatory theorist Karen Yeung points out, there might be something different 
about the kinds of nudging that are made possible through AI assistants: they can 
constantly and dynamically update an individual’s choice architecture to make it as 
personally appealing as possible, learning from past behaviour and preferences, and so 
make it much more likely that they will select the choice architect’s preferred option. As 
she puts, the technology enables a kind of ‘hypernudging’ or nudging on steroids 
(Yeung 2017).  

 
Is this problematic? Here, I think there are indeed reasons to be concerned about the 

impact of AI assistants, but those reasons have to be kept in perspective. In the first 
instance, it is important to acknowledge that our preferences and beliefs about the world 
are never of our own making. The projects we deem important and the options made 
available to us are always products of cultural and environmental forces that are beyond 
our control. This is, perhaps, one of the key insights of Sunstein and Thaler in their 
original defence of nudges: there is no perfectly neutral design for a choice architecture; 
every design embodies some bias or preference, even if it is not stated or appreciated. I 
would simply expand this observation and point out that the total set of choice 
architectures within any society at any given time is going to reflect a range of cultural 
and historical forces that are beyond the control of the individual and that are not neutral 
with respect to particular options.  The widespread use of AI assistants changes nothing 
about these basic facts. It is, consequently, naïve to assume that simply because AI 
assistants are newer and hence more salient features of our cognitive ecology, that they 
pose a more significant threat to our autonomy. Indeed, as noted above, they may 
actually improve things relative to the current status quo, at least when it comes to 
making options more manageable.  

21 



 
To make the case that there is some special and significant threat you need to 

provide some reasons for thinking that AI assistants are radically different from what 
has gone before. To do this, you will need to highlight something especially 
problematic about the origin/purpose of AI-mediated influence or the modality of that 
influence. There might be reason for concern on both fronts. When it comes to the 
source/origin of AI-mediated influence it is important to bear in mind that most AI 
assistants are constructed by commercial enterprises with their own profit-seeking 
agendas. We might legitimately worry that those corporations will impose preferences 
and options on their AI-mediated choice architectures that suit those agendas but not our 
own well-being. Furthermore, as a reviewer to this paper pointed out, we may worry 
that because there are relatively few companies controlling this technology – Google, 
Amazon, Facebook and Apple being the obvious culprits – the widespread use of AI 
assistance will lead to greater centralization of control over choice architectures than 
was historically possible.  

 
It is worth taking these concerns seriously, but again perspective is required. I think 

there are three reasons to be less concerned about the source/origin of AI-mediated 
influence than critics commonly suppose. The first is simply that certain cultural 
institutions have always had outsized influence over our choice architectures – e.g. 
religions and the State – and its not clear that the degree of power or its centralization is 
worse now than it was in the past. If anything the reverse might be true. The Church had 
an outsized influence over the choice architectures of ordinary citizens in Europe for 
centuries and this influence expressed itself in ways that were pervasive and highly 
restrictive (Wu 2017). The choice architectures made possible by modern technology 
are more diverse and varied, and their tendency towards personalization and 
customisation, though costly in other ways, may often save them from the charge of 
being overly manipulative: they work to serve the previously expressed 
preferences/interests of the user and not necessarily those of the corporation controlling 
the technology.5 Furthermore, centralization is a double-edged sword. Although it may 

                                                            
5 I am indebted to an anonymous reviewer for pointing out the distinction between 
personalization and manipulation. As they pointed out, personalization also has costs, e.g a 
filter bubble that serves to reinforce prejudices, that may not be desirable in a pluralistic, 
democratic society, but it’s not clear that those problems are best understood in terms of a 
threat to autonomy. Cass Sunstein’s #Republic (2017) explores the political fallout of filter 
bubbles in more detail. 

22 



increase the power of certain organisations, it also makes it easier when it comes to 
holding people to account when things go wrong. If AI-mediated choice architectures 
are hijacked to serve nefarious and manipulative purposes, then the fact that they are 
controlled by relatively few, well-known corporations, makes it easier to locate 
potentially responsible agents and hold their feet to the fire for what they have done. 
The recent Cambridge Analytica/Facebook scandal over the ‘manipulation’ of voters in 
the lead-up to the 2016 election illustrates this point. Although it clearly highlights the 
problems with centralized control, it also provides some reassurance that when the 
problem is exposed it is possible to subject the organisation involved to critical and 
ultimately legal scrutiny. Finally, the centralization of power is not an intrinsic feature 
of this technology. There are many smaller-scale, narrowly-circumscribed, AI assistants 
available for use, some of which are discussed in the next section. It is possible for the 
individual to select a less centralized form of assistance for discrete tasks. 

 
What about the modality of interference? Is there some distinctive threat from AI 

assistants on that front? The closest thing to a distinctive threat would be Yeung’s 
concerns about hypernudging: the dynamic, constantly updated choice architecture. This 
is certainly worth taking seriously, but again it’s not clear that this is a distinctive or 
significant threat. Hypernudging differs from ordinary nudging by degree and not by 
kind. Furthermore, the differences in degree seem to go in the direction of 
personalization as opposed to outright manipulation. Nevertheless, there is something to 
be worried about here, particularly if the hypernudging capacity is hijacked by some 
nefarious third party. It would seem to be impossible for an individual user to 
adequately address this threat by themselves – some regulatory reform/legal 
intervention would likely be required to deal with all the potential problems. 
Consideration of such reforms lies beyond the scope of this essay, which is concerned 
with the personal ethics of AI use. Nevertheless, there is some guidance to be given at 
the personal level. I would argue that we should avoid a narrative of helplessness in the 
face of AI assistance. In the world as it is currently constituted we are not slaves to AI 
assistance, we do have some residual control over the extent to which we make use of 
this technology. There is no legal or moral compulsion to use them and we have our 
own self-judgment about the effect of certain choices on our happiness and fulfillment.. 
This self-judgment is an authoritative source of guidance when it comes to that 
happiness and fulfillment: no third party can second-guess whether selecting a certain 

23 



option has made us happy or not (Hare and Vincent 2016).6 We should, therefore, not 
be afraid to rely on this self judgment to either veto the influence of AI over our lives or 
to avoid making use of it in the first place. 

 
 So what emerges from this? Does AI assistance threaten our autonomy or not? 
There is no hard and fast answer. When we examine the conditions for autonomy in 
more detail we see that AI assistants don’t necessarily pose an unusual threat to 
autonomy: it’s unlikely that they would undermine the capacity for rational choice, and 
they may actually help, by pre-filtering options, to satisfy the optionality condition for 
autonomy. There are, however, risks of manipulation, particularly in the more subtle 
form of nudging or hypernudging (as opposed to the overt form of coercion). Some 
regulatory intervention and reform, such as the reforms under the new GDPR, will be 
needed to adequately address all of those risks. Nevertheless, we are not individually 
powerless in the face of such threats: people need to be made more cognizant of this. 
One of the purposes of this article is to do exactly this and to formulate principles that 
individuals could use to address the risks. A suggested principle here would be to get 
the person think about the decision-making context in which the AI assistant is being 
used, and to consider the source/motivations underlying the creators of the assistant: If 
you are using AI in a choice-context where there is not an overwhelming number of 
options, and if the commercial motivations, or centralized power of the organisation 
behind the app are abhorrent to you, then you probably should avoid using it. If the 
opposite is the case, there is likely nothing to fear and much to gain. In the intermediate 
case, where there are overwhelming options but also abhorrent motivations/centralizing 
power, more judgment will be required as the individual will need to tradeoff the risks 
involved. 

 
 
5. The Interpersonal Communication Argument 
This brings us to the final objection (or set of objections). The preceding objections 

tended toward the general: the degeneration of cognitive faculties and the undermining 
of autonomy and responsibility. The critical evaluation in each instance tended toward 
the specific: generalised objections are too vague to work; we need to pay attention to 
                                                            
6 As Hare and Vincent point out, while humans may be bad at predicting whether a future option 
will makes us happy our judgment as to whether a chosen option has made us happy is, 
effectively, incorrigible. Nobody knows better than ourselves. It is to this latter type of judgment 
that I appeal in this argument. 

24 



the specific ecological context in which AI gets used and the impact it has on cognitive 
ability, freedom and responsibility in those contexts. Now, I want to look at an 
objection that is tailored to a more narrowly-conceived set of contexts, while still 
retaining some degree of generality (as it concerns the ethics of interpersonal 
relationships). I do so partly because I think the objection is interesting in its own light, 
and partly in order to answer a challenge that was left lingering from the initial 
discussion of Carr’s degeneration argument. If you recall, I noted there that in order to 
convince us to take degeneration seriously, Carr either needed to show that it had some 
pervasive negative effect on our cognitive ecology (which I argued he could not do), or 
that it would knock-out or eliminate some set of abilities with really high intrinsic 
value, and whose loss would make our lives much worse. Is there any such risk? 

 
Evan Selinger’s work is instructive in this regard. In a series of articles and op-eds 

(Selinger 2014a, 2014b, 2014c; and Selinger and Frischmann 2016), he has articulated 
ethical objections to the use of AI that apply, in particular, to interpersonal 
communications. He focuses specifically on the negative impact of automated 
communication services. The essence of all these services is that they allow you to send 
text messages to your friends and intimate partners on preordained or random schedules, 
without the need for you to draft the messages at the particular moment that they are 
sent. Sometimes this is because you will have written the messages long in advance; 
sometimes it will be because you will have selected messages from a menu of options 
provided by the automated service; and sometimes it will be because the service drafts, 
selects and sends the message for you.  

 
There are several services that do this. Google’s Allo is a smart messaging platform 

that uses Google’s general Assistant AI software to learn about your messaging style, 
suggest messages to you, and automate sending and responding to messages. More 
specific services include apps like Romantimatic and Bro App, which are designed to 
send affectionate messages to romantic partners. Romantimatic seems well-intentioned, 
and is primarily designed to remind you to send messages (though it does include pre-
set messages that you can select with minimal effort). Bro App is probably less well-
intentioned (though it may be intended as a joke). It is targeted at men and is supposed 
to allow them to spend more time with their ‘bros’ by facilitating automated messaging. 
While these examples are interesting, it is important not to get too bogged down in their 
details. Apps of this sort pass in and out of existence frequently. What is popular today 

25 



may no longer be popular tomorrow. The more important thing is the general 
phenomenon of automated messaging which is made possible by advances in AI. 
Should we worry about this phenomenon? Selinger thinks we should, at least in some 
interpersonal contexts. While he fleshes out his concerns in several ways in his writings, 
two specific concerns stand out.  

 
The first is a worry about deceptiveness/inauthenticity: 
 

“…the reason technologies like BroApp are problematic is that they’re deceptive. They 
take situations where people make commitments to be honest and sincere, but treat those 
underlying moral values as irrelevant — or, worse, as obstacles to be overcome.”  
 
(Selinger 2014a) 

 
When sending messages to romantic or intimate partners, you should be sincere in 

what you say. If you say, ‘I’m thinking about you’ or ‘I miss you’ or ‘I love you’, you 
should mean what you say. If you haven’t actually drafted the message, or you have 
automated its communication in order to spend more time with your friends, then you 
are not being sincere. To put this into argumentative form: 

 
(9) It is a bad thing to be deceptive in your interpersonal relationships. 
 
(10) Automated messaging services encourage deceptive interpersonal communications. 
 
(11) Therefore, these apps are bad things. 

 
This is a relatively weak objection to the use of automated messaging. Premise (9) 

would seem to be flawed. There may be contexts in which a degree of deceptiveness is 
desirable in interpersonal relationships. The so-called ‘white lies’ that we tell to keep 
our partners happy may often be justifiable and may help to sustain rather than 
undermine a relationship. And premise (10) is definitely problematic insofar as the mere 
automation of a message does not make the sentiment or meaning deceptive. 
Deceptiveness is usually taken to denote an active intent to mislead another as to the 
context or the truth of what you are saying. It’s not clear to that automated messaging 
always involves that kind of active intent. Something like Bro App may do so, but this 
is a rather exceptional service. When you focus on the more general phenomenon, there 

26 



is no reason why someone could not set up a pre-scheduled list of messages that 
sincerely and truthfully conveyed their feelings toward another person. Suppose at the 
start of the week I ask Siri or Google Assistant to text my partner at three random 
intervals with affectionate messages. Suppose the messages are sent and received at 
those times, and my partners feels good about them. Am I being deceptive or 
inauthentic simply because I dictated it all in advance? Not necessarily. As long as I 
retain a principal-agent type relationship with the automated messaging service, there is 
no reason to think that the actions performed by the service are not accurately 
communicating my true feelings towards my partner.7 

 
This brings us to Selinger’s second concern about this type of technology. Beyond 

deceptiveness, he suggests that there are some interpersonal communications in which 
the value of the communication lies in the fact that it is an immediate, deliberate and 
conscious representation of how you feel about another person. In other words, that 
sometimes the real value of receiving an affectionate message from a loved one, lies not 
in the content of the message or the effect it produces, but rather in the fact that the 
other person is really thinking about you at that moment — that they are being 
intentional and ‘present’ in the relevant communicative context. The problem is that the 
very logic of these apps — the automated outsourcing of communications — serves to 
corrode this ‘real’ value. The apps are telling us that it is no longer important to be 
conscious and present in our communications; they tell us that what matters is content 
and effect; and that you can produce the required content and effect without being 
immediately present:  

 
(11) The real value of certain types of interpersonal communication is that they are 
immediate, conscious and intentional representations of how we feel. 
 
(12) Automated messaging services (by their very nature) create communications that are 
not immediate, conscious and intentional. 
 
(13) Therefore, automated messaging services undermine the real value of certain types 
of interpersonal communication. 

 
                                                            
7 As a reviewer points out, it may be impossible for interpersonal communication to ever 
adequately capture one’s true feelings. This may well be right but if so it would seem to be a 
problem for both automated and non-automated communications alike.  

27 



This is a more formidable objection because it gets to the heart of what AI assistants 
do (namely: redistribute cognitive labour). It is as close to an in-principle objection to 
the use of AI assistants, in a certain context, as you are likely to get. But there are 
several limitations to bear in mind.  

 
First, this objection cannot be easily generalised. It certainly applies beyond the 

communicative context, but even then it only applies to those cases in which the 
primary value of an act or performance lies in the fact that it is performed with 
immediate, conscious intention. This brings us back to the tradeoff between intrinsic vs 
instrumental values within our cognitive ecology that was raised earlier when discussing 
the degeneration problem. If a cognitive task derives most of its value from its extrinsic 
(instrumental) properties, then the objection does not apply. For example, the value of 
dividing up a bill at a restaurant does not lie in the conscious performance of the 
arithmetical operation; it lies in getting the right result. Outsourcing this activity seems 
unobjectionable. The same goes for many interpersonal communications. If my partner 
asks me to text her when I arrive at my destination after a long journey, the value of the 
message lies in conveying truthful information about my safe arrival, not in my 
consciously sending it at that particular moment in time. So whenever we have a case in 
which the value of the performance is instrumental, we can use the AI assistant without 
great concern. 

 
The tricky question is what we should do when the value of a performance is partly 

instrumental and partly intrinsic and there is no clearly asymmetric distribution of value 
across the instrumental and intrinsic components. The value of saying ‘I love you’ 
might sometimes lie in the fact that it is an immediate conscious expression of your 
feelings (e.g. if you are in a face-to-face setting and your partner says ‘I love you’, 
sending a text message hours later that reciprocates might not cut it). At other times the 
value might lie more in its sincerity and the effect it produces in your partner, not 
whether it tracks your immediate conscious affect. On still other occasions, the value 
might oscillate back and forth between these two poles.  

 
In those latter cases, the solution to Selinger’s ethical objection would be to ensure 

that the AI assistant does not completely dominate the performance of the activity. In 
other words, to ensure that we allow ourselves to take control and be involved in the 
immediate conscious performance of the activity on at least some occasions (or on those 

28 



occasions when it seems most appropriate). This is where Carr’s concerns about 
degeneration and cognitive resilience have most force. It is important to maintain some 
cognitive resilience so that we can be occasionally involved in those activities where 
immediate conscious intention is sometime desirable. This, however, leads to an 
important paradox. 

 
 
6. The Paradox of Internal Automaticity 
Selinger’s criticisms of AI are interesting because they highlight the value of 

immediate conscious intention in some activities. But in doing this, they also force us to 
confront the paradox of internal automaticity. Throughout this article we have been 
assuming that automation involves outsourcing cognitive performance to a machine 
(AI) that is external to your body and mind. But this assumption is not quite right. 
Automation can happen inside the body too. When you first learn how to do something 
(like ride a bike or drive a car or talk in front of an audience) it requires intense, 
occurrent, conscious effort: you have to think and represent each stage of the 
performance in order to get it right. But once the performance is mastered it becomes 
automatic: a subconscious routine that you can easily perform without the intense 
conscious effort. This is referred to as automaticity in the psychological literature. Some 
of the objectors to AI outsourcing are familiar with this phenomenon. Nicholas Carr, for 
one, pairs his discussion of cognitive degeneration with a sensitive account of internal 
automaticity. But he has no qualms or concerns about internal automaticity.  

 
This is, prima facie, paradoxical. Both internal automaticity and external automation 

involve the bypassing of immediate conscious intention. If the latter is problematic then 
it seems like the former should be too. If, when my partner says ‘I love you’ and I 
automatically and without thinking say ‘I love you’ back, this would seem to be just as 
problematic as if I got an app to send the message on my behalf (if we assume, 
arguendo, that the value of the communication lies primarily in its immediate conscious 
performance rather than the effect it produces). But the reality is that reflexive, 
unthinking, internally automated actions of this sort are commonplace. We often zone 
out of what we do because it is routine. Those who think that this is fine but that 
external automation is not owe us some explanation of why this is the case. That’s the 
paradox of internal automaticity. 

 

29 



There are at least three ways to address the paradox. The first is simply to concede 
defeat and accept that internal automaticity is just as much a problem as external 
automation, at least in those contexts where immediate conscious participation in an act 
is ethically preferred. This does not weaken the objection to the use of personal AI 
assistants, but it does suggest that the objection lies within a more general objection 
namely: thoughtless engagement in certain activities. Personal AI assistants are then 
problematic to the extent that they encourage or foster this thoughtlessness. The second 
way to address the paradox is to suggest that internal automaticity is less of problem 
due to its locus of control. This gets us back into the earlier debate about autonomy and 
manipulation. It could be that the internally automated behaviours are more under the 
control of the biological agent than external automating devices (which may be 
controlled by corporations with ulterior motives). This allows the objector to maintain 
some principled distinction between the two phenomena, but it’s not clear how 
persuasive the distinction is. Many internally automatised behaviours (e.g. default 
greetings and utterances) are the product of external cultural forces acting on the 
biological agent. These forces may also have ulterior motives or unwelcome 
consequences, e.g. automatised deference to one’s perceived social superiors is common 
in hierarchical societies and may serve to perpetuate the inequalities in such societies. A 
third way to address the paradox is to appeal to a conservative bias, i.e. to say that 
because internal automaticity has deeper biological and cultural roots, it is less morally 
problematic than external automation. Indeed, depending on how we interpret the 
conservative bias, we may even be inclined to presume that it this internal automaticity 
has some important intrinsic or instrumental value. As one reviewer to this article put it, 
internal automaticity is an essential conserved evolutionary resource: it is a way of 
adapting and learning behaviour at a relatively low-cost. This conservative bias is fine 
insofar as it goes, but it is unlikely to appeal to those who demand some rationale or 
explicit justification in their applied ethical reasoning. 

 
The point here is not to suggest that the paradox of internal automaticity is fatal to 

objections to personal AI assistance. The point, rather, is to suggest that once we are 
aware of it, some of the objections to AI assistance are a little bit blunter since we 
already live with the reality they seem to fear. 

 
 
 

30 



 7. Conclusion 
 This article has examined the ethics of personal AI assistants. It has argued that 
we should look at the use of such technology through an ecological lens: as something 
that changes the cognitive ecology in which we operate and that requires careful 
consideration of the effects within that ecology, particularly in terms of the tradeoff 
between instrumental and intrinsic value. It has assessed some paradigmatic ethical 
objections to the personal use of AI assistants, arguing that while none of them amounts 
to knockdown objection to the practice, they do raise important concerns. Responding 
to those concerns allows us to draft an initial framework for thinking about the sensible, 
ethical use of personal AI assistance. The framework comes as a series of ethical 
risk/reward guidance principles that ask users to pay attention to potential risks in 
certain contexts and to tailor their use of AI assistance in response to those risks: 
 

Degeneration Risk/Reward: When using AI assistants, one should be aware of the 
potential for cognitive degeneration with respect to an assisted task. If the task is one 
whose primary value is instrumental, and if the risk of being decoupled from the AI 
assistant is minimal, then this may not be a problem. But if the primary value of the task 
is intrinsic, and/or the risk of decoupling is high, it is probably best to cultivate one’s own 
cognitive capacities for the task. 
 
Autonomy Risk/Reward: When using AI assistants, one should be aware of the potential 
threat to autonomy due to (a) the fact that AI can sever the link between choice and result 
and (b) the fact that AI assistants can design the choice-architecture in which we make 
decisions. But one should bear in mind that AI assistants do not pose a unique and special 
threat to autonomy – many cultural and environmental forces already structure choice 
architectures in problematic ways – and that there is a danger in adopting a narrative of 
helplessness in relation to such threats. One should, consequently, approach personal AI 
assistants with the same discernment with which we approach existing threats. 
Furthermore, one should bear in mind that if the assistance comes in a domain with a lot 
of options, then the pre-selection may be advantageous and autonomy-enhancing. 
 
Interpersonal Virtue Risk/Reward: When using AI assistants to mediate interpersonal 
relationships, one should be aware that the primary value of some interpersonal actions 
comes from immediate, conscious engagement in the performance of that action. To the 
extent that AI assistants replace that immediate, conscious engagement, they should be 
avoided. Nevertheless, in many other cases, the value of interpersonal actions lies in their 

31 



content and effect, in these cases the use of AI assistants may be beneficial, provided it is 
not used in a deceptive/misleading way. 

 
This is, of course, very much a first draft. The intention would be for these risk/reward 
principles to be borne in mind by users of the technology as they try to make judicious 
use of them in their lives. But the principles could also be of use to designers. If they 
wish to avoid negatively impacting on their user’s lives, then considering the effect of 
their technologies on cognitive capacity, autonomy and interpersonal virtue would be 
important. Further elaboration of this framework is certainly required, and more 
guidance on which types of activity derive their value from immediate conscious 
engagement or the situations/abilities that would be most affected by decoupling, and 
hence in need of some resiliency, would be desirable, but it is hoped that this provides a 
platform on which others can build. 
 
 
 
 
Bibliography 
 
Burgos, D, Van Nimwegen, C, Van Oostendorp, H. and Koper, R. (2007). Game-based 
learning and inmediate feedback. The case study of the Planning Educational Task. 
International Journal of Advanced Technology in Learning. Available at 
http://hdl.handle.net/1820/945 (accessed 29/11/2016). 
 
Burrell, J. (2016). How the Machine Thinks: Understanding Opacity in Machine 
Learning Systems. Big Data and Society DOI: 10.1177/2053951715622512 
 
Carr, N. (2014) The Glass Cage: Where Automation is Taking Us (London: The Bodley 
Head) 
 
Crawford, M. (2015). The World Beyond Your Head. New York: Farrar, Strauss and 
Giroux. 
 
Danaher, J. (2016). The Threat of Algocracy: Reality, Resistance and Accommodation. 
Philosophy and Technology 29(3): 245-268. 

32 



 
Dworkin, G. The Theory and Practice of Autonomy (Cambridge: CUP 1988) 
 
Frankfurt, H ‘Freedom of the will and the concept of a person’ (1971) 68 Journal of 
Philosophy 5-20 
 
Frischmann, B. and Selinger, E. (2018). Reengineering Humanity. Cambridge: 
Cambridge University Press. 
 
Frischmann, B and Selinger, E. (2016). Engineering Humans with Contracts. Cardozo 
Legal Studies Research Paper No. 493 - available at 
https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2834011 (accessed 
29/11/2016) 
 
Frischmann, B. (2014). Human-Focused Turing Tests: A Framework for Judging 
Nudging and the Techno-Social Engineering of Humans. Cardozo Legal Studies 
Research Paper No. 441 - available at 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2499760 (accessed 
29/11/2016) 
 
Gunkel, D. (2011). The Machine Question. Cambridge, MA: MIT Press. 
 
Hare, S. and Vincent, N. (2016). Happiness, Cerebroscopes and Incorrigibility: 
Prospects for Neuroeudaimonia. Neuroethics 9(1): 69-84. 
 
Haraway, D. (1991). A Cyborg Manifesto: Science, Technology, and Socialist-
Feminism in the Late Twentieth Century. In Haraway, Simians, Cyborgs and Women: 
The Reinvention of Nature. New York; Routledge pp.149-181 
 
Heersmink, R. (2015). Extended Mind and Cognitive Enhancement: Moral Aspects of 
Extended Cognition. Phenomenal Cognitive Science DOI 10.1007/s11097-015-9448-5  
 
Heersmink, R. (2013). A Taxonomy of Cognitive Artifacts: Function, Information and 
Categories. Review of Philosophical Psychology 4(3): 465-481. 
 

33 



Kelly, S. and Dreyfus, H. (2011). All Things Shining. New York: Free Press. 
 
Kirsh, D. (2010) Thinking with External Representations. AI and Society 25:441-454 
 
Kirsh, D. (1995) The Intelligent Use of Space. Artificial Intelligence 73:31-68 
 
Krakauer, D. (2016). Will AI Harm Us? Better to Ask How We’ll Reckon with Our 
Hybrid Nature. Nautilus 6 September 2016 - available at http://nautil.us/blog/will-ai-
harm-us-better-to-ask-how-well-reckon-with-our-hybrid-nature (accessed 
29/11/2016) 
 
Luper, S. (2014). Life’s Meaning. In Luper (ed) The Cambridge Companion to Lie and 
Death. Cambridge: Cambridge University Press. 
 
Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S. & Floridi, L. (2016). The Ethics of 
Algorithms: Mapping the Debate. Big Data and Society. DOI: 
10.1177/2053951716679679  
 
Morozov, E. (2013). The Real Privacy Problem. MIT Technology Review. Available at 
http://www.technologyreview.com/featuredstory/520426/the-real-privacy-problem/ 
(accessed 29/11/16) 
 
Mullainathan, S. and Shafir, E. (2014) Freeing Up Intelligence. Scientific American 
Mind Jan/Feb: 58-63 
 
Mullainathan, S. and Shafir, E. (2012) Scarcity: The True Cost of Not Having Enough. 
London: Penguin. 
 
Nagel, S. 2010. Too much of a good thing? enhancement and the burden of self-
determination. Neuroethics 3: 109–119  
 
Nass, C./Flatow, I. (2013) The Myth of Multitasking. NPR: Talk of the Nation 10 May 
2013 - available at http://www.npr.org/2013/05/10/182861382/the-myth-of-
multitasking (accessed 29/11/2016). 
 

34 



van Nimwegen, C., Burgos, D., Oostendorp, H and Schijf, H. (2006). The Paradox of 
the Assisted User: Guidance Can be Counterproductive. Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems 917-926  
 
Newport, C. (2016) Deep Work. New York: Grand Central Publishing. 
 
Norman, D. (1991). Cognitive Artifacts. In JM Carroll (ed) Designing Interaction: 
Psychology at the Human-Computer Interface. Cambridge: Cambridge University 
Press. 
 
Ophir, E., Nass, C. and Wagner, A. (2009). Cognitive Control in Media Multitaskers. 
PNAS 107(37): 15583-15587. 
 
Pinker, S. (2010). The Cognitive Niche: Coevolution of Intelligence, Sociality, and 
Language. PNAS 107/Suppl 2: 8993-8999. 
 
Plato. The Phaedrus. From Plato in Twelve Volumes, Vol. 9, translated by Harold N. 
Fowler. Cambridge, MA, Harvard University Press; London, William Heinemann Ltd. 
1925. Available at http://www.english.illinois.edu/-people-
/faculty/debaron/482/482readings/phaedrus.html (accessed 29/11/2016) 
 
Raz, J. (1986). The Morality of Freedom. Oxford: OUP. 
 
Russell, S. and Norvig, P. (2016) Artificial Intelligence: A Modern Approach (Global 
3rd edition). Essex: Pearson.  
 
Sandel, M. (2012) What Money Can’t Buy: The Moral Limits of Markets. London: 
Penguin. 
 
Scheibehenne, B., R. Greifeneder, and P.M. Todd. 2010. Can there ever be too many 
options? a meta-analytic review of choice overload. Journal of Consumer Research 37: 
409–425  
 
Scherer, M. (2016). Regulating Artificial Intelligence Systems: Challenges, 
Competencies and Strategies. Harvard Journal of Law and Technology 29(2): 354-400 

35 



 
Schwartz, Barry. 2004. The paradox of choice: Why less is more. New York, NY: 
Harper Collins. 
 
Selinger, E. and Frischmann, B. (2016). The Dangers of Smart Communication 
Technology. The Arc Mag 13 September 2016 - available at 
https://thearcmag.com/the-danger-of-smart-communication-technology-
c5d7d9dd0f3e#.3yuhicpw8 (accessed 29/11/2016). 
 
Selinger, E. (2014). Today’s Apps are Turning us Into Sociopaths. WIRED 26 February 
2014 - available at https://www.wired.com/2014/02/outsourcing-humanity-apps/ 
(accessed 29/11/2016). 
 
Selinger, E. (2014). Don’t Outsource Your Dating Life. CNN: Edition 2 May 2014 - 
available at http://edition.cnn.com/2014/05/01/opinion/selinger-outsourcing-
activities/index.html (accessed 29/11/2016) 
 
Selinger, E. (2014). Outsourcing Your Mind and Intelligence to Computer/Phone Apps. 
Institute for Ethics and Emerging Technologies 8 April 2014 - available at 
http://ieet.org/index.php/IEET/more/selinger20140408 (accessed 29/11/2014). 
 
Shah, AK, Mullainathan, S. and Shafir, E. (2012) Some Consequences of Having too 
Little. Science 338: 682-685. 
 
Slamecka, N. and Graf, P. (1978). The Generation Effect: The Delineation of a 
Phenomenon. Journal of Experimental Psychology: Human Learning and Memory. 
4(6): 592-604. 
 
Smuts, A. (2013). The Good Cause Account of the Meaning of Life. Southern 
Philosophy Journal 51(4): 536-562. 
 
Sunstein, C. (2016). The Ethics of Influence. Cambridge, UK: Cambridge University 
Press. 
 

36 



Sunstein, C. (2017). #Republic: Divided Democracy in an Age of Social Media. 
Princeton, NJ: Princeton University Press. 
 
Thaler, R. and Sunstein, C. (2009). Nudge:Improving Decisions About Health, Wealth 
and Happiness. London: Penguin. 
 
Wertheimer, A. (1987). Coercion. Princeton, NJ: Princeton University Press. 
 
Whitehead, AN (1911). An Introduction to Mathematics. London: Williams and 
Norgate. 
 
Wu, T. (2017). The Attention Merchants. New York: Atlantic. 
 
Yeung, K. (2017). ‘Hypernudge’: Big Data as a mode of regulation by design. 
Information, Communication and Society 20(1): 118-136 
 
 
 
 

37﻿Why Teaching Ethics to AI Practitioners is Important

Judy Goldsmith and Emanuelle Burton

Abstract albeit often for work couched in terms of defense and secu-
rity, rather than offense and weapons. Some research labs are

We argue that it is crucial to the future of AI that our students able to support students because they accept military fund-
be trained in multiple complementary modes of ethical rea- ing; as such, concern about funding cannot be dismissed as
soning, so that they may make ethical design and implemen-
tation choices, ethical career decisions, and that their software purely selfish (though they still reflect a preference for their
will be programmed to take into account the complexities of own community of AI researchers.)
acting ethically in the world. In fact, many people made their decision based on moral

concerns, and yet arrived at different answers. It wasn’t just
that people were coming to different conclusions; in many

Introduction cases, they were beginning with different ideas about how to
Consider the decision many of us made last year about make an ethical decision. In this paper, we show how knowl-
whether to sign the open letter calling for a ban on offen- edge of different ethical frameworks can illuminate the dif-
sive autonomous weapons beyond meaningful human con- ferent approaches to decision-making that different AI prac-
trol. The authors (one of whom is an AI practitioner who titioners took, and re-examine the question of whether or not
also had to make this decision) use this case study to demon- to sign from within each of these frameworks.
strate that a knowledge of ethical frameworks is a crucially We demonstrate that most AI practitioners operate within
important tool in an AI student, AI practitioner, and AI the- the ethical framework called utilitarianism, which has been
orist’s toolbox. the dominant mode of ethical thought in the west for the past

For some AI practitioners, the decision to sign this open 150 years, and which is the ethical theory that is by far the
letter was a no-brainer, either because those individuals were most compatible with decision-theoretic analysis (Burton et
already committed to non-violence in some form, or because al. ). After describing utilitarian theory, we briefly introduce
they had thought at length about the dangers of weaponized deontology and virtue ethics, the other two major modes of
AI. Some no doubt signed because the leaders of the com- ethical analysis, and show how these two modes can offer
munity did so, and they wanted to be seen as one of the new perspectives on the decision of whether to add one’s
“cool kids.” Many others, however, chose not to sign the let- name to such a letter. We readily acknowledge that there is
ter, and even strongly opposed it, because they believe that much more to AI than war bots, and more to practitioners’
weaponized AI is inevitable, or desirable; because they be- decisions about public declarations than this initial summary
lieve that any AI can be used as a weapon;1 because they indicates. We could apply similar framing and analysis to the
were reluctant to associate their name with a petition when use of AI in medicine, management, computer games, or any
they could not predict the ramifications, or for many other other area. However, this case exemplifies one of the two
reasons arising from their understanding of research, poli- broad types of decisions that call for ethical analysis: per-
tics, or their personal moral imperatives. sonal decisions by AI practitioners and programmers, and

It was not entirely clear where a greedy agent would land decisions made by AI systems.
on this question. The letter was linked to an organization
that was offering grants for AI, more particularly for ethics- What appears to be a simple binary decision — sign the
and-AI projects. On the other hand, one might reasonably letter, or not? — is only the final stage in assessing one’s
conclude that signing such a letter could have a negative ef- (probably non-binary) views on several complicated ques-
fect on possible support through military-funded research. In tions. Should robots be used to kill people? Under what con-
some countries, there is significant military funding for AI, ditions could such robots be developed responsibly, and are

those conditions in place? But also: what other valuable or
Copyright ©c 2017, Association for the Advancement of Artificial positive purposes could this same technology serve? Could
Intelligence (www.aaai.org). All rights reserved. our work in other areas of AI continue without the finan-

1In the words of Ani DiFranco, “’Cause every tool’s a weapon cial support of the military? For many AI practitioners, the
— if you hold it right.” (From the song My I.Q.) answers to these questions are not black and white.



A Utilitarian Analysis of the Decision to Sign than with methods or intentions. This means that any possi-
When making a decision, computer scientists usually ana- ble law or rule could be set aside, in a given situation, if an
lyze the question in terms of utility. The first question an AI agent determines that adhering to that law conflicts signifi-
practitioner might ask, in defining the utility of signing and cantly with the greater good. According to utilitarianism, an
of not signing, is “Utility to whom?” This agent can consider agent is sometimes required to do harm in order to choose
the cost to herself in terms of potential future military fund- the best possible course; as such, the agent is not morally
ing, and to her institution and her students. She can weigh accountable for harm done under such circumstances, be-
that against the effect on her reputation as an ethical re- cause it was the “right” choice. “Ticking time bomb” sce-
searcher. Or she can consider the possible impact on enemies narios, such as those portrayed in the TV series 24 (in which
of her country if she were to choose to develop weaponized counterterrorism agent Jack Bauer routinely tortures sus-
AI, and weigh that against the impact of having human sol- pects for information), highlight both the prevalence of utili-
diers attempt the same acts. She might, further, consider the tarian thought and the appeal of this line of reasoning (Nissel
impact of her country — or all countries — having the tech- 2010).
nology she might develop. But decision-theoretic analysis is not the same as utilitar-

In order to compute the expected values of signing or ian analysis, and applying the core utilitarian principle of
not signing, this practitioner must decide the values of in- “the greatest good for the greatest possible number” places
dividual lives that could be ended by the technology, or lack some limits on my decision-theoretic analysis that are both
thereof. She must decide whose utility matters, and the rel- challenging and useful. This principle requires that the moral
ative weights of each person’s needs and desires. She must agent consider the well-being of everybody who is even po-
decide how she will handle expected future rewards and pay- tentially affected, and act in the way that produces the great-
offs. Each of these modeling decisions has enormous impact est possible benefit across that group. It is not acceptable, for
on the optimal policy for that model. instance, for her to decide that the needs of AI researchers

This type of moral reasoning has a solid foundation in (or denizens of her country, or members of her own faith)
ethical theory, specifically in a theory called utilitarianism. outweigh the needs of other people; and if she finds her-
As ethical theories go, utilitarianism is very recent, dating self reaching this conclusion, she needs to submit it to care-
back only to the late 18th century (though some elements of ful scrutiny. What is less obvious is how different kinds
it appeared much earlier. (Driver 2014).) Its basic principle of utility (running the gamut from basic physical safety to
is commonly formulated as “the greatest good for the great- improved professional opportunities) should weigh against
est possible number.” Utilitarianism holds that one’s primary each other, and — even more importantly — whether the
moral obligation is to work for the greatest possible happi- well-being of “the enemy” should figure into her calcula-
ness (defined by John Stuart Mill in his influential book Util- tions, and how much their well-being should matter com-
itarianism both as pleasure and as the absence of pain) for pared to her own side’s citizens and soldiers.
the greatest number of people (Mill 2002). This insistence Utilitarian thought — which we have only briefly sum-
on public (or even universal) good is what distinguishes util- marized here — does not provide straightforward answers
itarianism from a simple cost-benefit analysis, because it de- to these questions, but it offers analytical tools to help one
mands that the agent discount her own preferences for her- address them thoroughly and responsibly in a range of situ-
self, or for certain favored groups. ations. It can enable the AI practitioner to reach a more ethi-

Note that the language “optimal policies” in the example cally comprehensive position, allowing her to deploy famil-
above corresponds to that of decision-theoretic planning; we iar modes of reasoning while challenging her to look beyond
perceive a strong correspondence between a utilitarian anal- her own utility and personal concerns.
ysis of a decision on the one hand, and modeling the deci- Despite the value of utilitarian ethics, we believe that
sion and consequent choices as a Markov decision process AI practitioners should also be familiar with the other
on the other. In either case, the agent is attempting to opti- two major schools of ethical theory, deontology and virtue
mize total expected utility over time. Deciding on the scope ethics. These two approaches are far less compatible with
of utility, and assigning values to different kinds of lives, decision-theoretic analysis and other familiar analytic strate-
are the “moral” decisions available. Everything else is de- gies, which can make them challenging to understand and to
scriptive, although its validity as description depends on first apply. We argue that it is worthwhile, even essential, for AI
granting the values the agent has assigned. Yet the scope of practitioners to confront this challenge, and apply these the-
these moral decisions is significant: the outcome of a given ories in order to achieve the clearest possible understanding
analysis can, and usually does, turn on how “well-being” of a given situation, and of their own reasoning and decision-
or “utility” are defined, and on who qualifies to be a part making in response to it.
of the “everyone” whose well-being is taken into account.
As Singer (Singer 1981) shows, the appropriate definition of Deontology: Ethics by Rules
“everyone” has never been stable and continues to be con- Though utilitarianism is the ethical theory most compatible
tested (such as when Singer himself argues (Singer 1975) with contemporary culture, and thus typically feels the most
that there is no logical reason that the well-being of animals “useful”, almost everyone in the world today also has some
should not be part of our ethical calculations.) experience with deontology, or law-based ethics. Deontolog-

In contrast to the two other major schools of ethical the- ical ethics (from the Greek “deon,” which means “duty” or
ory, utilitarianism is concerned only with outcomes, rather “obligation”), conceives of ethics in terms of laws, or rules:



my actions are ethical insofar as they conform to (or do not Alternately, one could begin with the axiom that scholar-
violate) the law (Alexander and Moore 2015). It is worth ship should be publicly available (or, similarly, that scholars
noting that there can be several layers of law; for example, should be able to distribute their own work freely), and that
the law “do not kill” is more fundamental than the law to Swartz acted rightly; indeed, if one considers the free avail-
drive on the correct side of the road in your country, but ability of scholarship to be a fundamental law, then one is
this latter law is still binding (except in those rare conditions obligated to uphold that law even if one will face criminal
when it is necessary to violate the lower law to preserve the charges. Deontological analysis helps explain why Swartz
higher one), because it creates the conditions for you to fol- might have felt his actions were ethically necessary; whether
low the moral law. Though is not always easy to know what or not one believes they were ethical, full stop, will depend
the law requires, one is always required to follow it. This on whether or not one shares his axioms, and believes it is
requirement makes deontology far less flexible than utilitar- right to apply those same axioms in all situations concerning
ianism, but can also furnish one with the conviction to take intellectual property, or violations of civil law.
difficult or unpopular stances. Similarly, there are different possible starting places for a

There are different ways of understanding or defining the deontological analysis of the decision to sign (or not to sign)
law. The three Abrahamic religions (Judaism, Christianity, the open letter. An AI practitioner who begins with the law
and Islam) are all Divine Command traditions, in which the “do not kill” will probably sign the letter, because building
law is understood to be given by God; it is a person’s duty war bots requires working against that principle. But he also
to follow that law, although it is recognized by most denom- might also decide not to sign on the basis of Just War The-
inations of each of these religions that human beings have ory. This theory invokes cost-benefit logic to argue that war,
to do a lot of work to interpret the laws, and to ascertain the while awful, is sometimes better than the alternative. Just
best way to apply them in complex situations. In Immanuel War Theory demands that specific criteria be met in order
Kant’s reformulation, however, the moral law is something for a war to be deemed just; for example, acts of war must
each individual must discover herself, not by trusting in au- be proportional, and must be directed only at active enemy
thority but through the ongoing application of reason. Ac- combatants, rather than civilians or injured soldiers. If this
cording to Kant, the true law is universal: the further a given practitioner were to conclude that military AI would help his
principle can be generalized, the closer it is to the true law country’s military effect more targeted attacks with less col-
(Rohlf 2016). (Thus, one can discover through reason that lateral damage (and if he trusted military leaders to use it in
“do not kill” is more fundamental than “drive on the correct that way), then he might conclude that developing this tech-
side of the road.”) In all forms of deontology, the ability to nology would be the best way to honor this principle. An-
follow the law relies on the agent’s ability to correctly assess other practitioner might believe that her primary duty is to
what part(s) of the law are most fundamental. fulfill her professional duties because she has made the com-

The language of rules or laws can make deontology seem mitment to do so, both by becoming an expert in the field and
analogous, at first glance, to the application of axiomatic by accepting a job. She may conclude that she should not
systems. In some ways, the comparison is helpful: just as sign the letter, because so much of AI research relies on mil-
members of religious traditions have to analyze situations itary funding. Alternately, she might decide that AI research
that fit uncomfortably within existing laws (sometimes be- needs to break its ties to the military, even though this break
cause the situations are the product of modern developments will entail a significant loss in funding and prestige; if she
that postdate the laws (see extensive discussion of this in reached this conclusion, she would be morally obligated to
Johnson (2009)), sometimes because the situation exists at sign onto the letter, even though the consequences would be
the intersection of several laws, which appear to dictate dif- severe.
ferent solutions), so too do programmers spend considerable These are only two of many possible principles or axioms
time, energy, and creativity on exception handling. But abid- that could be used to guide one’s reasoning. As we have
ing by deontology is more like living within an axiomatic shown, deontological reasoning does not always lead differ-
system than building one, because deontological reasoning ent agents to the same conclusions, even when the founda-
and analysis do not allow you to change the laws. Whether tional principle is consistent, but will also depend on how the
the laws are given by authority or ascertained by reason, they agent describes the situation (s)he is responding to (e.g., will
are understood to exist independently of the goals or desires this technology be used to pursue a more just war?) For the
of the individual. practitioner struggling to ascertain which principle(s) should

Although deontology presumes that a given agent abide take priority over the others, deontology’s preference for
consistently by the same set of laws or axioms (rather generalizable rules can be a helpful way to determine which
than picking and choosing according to the situation), one principles are most fundamental.
can begin with any somewhat-general axiom and apply de-
ontological reasoning. Consider Aaron Swartz’ project to Virtue Ethics
make copyrighted articles freely available (Mechanic 2013; Virtue ethics, the third major approach to ethics, is focused
Swartz 2008). One could argue that, since Swartz was liv- on individual character, and how one develops good quali-
ing in the United States and benefiting from the order cre- ties or “virtues” (such as honesty or courage) and the abil-
ated by its laws, he was bound to uphold those laws; and be- ity to apply them. Virtue ethics has ancient roots in both
cause downloading copyrighted material with intent to share Greek and Chinese thought, though contemporary terminol-
it freely is illegal, Swartz’s actions were therefore unethical. ogy is drawn primarily from Aristotle, whose Nichomachean



Ethics is still considered foundational (Hursthouse 2013) Because virtue ethics is concerned with character, it focuses
(Aristotle 1999). Virtue ethics is a “big picture” system, and on long-term patterns of action, rather than on single acts;
individual actions and problems are evaluated in terms of the analysis of any single decision will need to be framed by
how they fit into the arc of a person’s life. According to questions like: “who do I want to be?” and “what do I hope
virtue ethics, a person’s character is a product of habits, to accomplish?”
which are strongly influenced by their social context: a per- The agent’s decision might be entirely separate from his
son is much more likely to do something that feels ordinary, beliefs about whether an open letter to the UN will im-
whether because it matches their self-understanding or just pact the future of mechanized warfare, but will instead re-
seems like a “normal” thing to do. This means that a person’s flect what he believes will be his decision’s impact on his
past actions are a useful predictor of how they will choose to own sphere. His question is not whether robots should kill,
act in the present. It also means that a person’s present-day but whether he should be the sort of person who publicly
choices can and should be understood, in part, as choices protests against weaponized AI, and how the decision to
about who she will become in the future, because they affect sign will influence the person (and the professional) he will
the scope of actions that feel familiar or “like me.” become. He will probably weigh the question of whether

In this sense, virtue ethics is the approach most focused signing the letter will endear him to his supervisor and col-
on the interior life of the agent. But this intensive focus on leagues or alienate them; he might also consider how his
interiority is balanced by a profound concern for the “pub- decision will affect the possibility of future avenues of re-
lic self”–the agent’s present and future reputation among search, though he might conclude that his decision about
friends, family, and community. Though worrying about the letter is more important. He might ask himself how he
one’s own reputation might seem at first to fall outside the would explain his decision to his spouse, or his friends, and
purview of ethical concern — or even, from a deontological how it would change their understanding of him. He might
perspective, to obstruct one’s focus on the principles at stake consider how his decision will raise his profile, within his
— this concern becomes more intelligible when we consider local community, as an advocate for certain kinds of causes,
the ways in which one’s public reputation shapes the way thus enhancing (or limiting) his ability to influence public
one is able to act or to contribute to the public sphere. Only discussions in the future. His decision may also depend on
a licensed doctor can practice medicine; a person with a low whether he opts to define himself as an AI professional, as
credit rating will have a harder time obtaining a loan; and ac- a US citizen, as a father, or as a human being (whatever any
tors are often criticized when they talk about social issues at of those terms means to him.) All of these questions reflect
awards shows, on the grounds that the nature of their social the practitioner’s interest in who he will become, both as an
contributions does not qualify them or afford them the right individual and as a member of his community. Many people
to engage in political advocacy. Although much of the im- already ask themselves these sorts of questions when facing
portant work of character takes place internally, an agent’s an important decision. Virtue ethics offers a model for or-
character is meaningful only inasmuch as it manifests in ganizing them around considered goals, and for evaluating
public action, which can and should be evaluated by oth- how each possible choice fits in with those goals.
ers: if (to pick a low-stakes example) you think your jokes By focusing on the social context of ethical action, virtue
are hilarious but everyone else find them tedious, then they ethics also affords a framework for an agent to reflect upon
are right and you are wrong. The importance of one’s repu- the ways in which AI, and the actions of AI practition-
tation can have significant consequences for decision mak- ers, creates a broader social context for moral action. Virtue
ing. For example, Aristotle argues in Book 9, Chapter 8 of ethics prompts us to consider the character of the decision-
the Nichomachean Ethics that it can be to a man’s advan- makers who program war bots, or the politicians or mili-
tage to sacrifice his own life for others; though he will not tary leaders who decide to deploy them. According to virtue
live to reap the material or emotional benefits of his own ethics, the act of making these decisions, and of engaging
heroism, the benefit to his reputation will be great enough to in their implementation, has an impact on those individuals:
outweigh even the sacrifice of his own life, a calculation that over time, their character shifts to accommodate the actions
can actually make sense if public reputation (along with the they are undertaking, as their sense of what is normal or ap-
well-being of one’s beloved friends) is understood to be an propriate migrates. By building robots that kill (even if only
essential and valuable part of oneself. Aristotle’s example for particular specialized purposes) they adapt their sense of
illustrates nicely how different theories can seem to align, what is normal to make room for what they are doing, and
but for different reasons: we can see from this example how thus they become the sort of people who are more likely to
either virtue ethics or utilitarianism might lead an agent to think robots that kill are the appropriate solution to a given
make the same decision (to sacrifice his life for his fellow problem. Virtue ethics therefore pushes us to consider the
citizens or soldiers) even though his reasons for reaching sort of decisions that are inherent in the jobs AI practition-
that conclusion would be very different. We can see, fur- ers have, and helps us think about the social and technolog-
ther, how two separate agents working within different ethi- ical contexts that force such choices. This can help us see
cal paradigms might arrive at the same decision, while being connections between technological development and moral
unable to reach consensus on why such an extreme action choices, and to choose whether or not to act in ways that will
could be considered the best choice. protect our own and others’ characters.

Virtue ethics offers a very different framework for a prac- The virtue ethics perspective shows us that developing
titioner who is deciding whether or not to sign the letter. war bot technology changes the social conditions for moral



decision making. By engaging with the virtue ethics frame- long-term project that informs this paper) and John Fike for
work, AI practitioners can interrogate how AI shapes society helpful discussions.
and thus creates the conditions for certain moral crises.

References
Conclusions Alexander, L., and Moore, M. 2015. Deontological ethics.

We have used the question of choosing whether to sign an In Zalta, E. N., ed., The Stanford Encyclopedia of Philoso-
open letter to show that applying ethical frameworks to the phy. Spring 2015 edition.
analysis of a decision greatly enriches our decision pro- Aristotle. 1999. Nichomachean Ethics. Hackett. trans.
cesses, and gives us tools for understanding and evaluating Terence Irwin.
decisions. As the above discussions make clear, ethical the- Burton, E.; Goldsmith, J.; Koenig, S.; Kuipers, B.; Mattei,
ory introduces new critical tools for analysis, but can help N.; and Walsh, T. Ethical considerations in artificial intelli-
identify the ways in which one is already thinking and rea- gence courses. to appear, AI Magazine.
soning about ethical questions. This same way of reframing
and discussing decisions can apply to many kinds of deci- Burton, E.; Goldsmith, J.; and Mattei, N. 2015. Teaching AI
sions we make as AI practitioners, and to decisions our soft- ethics using science fiction. In 1st International Workshop
ware may make on our behalf (Burton, Goldsmith, and Mat- on AI, Ethics and Society, Workshops at the Twenty-Ninth
tei 2015). AAAI Conference on Artificial Intelligence.

A solid grasp of the different ethical theories can help Burton, E.; Goldsmith, J.; and Mattei, N. 2016. Using “The
practitioners hold themselves, and others, accountable for Machine Stops” for teaching ethics in artificial intelligence
the decisions they make and how they make them, at both and computer science. In AI & Ethics: Workshops at the
a macro and a micro level. By understanding the reason- Thirtieth AAAI Conference on Artificial Intelligence.
ing structure of the different theories, a practitioner is bet- Driver, J. 2014. The history of utilitarianism. In Zalta, E. N.,
ter equipped to follow the ramifications of her own values ed., The Stanford Encyclopedia of Philosophy. Winter 2014
and judgments, and — once she has seen their implications edition.
— to reconsider those judgments and values. For example, Hursthouse, R. 2013. Virtue ethics. In Zalta, E. N., ed., The
she might conclude that a particular surveillance device that Stanford Encyclopedia of Philosophy. Fall 2013 edition.
seemed (or still seems) harmless in itself is dangerous, be-
cause of how it fits into a larger view of privacy; or she might Johnson, D. G. 2009. Computer Ethics. Prentice Hall Press.
decide that she is willing to compromise on an issue that is Mechanic, M. 2013. Steal this research paper! (You already
important to her because she has come to consider it less paid for it.). Mother Jones.
important than the thing she gains by the compromise. For Mill, J. S. 2002. Utilitarianism. Hackett Publishing Co.
example, she might accept a job at a company whose de- Nissel, M. 2010. The ever-ticking bomb: Examining 24’s
vices collect extensive data on their user, even though she promotion of torture against the background of 9/11. aspeers
worries about how these devices violate users’ privacy, be- 3:37–51.
cause working there gives her the opportunity to build in
privacy-protecting features. Rohlf, M. 2016. Immanuel kant. In Zalta, E. N., ed., TheStanford Encyclopedia of Philosophy. Spring 2016 edition.

Because AI work has enormous effects on society —
including the ways in which individuals interact, on our Singer, P. 1975. Animal Liberation. Clarendon Press Ox-
economies, on the practice of medicine and the uses of ford.
leisure, to name a few — we believe that all AI practitioners, Singer, P. 1981. The Expanding Circle. Clarendon Press
and those that reason about AI technology, should be able to Oxford.
frame discussion in terms of ethics. Further, we believe that Swartz, A. 2008. Guerilla open ac-
popular culture promotes a very limited understanding of cess manifesto. Online-Ressource,
how to frame and analyze ethical decision making. Thus, we http://archive.org/stream/GuerillaOpenAccessManifesto
advocate that ethics be taught in AI classes, and that we de- /Goamjuly2008 djvu.txt.
velop materials and courses for teaching ethical frameworks
and reasoning to people working in AI (Burton, Goldsmith,
and Mattei 2016). (See also (Burton, Goldsmith, and Mat-
tei 2015; 2016) for a discussion on using science fiction to
teach AI ethics.)

Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1646887. Any
opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the views of the National Science Foun-
dation. We thank Nicholas Mattei (our collaborator in the﻿See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/337228125

The Debate on the Ethics of AI in Health Care: a Reconstruction and Critical

Review

Preprint · November 2019
DOI: 10.13140/RG.2.2.27135.76960

CITATION READS

1 1,530

7 authors, including:

Jessica Morley Caio C V Machado
University of Oxford University of Oxford

37 PUBLICATIONS   143 CITATIONS    5 PUBLICATIONS   22 CITATIONS   

SEE PROFILE SEE PROFILE

Christopher Burr Josh Cowls
Alan Turing Institute University of Oxford

19 PUBLICATIONS   97 CITATIONS    37 PUBLICATIONS   446 CITATIONS   

SEE PROFILE SEE PROFILE

Some of the authors of this publication are also working on these related projects:

From What to How: Applied AI Ethics View project

Even good bots fight: The case of Wikipedia View project

All content following this page was uploaded by Jessica Morley on 13 November 2019.

The user has requested enhancement of the downloaded file.



The Debate on the Ethics of AI in Health Care: a Reconstruction and Critical Review 
 
 
Jessica Morley1* **, Caio C. V. Machado1*, Christopher Burr1, Josh Cowls1,2, Indra Joshi4, Mariarosaria 
Taddeo1,2,3, Luciano Floridi1,2,3   
 

1 Oxford Internet Institute, University of Oxford, 1 St Giles’, Oxford, OX1 3JS, UK 
2 Alan Turing Institute, British Library, 96 Euston Rd, London NW1 2DB, UK 
3 Department of Computer Science, University of Oxford, 15 Parks Rd, Oxford, OX1 3QD, UK 
* These authors contributed equally to the writing of this paper  
** Email of correspondence author: Jessica.morley@kellogg.ox.ac.uk  
 
 
Abstract 
 
Healthcare systems across the globe are struggling with increasing costs and worsening outcomes. This 

presents those responsible for overseeing healthcare with a challenge. Increasingly, policymakers, 

politicians, clinical entrepreneurs and computer and data scientists argue that a key part of the solution 

will be ‘Artificial Intelligence’ (AI) – particularly Machine Learning (ML). This argument stems not 

from the belief that all healthcare needs will soon be taken care of by “robot doctors.” Instead, it is an 

argument that rests on the classic counterfactual definition of AI as an umbrella term for a range of 

techniques that can be used to make machines complete tasks in a way that would be considered 

intelligent were they to be completed by a human. Automation of this nature could offer great 

opportunities for the improvement of healthcare services and ultimately patients’ health by 

significantly improving human clinical capabilities in diagnosis, drug discovery, epidemiology, 

personalised medicine, and operational efficiency. However, if these AI solutions are to be embedded 

in clinical practice, then at least three issues need to be considered: the technical possibilities and 

limitations; the ethical, regulatory and legal framework; and the governance framework. In this article, 

we report on the results of a systematic analysis designed to provide a clear overview of the second of 

these elements: the ethical, regulatory and legal framework. We find that ethical issues arise at six levels 

of abstraction (individual, interpersonal, group, institutional, sectoral, and societal) and can be 

categorised as epistemic, normative, or overarching. We conclude by stressing how important it is that 

the ethical challenges raised by implementing AI in healthcare settings are tackled proactively rather than 

reactively and map the key considerations for policymakers to each of the ethical concerns highlighted.  

 
Keywords 
 
Artificial Intelligence; Ethics; Healthcare; Health Policies; Machine Learning.  

 1 



1. Introduction 
 
Healthcare systems across the globe are struggling with increasing costs and worsening outcomes 

(Topol, 2019). This presents those responsible for overseeing healthcare systems with a ‘wicked 

problem’, meaning that the problem has multiple causes, is hard to understand and define, and hence 

will have to be tackled from multiple different angles. Against this background, policymakers, 

politicians, clinical entrepreneurs and computer and data scientists increasingly argue that a key part of 

the solution will be ‘Artificial Intelligence’ (AI), particularly Machine Learning (Chin-Yee & Upshur, 

2019). The argument stems not from the belief that all healthcare needs will soon be taken care of by 

“robot doctors” (Chin-Yee & Upshur, 2019). Instead, the argument rests on the classic counterfactual 

definition of AI as an umbrella term for a range of techniques (summarised in Figure 1 below) that 

can be used to make machines complete tasks in a way that would be considered intelligent were they 

to be completed by a human. For example1, as mapped by (Harerimana, Jang, Kim, & Park, 2018), 

decision tree techniques can be used to diagnose breast cancer tumours (Kuo, Chang, Chen, & Lee, 

2001); Support Vector Machine techniques can be used to classify genes (Brown et al., 2000) and 

diagnose Diabetes Mellitus (Barakat, Bradley, & Barakat, 2010); ensemble learning methods can predict 

outcomes for cancer patients (Kourou, Exarchos, Exarchos, Karamouzis, & Fotiadis, 2015); and 

neural networks can be used to recognise human movement (Jiang & Yin, 2015). From this 

perspective, AI represents a growing resource of interactive, autonomous, and often self-learning (in the 

machine learning sense, see Figure 1) agency, that can be used on demand (Floridi, 2019), presenting 

the opportunity for potentially transformative cooperation between machines and doctors (Bartoletti, 

2019). 

 
1 For a full overview of all supervised and unsupervised Machine Learning techniques and their applications in 
healthcare, see Harerimana, Jang, Kim, & Park, 2018 and for a detailed look at the number of papers related to 
AI techniques and their clinical applications see (Tran et al., 2019) 

 2 



 
Figure 1. AI Knowledge Map (AIKM). Source: Corea (2019), reproduced with permission courtesy 
of F. Corea. 
 

 

If harnessed effectively, such AI-clinician coordination (AI-Health) could offer great opportunities for 

the improvement of healthcare services and ultimately patients’ health (Taddeo & Floridi, 2018) by 

significantly improving human clinical capabilities in diagnosis (Arieno, Chan, & Destounis, 2019; De 

Fauw et al., 2018; Kunapuli et al., 2018), drug discovery (Álvarez-Machancoses & Fernández-Martínez, 

2019; Fleming, 2018), epidemiology (Hay, George, Moyes, & Brownstein, 2013), personalised medicine 

(Barton et al., 2019; Cowie, Calveley, Bowers, & Bowers, 2018; Dudley, Listgarten, Stegle, Brenner, & 

Parts, 2015) or operational efficiency (H. Lu & Wang, 2019; Nelson, Herron, Rees, & Nachev, 2019). 

However, as Ngiam & Khor (2019) stress, if these AI solutions are to be embedded in clinical practice, 

then at least three issues need to be considered: the technical possibilities and limitations; the ethical, 

regulatory and legal framework; and the governance framework.  

The task of the following pages is to focus on the second of these elements — the ethical, 

regulatory and legal framework — by stressing how important it is that the ethical challenges raised by 

 3 



implementing AI in healthcare settings are tackled proactively (Char, Shah, & Magnus, 2018). If they are 

not, there is a risk of incurring significant opportunity costs (Cookson, 2018) due to what Floridi terms 

a ‘double bottleneck’ whereby “a double bottleneck: ethical mistakes or misunderstandings may lead 

to social rejection and/or distorted legislation and policies, which in turn may cripple the acceptance 

and advancement of [the necessary] data science”.2 Although essential, encouraging this kind of 

proactive ethical analysis is challenging because – although bioethical principles for clinical research 

and healthcare are well established, and issues related to privacy, effectiveness, accessibility and utility 

are clear (Nebeker, Torous, & Bartlett Ellis, 2019) – other issues are less obvious (Char et al., 2018). 

For example, AI processes may lack transparency, making accountability problematic, or may be 

biased, and leading to unfair behaviour or mistaken decisions (Mittelstadt, Allo, Taddeo, Wachter, & 

Floridi, 2016). Identification of these less obvious concerns requires input from the medical sciences, 

economics, computer sciences, social sciences, law, and policy-making. Yet, research in these areas is 

currently happening in siloes, is overly focused on individual level impacts (Morley & Floridi, 2019b), 

or does not consider the fact that the ethical concerns may vary depending on the stage of the 

algorithm development pipeline (Morley, Floridi, Kinsey, & Elhalal, 2019). Taken together, these issues 

are inhibiting the development of a coherent ethical framework. 

Whilst AI-Health remains in the early stages of development and relatively far away from 

having a major impact on frontline clinical care (Panch, Mattie, & Celi, 2019) there is still time to 

develop this framework. However, this window of opportunity is closing fast, as the pace at which AI-

Health solutions are gaining approval for use in clinical care in the US is accelerating (Topol, 2019). 

Both the Chinese (Zhang, Wang, Li, Zhao, & Zhan, 2018) and British governments (Department of 

Health and Social Care, 2019) have made it very clear that they intend on investing heavily in the spread 

and adoption of AI-Health technologies. It is for these reasons that the goal of this article is to offer 

a cross-disciplinary systematic review mapping the potential ethical implications of the development 

of AI-Health in order to support the development of better design practices, and transparent and 

accountable deployment strategies. We will do this in terms of digital ethics. That is, we will focus on 

the evaluation of moral problems related to data, algorithms and corresponding practices (Floridi & 

Taddeo, 2016), with the hope of enabling governments and healthcare systems looking to adopt AI-

Health to be ethically mindful (Floridi, 2019a).  

 
  

 
2 https://digitalethicslab.oii.ox.ac.uk/the-ethics-of-medical-data-analytics-opportunities-and-challenges/  

 4 



2. Methodology in Brief 
 

A detailed outline of the methodology used to conduct the review can be found in the appendix. For 

now, suffice to say that a traditional thematic review methodology (following Abdul, Vermeulen, 

Wang, Lim, & Kankanhalli, 2018) was used to find literature from across disciplinary boundaries that 

highlighted ethical issues unique to the use of AI algorithms in healthcare. This means that the review 

did not focus on issues such as lack of evidence, privacy and security (Vayena, Tobias, Afua, & 

Allesandro, 2018), or definitions and secondary uses of healthcare data, as these are ethical issues for 

digital health at large and not unique to AI. More detailed discussion of these issues is highlighted in 

Table 1.  

 

General Digital Health Ethical Concern Example References   

Data Sharing/Data Privacy   (Kalkman, Mostert, Gerlinger, Van Delden, & Van Thiel, 

2019)(Olimid, Rogozea, & Olimid, 2018) (Parker, Halter, 

Karliychuk, & Grundy, 2019)(Quinn, 2017) (Richardson, Milam, 

& Chrysler, 2015) (Townend, 2018) 

Secondary use of Healthcare Data (Lee, 2017) (Nittas, Mütsch, Ehrler, & Puhan, 2018) (O’Doherty 

et al., 2016)  

Surveillance, Nudging and Paternalism (Maher et al., 2019) (Marill, 2019)(Nebeker et al., 2017) (Morley & 

Floridi, 2019d)(Burr, Mariarosaria, & Floridi, 2019) 

Consent  (S. Millett & O’Leary, 2015) (T. Ploug & Holm, 2016) (Mann, 

Savulescu, & Sahakian, 2016) (Balthazar, Harri, Prater, & Safdar, 

2018) 

Definition of Health Data  (Floridi et al., 2018) (Voigt, 2019) (Holzinger, Haibe-Kains, & 

Jurisica, 2019) (Kleinpeter, 2017) 

Ownership of Health Data  (Chiauzzi & Wicks, 2019) (Krutzinna, Taddeo, & Floridi, 2018) 

(Shaw, Gross, & Erren, 2016) (Sterckx, Rakic, Cockbain, & 

Borry, 2016) (Stephan Millett & O’Leary, 2015) 

Embodied Intelligence/Robotics (Fiske, Henningsen, & Buyx, 2019) 

Digital Divide/eHealth Literacy  (Celi et al., 2016) (Kuek & Hakkennes, 2019) 

Patient involvement  (Aitken et al., 2019) (Page, Manhas, & Muruve, 2016) 

Patient Safety  (Barras, 2019) 

Evidence of Efficacy  (Ferretti, Ronchi, & Vayena, 2019) (Henson, David, Albright, & 

Torous, 2019) (Larsen et al., 2019) 

Table 1: Example literature related to ethical concerns that are relevant for all digital health intervention, not unique to AI-Health and 

therefore not included in this review  

 

To ensure that the focus stayed on the unique ethical issues, the map, developed by (Mittelstadt et al., 

2016), of the epistemic, normative, and overarching ethical concerns related to algorithms was used as 

a base. First, the selected literature was reviewed to identify healthcare examples of each of the 

 5 



concerns highlighted in the original map, as shown in Table 2, and then reviewed more thoroughly to 

identify how the ethical issues may vary depending on whether the analysis was being conducted at: (i) 

individual, (ii) interpersonal, (iii) group (e.g. family or population), (iv) institutional, (v) sectoral, and/or 

(v) societal levels of abstraction (LoA)3 (Floridi, 2008). This helped the review avoid the narrow focus 

on individual-level impacts highlighted in the introduction. This approach is not intended to imply that 

there is no overlap between the levels.    

 

 Ethical Concern  Explanation Medical Example 
Epistemic concerns  Inconclusive Evidence  Algorithmic outcomes (e.g. EKG readers in smartwatches may ‘diagnose’ a patient 

classification) are as suffering from arrhythmia when it may be due to a 
probabilistic and not fault with the watch not being able to accurately read that 
infallible. They are rarely user’s heartbeat (for example due to the colour of their 
sufficient to posit the skin) or the ‘norm’ is inappropriately calibrated for that 
existence of a causal individual (Hailu, 2019) 
relationship. 

Inscrutable Evidence  Recipients of an algorithmic A clinical decision support system deployed in a hospital 
decision very rarely have full may make a treatment recommendation, but it may not 
oversight of the data used be clear on what basis it has made that ‘decision’ raising 
to train or test an algorithm the risk that it has used data that are inappropriate for 
or the data points used to the individual in question or that there is a bug in the 
reach a specific decision.  system leading to issues with over or under prescribing 
 (Wachter, 2015).  

Misguided Evidence  Algorithmic outcomes can Watson for Oncology is in widespread use in China for 
only be as reliable (but also ‘diagnosis’ via image recognition but has primarily been 
as neutral) as the data they trained on a Western data set leading to issues with 
are based on.  concordance and poorer results for Chinese patients than 

their Western counterparts (Liu et al., 2018). 
Normative Concerns  Unfair outcomes  An action can be found to An algorithm ‘learns’ to prioritise patients it predicts to 

having more of an impact have better outcomes for a particular disease. This turns 
(positive or negative) on out to have a discriminatory effect on people within the 
one group of people  Black and minority ethnic communities (Garattini, 

Raffle, Aisyah, Sartain, & Kozlakidis, 2019). 
Transformative effects  Algorithmic activities, like An individual using personal health app has limited 

profiling, re-conceptualise oversight over what passive data it is collecting and how 
reality in unexpected ways.  that is being transformed into a recommendation to 
 improve, limiting their ability to challenge any 

recommendations made and a loss of personal autonomy 
and data privacy (Kleinpeter, 2017).   

Overarching Traceability  Harm caused by algorithmic If a decision made by clinical decision support software 
activity is hard to debug (to leads to a negative outcome for the individual, it is unclear 
detect the harm and find its who to assign the responsibility and or liability to and 
cause), and it is hard to therefore to prevent it from happening again (Racine, 
identify who should be held Boehlen, & Sample, 2019)..  
responsible for the harm  
caused.  

Table 2: A summary of the epistemic, normative and overarching ethical concerns related to algorithmic use in healthcare based on 

Mittelstadt et al (2016) from (Jessica Morley & Floridi, In Press) .   

 
3 A level of abstraction can be imagined as an interface that enables one to observe some aspects of a system 
analysed, while making other aspects opaque or indeed invisible. For example, one may analyse a house at the 
LoA of a buyer, of an architect, of a city planner, of a plumber, and so on. LoAs are common in computer 
science, where systems are described at different LoAs (computational, hardware, user-centred etc.). Note that 
LoAs can be combined in more complex sets, and can be, but are not necessarily hierarchical, with higher or 
lower ‘resolution’ or granularity of information. 

 6 



 

3. Thematic Analysis  

What follows is a detailed discussion of the issues uncovered. A summary table (Table 3) is provided 

at the end of the section.  

 
3.1. Epistemic Concerns: Inconclusive, Inscrutable, and Misguided Evidence  

Many factors are encouraging the development of AI-Health (Chin-Yee & Upshur, 2019). One of the 

main driving forces is the belief that algorithms can make more objective, robust and evidence-based 

clinical decisions (in terms of diagnosis, prognosis or treatment recommendations) than a human 

health care provider (HCP) can (Kalmady et al., 2019). This is not an unfounded position. Machine 

learning methods, especially ensemble and unsupervised methods (Harerimana et al., 2018), can take 

into account a far greater range of evidence (data) than a Health Care Provider (HCP) when making a 

clinical decision, including five of the seven dimensions of healthcare data provided by the US 

Department of Health and Human services: (1) demographic and socioeconomic data; (2) symptom 

and existing diagnosis data; (3) treatment data; (4) outcome data; and (5) other omic data  (Holzinger 

et al., 2019)4. If designed taking into account the multiple epistemic concerns, this ability enables 

clinical algorithms to act as digital companions (Morley & Floridi, 2019d), reducing the information 

asymmetry that exists between a HCP and the individual seeking care by making available information 

accessible to both parties and helping ensure that the most informed decision possible is made by the 

person who has the right to make it (Morley & Floridi, 2019a).  

 It is at least in part due to this ability to make ‘evidence-based’ decisions that, as AI-health 

research has shown, AI techniques can considerably augment or surpass human capabilities when it 

comes to tasks including: (1) analysis of risk factors (De Langavant, Bayen, & Yaffe, 2018; Deng, Luo, 

& Wang, 2018); (2) prediction of disease (Moscoso et al., 2019); (3) prediction of infection (Barton et 

al., 2019)(López-Martínez, Núñez-Valdez, Lorduy Gomez, & García-Díaz, 2019); (4) population 

health monitoring (F. S. Lu, Hattab, Clemente, Biggerstaff, & Santillana, 2019; Zacher & Czogiel, 

2019); (5) prediction of adverse effects (Ding, Tang, & Guo, 2019; Mortazavi et al., 2017); (6) 

prediction of outcome and/or likelihood of survival (Dong et al., 2019; Popkes et al., 2019; Topuz, 

Zengul, Dag, Almehmi, & Yildirim, 2018); and (7) analysing electronic health records (Shickel, Tighe, 

Bihorac, & Rashidi, 2018). These capabilities should not be underestimated, particularly as AI-Health 

solutions can operate at scale, diagnosing or predicting outcomes for multiple people at once – 

 
4 The other two categories refer to data from the Healthcare system, such as expenditure and healtchcare 
resources data. 

 7 



something that an HCP could never do. Yet in many ways this almost unwavering faith in the truth-

telling power of AI-Health is flawed.  

 As has been highlighted multiple times in the wider ethical AI literature, the belief that 

algorithms are more objective than humans is a ‘carefully crafted myth’ (Gillespie, Boczkowski, & 

Foot, 2014), and just because an algorithm can recognise a pattern (for example) does not necessarily 

make it meaningful (Floridi, 2014). In the context of healthcare, existing methods and studies 

(potentially including those referenced) suffer from overfitting due to small numbers of samples, 

meaning that the majority of results (e.g. patterns of disease risk factors, or presence of disease) are 

inconclusive (Holzinger et al., 2019). This is a problem that is further magnified by the lack of 

reproducibility, and external validity, of results. AI-Health solutions are often untranslatable between 

different settings and rarely work in settings different to those in which the initial result was obtained 

(Vollmer, Mateen, Bohner, Király, Ghani, et al., 2018), raising serious questions about the scientific 

rigor of AI-Health and its safety (Vayena, Blasimme, & Cohen, 2018). Furthermore, the results can 

often be heavily value-laden, based on the definition of ‘healthy’ by influential people or powerful 

companies (McLaughlin, 2016). This raises a number of significant ethical concerns.  

 At the individual LoA there is considerable risk of misdiagnosis. This can happen in at least 

two ways: either, as highlighted in table 2, by an individual using a wearable device that has a bug, or 

is inappropriately calibrated for them could be ‘told’ that they are suffering from a health condition 

when they are not (or vice versa) or, an HCP relying on clinical decision support software (CDSS) 

(Ruckenstein & Schüll, 2017) could be given an inaccurate diagnosis or recommendation which they 

do not question due to a tendency to uncritically accept the decisions of automated systems (Challen 

et al., 2019). Moreover, this can have impacts in medical practice, causing overreliance on the machine 

diagnostics and deskilling of practitioners (Cabitza, Rasoini, & Gensini, 2017). Not only is this a risk 

for individuals, but it also reverses the advantage of AI-Health solutions being able to operate at scale 

by introducing the group LoA ethical concern of misdiagnosis or missed diagnosis happening 

repeatedly. Whilst an HCP might give one person the wrong diagnosis and then be corrected, a faulty 

algorithm, based on the misguided, inscrutable or inconclusive evidence could give the same wrong 

diagnosis to hundreds or thousands of people at a time (Topol, 2019). The scale of the problems is as 

large as the scale of the solutions. 

 Building on this, there are also ethical implications at the interpersonal LoA. HCP-patient 

relationships are primarily based on trust and empathy, and whilst AI-Health solutions can take over 

tasks that are more routine and standardised, they cannot reproduce the emotional virtues of which 

human HCPs are capable (Ngiam & Khor, 2019). Consequently, an over-reliance on the ‘quantitative’ 

 8 



and objective evidence that fuels clinical algorithms (Cabitza et al., 2017) could discredit other forms 

of diagnosis (Rosenfeld et al., 2019) – a prominent concern in the case of clinical psychiatry 

(Burns, 2015) – and lead to the de-humanisation or impersonalisation of care provision (Juengst, 

McGowan, Fishman, & Settersten, 2016), from a service based on listening and theory to one based 

purely on categorisation (an issue that could again lead to a group LoA harm of group-profiling and 

associated discrimination by providers including insurers, see section 3.2.). Not only is this effectively 

‘paternalism in disguise’ (Juengst et al., 2016) but it could also lead to poorer health outcomes due to 

the lack of disconnect between pure medical evidence and actual behaviour change (Emanuel & 

Wachter, 2019).  

 Finally, scaling up to the institutional, sectoral and societal LoAs, there is the concern that 

public health decisions are increasingly made on predictive AI-Health algorithms, which too often rely 

on the same flawed assumptions as outlined above. Regarding these assumptions, consider what is by 

now a classic example: the Google Flu Trends monitoring of the influenza virus. The initial algorithm 

distorted the spread of the virus in the US (Vayena, Salathé, Madoff, & Brownstein, 2015). If policy 

decisions about where to deploy health resources are based on such poor-quality evidence, this could 

result in the waste of public funds (e.g., promoting vaccination campaigns where they are not needed), 

damage local economies (e.g., scaring away tourists from a region) – which would result in a positive 

feedback loop of less money available for public expenditure – and lead to poorer quality public 

healthcare provision and thus worse health outcomes for society at large. This worry is particularly 

paramount when it is considered that the ultimate ambition of AI-Health is to create a learning 

healthcare system where the ‘system’ is constantly learning from the data it receives on the 

performance of its interventions (Faden et al., 2013). Furthermore, it is worth noting that, at this 

juncture, the example offered above of Flu Trends does not represent the limits of Google’s interest 

– and that of its subsidiaries and its siblings under parent company Alphabet – in public health. As we 

discuss below, the engagement between Alphabet’s artificial intelligence subsidiary DeepMind and a 

major UK hospital has attracted the attention of data protection regulators, the press, and academics 

(Information Commissioner, 2018; Powles & Hodson, 2017). The challenge of ensuring that AI-

Health systems function accurately has in turn sparked debates about the appropriateness of sharing 

data between public and private entities. In response to claims that patient data transferred from the 

Royal Free Hospital to DeepMind was “far in excess of the requirements of those publicly stated 

needs” (Powles & Hodson, 2017), DeepMind representatives argued that “data processed in the 

application have been defined by and are currently being used by clinicians for the direct monitoring 

 9 



and care of AKI [acute kidney injury] patients” (King et al., 2018). Powles and Hodson responded in 

turn that it is a “statement of fact that the data transferred is broader than the requirements of AKI” 

(Powles & Hodson, 2018). As this series of claims and counter-claims demonstrates, the quality and 

quantity of data required for a particular AI-Health application is likely to be a matter of dispute in the 

context of the collection and sharing of patient data in training AI-Health. 

Ultimately, data is necessary for medical practice and thus so are AI-Health solutions that can 

take in greater volumes of data. But data collected and used in this way is insufficient to inform medical 

practice; it must be transformed to be useful (Car, Sheikh, Wicks, & Williams, 2019) and if this 

transformation process is flawed the results could be hugely damaging, resulting in either wasted funds 

and poorer health provision, or undue sharing of patient data with private sector actors under the guise 

of AI-Health.  

 
3.2. Normative Concerns: Unfair Outcomes and Transformative Effects  

As referenced in the introduction, healthcare systems across the globe are struggling with increasing 

costs and decreasing outcomes (Topol, 2019) and their administrators increasingly believe that the 

answer may well lie in making healthcare systems more informationally mature  and able to capitalise 

on the opportunities presented by AI-Health significantly to improve outcomes for patients, and to 

reduce the burdens on the system (Cath, Wachter, Mittelstadt, Taddeo, & Floridi, 2017). Whilst it 

would be ethically remiss to ignore these opportunities (Floridi, 2019a), it would be equally ethically 

problematic to ignore the fact that these opportunities are not created by AI-Health technologies per 

se but by their ability to re-ontologise (that is, fundamentally transform the intrinsic nature of) the ways in 

which health care is delivered by coupling, re-coupling and de-coupling different parts of the system 

(Floridi, 2017a). For example (Morley & Floridi, 2019b):  

• Coupling: patients and their data are so strictly and interchangeably linked that the 

patients are their genetic profiles, latest blood results, personal information, allergies 

etc. (Floridi, 2017a). What the legislation calls ‘data subjects” become “data patients”; 

• Re-Coupling: research and practice have been sharply divided since the publication 

of the National Commission for the Protection of Human Subjects in the 1970s, but 

in the digital scenario described above, they are re-joined as one and the same again 

(Petrini, 2015) (Faden et al., 2013); 

• De-Coupling: presence of Health Care Provider (HCP) and location of Patient 

become independent, for example because of the introduction of online consultations 

(NHS England, 2019).  

 10 



 

As a result of these transformations a number of ethical concerns arise.  

Starting once again with the individual LoA: as more diagnostic and therapeutic interventions 

become based on AI-Health solutions, individuals may be encouraged to share more and more 

personal data about themselves (Racine et al., 2019) — data that can then be used in opaque ways 

(Sterckx et al., 2016). This means that the ability for individuals to be meaningfully involved in shared 

decision making is considerably undermined (Vayena et al., 2018) As a result, the increasing use of 

algorithmic decision-making in clinical settings can have negative implications for individual 

autonomy, as for an individual to be able to exert agency over the AI-Health derived clinical decision, 

they would need to have a good understanding of the underlying data, processes and technical 

possibilities that were involved in it being reached (DuFault & Schouten, 2018) and be able to ensure 

their own values are taken into consideration (McDougall, 2019). The vast majority of the population 

do not have the level of eHealth literacy necessary for this (Kim & Xie, 2017), and those that do 

(including HCPs) are prevented from gaining this understanding due to the black-box nature of AI-

Health algorithms (Watson et al., 2019). In extreme instances, this could undermine an individual’s 

confidence in their ability to refuse treatment (Thomas Ploug & Holm, 2019). Such issues pose a 

substantial threat to an individual’s integrity of self (the ability of an individual to understand the forces 

acting on them) (Cheney-Lippold, 2017). Given that damage to a person’s psychological integrity can 

be perceived as a ‘harm’, not accounting for this potentiality poses the risk of creating a system that 

violates the first principle of medical ethics: primum non nocere (“first, do no harm”) (Andorno, 2004; 

Morley & Floridi, 2019d).  

It is not necessarily the case that harmful impacts will primarily be felt by the patients. At the 

interpersonal LoA, HCPs may themselves feel increasingly left ‘out of the loop’ as decisions are made 

by patients and their ‘clinical advice’ algorithm in a closed digital loop (Nag, Pandey, Oh, & Jain, 2017). 

As a result, HCPs may too feel unable to exert their own agency over the decision-making capacity of 

AI-Health solutions. Though the use of algorithmic decision-making makes diagnostics seem like a 

straightforward activity of identifying symptoms and fitting them into textbook categories, medical 

practice is much less clear-cut than it seems (Cabitza et al., 2017). Clinical practice involves a series of 

evaluations, trial and error, and a dynamic interaction with the patient and the medical literature. As a 

result, formal treatment protocols should be seen more as evaluative guidelines than well-defined, 

isolated categories. AI-Health solutions may not be in accordance with current best practice, which is 

 11 



necessary to handle the great degree of uncertainty5 and can only be fully evaluated by physicians 

(Cabitza et al., 2017).  Therefore, AI-Health solutions need to allow HCPs to exert influence in the 

decision-making process.   

At the group LoA the concern is that AI-Health systems may well be able to cope better with 

illnesses and injuries that have well-established and fairly set (and therefore automatable) treatment 

protocols. These are more likely to exist for afflictions most commonly suffered by white men as there 

is a greater volume of medical trials data for this group than there is for almost any other group. 

Algorithms trained on such biased datasets could make considerably poorer predictions for, for 

example, younger black women (Vayena et al., 2018). If HCPs are left out of the loop completely and 

learning healthcare systems primarily rely on automated decisions, there is considerable potential to 

exacerbate existing inequalities between the “haves” and the “have-nots” of the digital healthcare 

ecosystem, i.e., those that generate enough data on themselves to ensure accurately trained algorithms 

and those that do not (Topol, 2019).   

To mitigate these and associated risks, institutions need to be asking the crucial question: 

how much clinical decision-making should we be delegating to AI-Health solutions (Di Nucci, 2019)? 

If it is known that algorithms which enable profiling (e.g. those that determine genetic risk profiles) 

can ignore outliers and provide the basis for discrimination (Garattini et al., 2019), so deciding whether 

healthcare also ought to be a means to promote social justice is crucial in order to establish what type 

of data services will be embedded in the system (Voigt, 2019), what data should be collected, and 

which values should be embedded in algorithmic decision-making services (McDougall, 2019). This 

decision also determines what sort of population-level behavioural change the health system should 

be able to aim for depending on cost management, data collection and fairness in data-driven systems 

(Department of Health and Social Care, 2018.). If not carefully considered, this process of 

transforming the provision of care risks over-fitting the system to a specific set of values that may not 

represent those of society at large (McDougall, 2019).  

Another, more subtle yet pervasive transformative effect arises at the sectoral level. Powles 

& Hodson (2017) argue that one risk that may arise from collaboration between public and private 

sector entities such as that between the Royal Free London hospital and DeepMind is that the positive 

 
5 Here we are discussing fairly routine illnesses and injuries that have set treatment protocols that may need to 
be flexibly interpreted on a case-by-case basis. We recognize that there are other instances, such as in the case 
of rare diseases, where algorithmic systems might be better equipped to deal with diagnostic uncertainty (for 
example in cases of rare disesaes) by being able to draw on a wider range of data points and information 
sources than a human clinician could. 

 12 



benefits of AI-Health “solutions” will be siloed within private entities. They note that in the Royal 

Free case, “DeepMind [was given] a lead advantage in developing new algorithmic tools on otherwise 

privately-held, but publicly-generated datasets” (Powles & Hodson, 2017, p. 362). This, they suggest, 

may mean that the only feasible way that future advances may be developed is “via DeepMind on 

DeepMind’s terms”. This interpretation was contested by DeepMind, who called it “unevidenced and 

untrue” and claimed that the Information Commissioner agreed with their stance in her 2018 ruling 

(King et al., 2018). Whatever the circumstances of this particular case, the broader risk of privately 

held AI-Health solutions – trained on datasets that have been generated about the public by public 

actors but then (lawfully) shared with private companies – is worthy of caution going forward.  

 As may now be clear, these transformative effects also have significant ethical implications at 

the societal LoA. Before institutions can establish where and how (and, from the sectoral perspective, 

whether) AI-Health solutions can improve care, society itself must make difficult decisions about what 

care is and what constitutes good care (Coeckelberg, 2014). To offer a simplistic example, does it mean 

purely providing a technical diagnosis and an appropriate prescription or does it involve contemplating 

a series of human necessities that revolve around well-being (Burr, Taddeo, & Floridi, 2019)? If it is 

the former, then it is relatively easy to automate the role of non-surgical clinicians through AI (although 

this does not imply that doctors should be substituted by AI systems). However, if is the latter, then 

providing good health care means encompassing psychological wellbeing and other elements related 

to quality of life, which would make human interaction an essential part of healthcare provision, as a 

machine does not have the capability to make emotionally-driven decisions. Consequently, certain 

decisions may completely exceed the machine’s capabilities and thus delegating these tasks to AI-

Health would be ethically concerning (Matthias, 2015).  

Consider, for example, a situation where an AI-Health solution decides which patients are 

sent to the Intensive Care Unit (ICU). Intensive care is a limited resource and only people who are at 

risk of losing their lives or suffering grave harms are sent there. Triage decisions are currently made by 

humans with the aim of maximising well-being for the greatest number of people. Doctors weigh 

different factors when making this decision, including the likelihood of people surviving if they are 

sent to the ICU. These situations often involve practitioners (implicitly) taking moral stances, by 

prioritising individuals based on their age or health conditions. These cases are fundamentally oriented 

by legal norms and medical deontology, yet personal expertise, experience and values also inevitably 

play a role. Having the support of AI-Health in the ICU screening increases the number of agents and 

complicates the norms involved in these decisions, since the doctor may follow his or her professional 

guidelines, while the algorithm will be oriented by the values embedded in its code. Unless there is a 

 13 



transparent process for society to be involved in the weighing of values embedded in these decision-

making tools (for instance, how is ‘fair’ provision of care defined?) (Cohen, Amarasingham, Shah, Xie, 

& Lo, 2014), then the use of algorithms in such scenarios could result in the overfitting of the health 

system to a specific set of values that are not representative of society at large.  

In response to this risk, some attempts have already been made to involve the public at large 

in decisions over the design and deployment of AI systems. In early 2019, the UK’s Information 

Commissioner’s Office and the National Institute for Health Research staged a series of “citizens’ 

juries” to obtain the opinions of a representative cross-section of British society regarding the use of 

AI in health (Information Commissioner, 2019). The “juries” were presented with four scenarios, two 

relating to health — using AI to diagnose strokes, and using it to find potential matches for a kidney 

transplant — and another two relating to criminal justice. Notably, the juries “strongly favoured 

accuracy over explanation” in the two scenarios involving AI in health (National Institute for Health 

Research, 2019). This is just one example of research which attempted to obtain public opinion data 

regarding AI in health, and there are reasons to suppose that the apparent preference among 

participants for accurate over explainable AI systems reflects the high-stakes and fast-moving scenarios 

that were presented (as opposed to, say, the more routine illnesses and injuries we are focusing on 

here). Nonetheless, it demonstrates the plausibility and preferability of involving the public in 

designing AI-Health systems. 

To conclude this sub-section, the notion that AI-Health technologies are ethically neutral is 

unrealistic, and having them perform moral decision-making and enforcement may provoke immoral 

and unfair results (Rajkomar, Hardt, Howell, Corrado, & Chin, 2018). The direct involvement of the 

public in the design of AI-Health may help mitigate these risks. This should be borne in mind by all 

those involved in the AI-driven transformation of healthcare systems.  

 
3.3 Overarching Concerns: Traceability  

The previous sub-section outlined how the increasing use of AI-Health is fundamentally transforming 

the delivery of healthcare and the ethical implications of this process, particularly in terms of potentially 

unfair outcomes. This transformation process means that healthcare systems now rely on a dynamic, 

cyclical and intertwined series  of interactions between human, artificial and hybrid agents (Vollmer, 

Mateen, Bohner, Király, & Ghani, 2018)(Turilli & Floridi, 2009). This is making it increasingly 

challenging identify interaction-emerging risks and allocate liability, raising ethical concerns with 

regards to moral responsibility.  

 14 



Moral responsibility involves both looking forward, where an individual, group or 

organisation is perceived as being in charge of guaranteeing a desired outcome, and looking backwards 

to appropriate blame and possibly redress, when a failure has occurred (Wardrope, 2015). In a well-

functioning healthcare system, this responsibility is distributed evenly and transparently across all 

nodes so that the causal chain of a given outcome can be easily replicated in the case of a positive 

outcome, or prevented from repeating in the case of a negative outcome (Floridi, 2013, 2016). In an 

algorithmically-driven healthcare system, a single AI diagnostic tool might involve many people 

organising, collecting and brokering data, and performing analyses on it, making this transparent 

allocation of responsibility almost impossible. In essence, not only is the decision-making process of 

a single algorithm a black-box, but the entire chain of actors that participate in the end product of AI-

Health solutions is extremely complex. This makes the entire AI-Health ecosystem inaccessible and 

opaque, making responsibility and accountability difficult. 

To unpack the ethical implications of this at-scale lack of traceability, let us take the example 

of a digital heart-rate monitor that ‘intelligently’ processes biological and environmental data to signal 

to its user their risk of developing a heart condition.  

At the individual LoA this process relies on what can be termed the ‘digital medical gaze’ 

(Morley & Floridi, 2019d) and is based on this micro-cycle of self-reflection adapted from (Garcia, 

Romero, Keyson, & Havinga, 2014):  

1. Gaining Knowledge: Algorithm reads multi-omic dataset to determine risk of heart attack 

and providers individual with a ‘heart health score’  

2. Gaining Awareness: on the advice of the algorithm, individual starts monitoring their 

activity level and becomes aware of how active they are 

3. Self-reflection: as directed by the algorithm the individual reflects on how much high fat 

food they are eating in a day and compares this to their optimal diet based on their 

genomic profile and their level of activity  

4. Action: individual takes the advice of the algorithm and takes specific actions to improve 

their heart-health score e.g. starts regular exercise.  

If this does not work, and the individual still ends up experiencing heart failure, this process of 

algorithmic surveillance (Rich & Miah, 2014) risk creating an elaborate mechanism for victim-blaming 

(Danis & Solomon, 2013; McLaughlin, 2016). The individual may be seen as being a ‘bad user’ for 

failing to act upon the allegedly objective and evidence-based advice of the algorithm (see section 3.1), 

and may therefore be framed as being morally responsible for their poor health and not deserving of 

state-provided healthcare. Yet, due to the lack of traceability, there can be no certainty that the poor 

 15 



outcome was due to the lack of action by the individual: it could be a faulty device, buggy code, or the 

result of biased datasets (Topol, 2019). Moreover, even if a negative outcome were to result purely 

from an individual disregarding the guidance, the adoption of digital infrastructure that enables failure 

to be ascribed to a morally ‘culpable’ individual is itself a matter of ethical concern. These new insights 

may enable lives to be saved and quality of life to be drastically improved, yet they also shift the ethical 

burden of ‘living well’ squarely onto newly accountable individuals. The ontological shift that this new 

infrastructure permits — from individuals-as-patients deserving quality healthcare, regardless of their 

prior choices as fallible humans, to individuals-as-agents expected to take active steps to pre-empt 

negative outcomes — raises stark questions for bioethics, which has traditionally been seen as an 

“ethics of the receiver” (Floridi 2008). Moreover, these technological changes might prompt a shift in 

the ethical framework, burdening the individuals, while not providing de facto means of behavioural 

change. Many concerns stem from socio-demographic issues which entail harmful habits, and cannot 

oversimplified to a matter of delivering the adequate information to the patient (Owens & Cribb, 

2019). 

 Due to issues of bias (discussed further in section 3.2), there is, further, a group LoA ethical 

risk that some groups may come to be seen as being more morally irresponsible about their healthcare 

than others. Heart-rate monitors, for example, are notoriously less accurate for those with darker skin 

(Hailu, 2019), meaning that they could give considerably less accurate advice to people of colour than 

to those with light skin. If this results in people of colour being less able to use AI-Health advice to 

improve their heart-health, then these groups of people may be seen as morally reprehensible when it 

comes to their health. Furthermore, the healthcare could then ‘learn’ to predict that people of colour 

have worse heart-health, potentially resulting in these groups of individuals being discriminated against 

by, for example, insurers (Martani, Shaw, & Elger, 2019).  

At the interpersonal, institutional and sectoral LoAs, this moral responsibility translates 

into liability. If for example, instead of the heart-health algorithm providing the advice back to the 

individual, it provides the data to the individual’s HCP and the HCP provides advice that either fails 

to prevent an adverse event or directly causes an adverse event, this could be the basis of a medical 

malpractice suit (Price, 2018). In this scenario, it remains unclear where the liability will eventually sit 

(Ngiam & Khor, 2019). Current law implies that the HCP would be at fault, and therefore liable, for 

an adverse event as the algorithm in this scenario would be considered a diagnostic support tool – just 

like a blood test – with no decision making capacity, so it is the HCP’s responsibility to act 

appropriately based on the information provided (Price, Gerke, & Cohen, 2019; Schönberger, 2019; 

Sullivan & Schweikart, 2019). However, the supply chain for any clinical algorithm is considerably 

 16 



more complex and less transparent than that of a more traditional diagnostic tool meaning that many 

are questioning whether this is actually how the law will be interpreted in the future. For example, does 

the liability really sit with the HCP for not questioning the results of the algorithm, even if they were 

not able to evaluate the quality of the diagnostic against other sources of information, including their 

own personal knowledge of the patient due to the black-box nature of the algorithm itself? And what 

about the role of the hospital or care facility: does it have a responsibility to put in place a policy 

allowing HCPs to overrule algorithmic advice when this seems indicated? Similarly, what role do 

commissioners or retailers of the device that contains the algorithm play? Do they not carry some 

responsibility for not checking its accuracy, or do they assume that this responsibility sits with the 

regulator (for example MHRA in the UK, the FDA in the US or the CFDA in China) who should, 

therefore, carry the burden for not appropriately assessing the product before it was deployed in the 

market? What if the problem is further back in the chain, stemming from inaccurate coding or poor-

quality training data? There is a clear lack of distributed responsibility (Floridi, 2013, 2016)– a problem 

that is exacerbated by a lack of transparency – making it hard to hold individual parts of the chain 

accountable for poor outcomes which poses a significant ethical risk.  

In their overview of patient-safety issues with AI in healthcare He et al. (2019) state that those 

working in the field are trying to establish a systems-wide approach that does not attribute blame to 

individuals or individual companies, but conclude that where liability will ultimately rest remains to be 

seen. This is problematic because, as Hoffman et al. (2019) stress, uptake of algorithmic-decision-

making tools by the clinical community is highly unlikely until this liability question is resolved (Vayena 

et al., 2018), which could result in the overarching ethical concern raised in the introduction – that of 

a significant missed opportunity. Many, including (Holzinger et al., 2019) believe that explainability is 

the answer to solving this problem and that, if HCPs can understand how a decision was reached, then 

reflecting on the output of an algorithm is no different from any other diagnostic tool. Indeed 

Schönberger (2019) argues that legally this is the case and that as long as it can be proven that the duty 

of care was met, then harm caused to a patient by an erroneous prediction of an AI-Health system 

would not yet constitute medical negligence but that it might in the near future constitute negligence to 

not rely on the algorithmic output – which brings us back to the issues outlined in section 3.1. 

Overall, this lack of clarity will continue to persist for some time (Schönberger, 2019), making 

it once again a social issue. Society will ultimately dictate what the socially acceptable and socially 

preferable (Floridi & Taddeo, 2016) answers are to these pressing questions. The ethical issue is 

whether all parts of society will have an equal say in this debate, as in the example of citizens’ juries 

above, or whether it will be those individuals or groups with the loudest voices that get to set the rules. 

 17 



As (Beer, 2017) attests, when thinking about the power of an algorithm, we need to think beyond the 

impact and consequences of the code, to the powerful ways in which notions and ideas about the 

algorithm circulate throughout the social world.  

 
 A. Individual  B. Interpersonal C. Group D. Institutional  E. Sectoral F. Societal  
1. Epistemic Misdiagnosis Loss of trust in Misdiagnosis or Waste of funds Excessively Poorer public 
concern or missed HCP-Patient missed diagnosis and resources not broad data healthcare 
(inconclusive, diagnosis  relationships, de- at scale – some directed to areas sharing provision and 
inscrutable and personalisation of groups more of greater need between worsening 
misguided care affected than public and health 
evidence)  others  private outcomes for 

entities  society 
2. Normative Surveillance & Deskilling of Profiling and Transformation Siloing of Inequalities in 
(transformative undermining HCPs, overreliance discrimination of care pathways new AI tool outcomes 
effects and of autonomy on AI-tools, and against certain & imposing of development 
unfair and integrity undermining of groups seen as specific values at within 
outcomes)  of self  consent practices being less scale – redefining private 

and redefining healthy or ‘good care’  sector 
roles in the higher risk  
healthcare system 

3. Overarching ‘Bad Users’ See institutional Specific groups Lack of clarity See Society must 
(traceability)  could come to framed as being over liability with institutional decide through 

be blamed for more morally regards to issues regulation 
their own ill- irresponsible with safety and preferable 
health with regards to effectiveness answers to the 

their health than could halt questions 
others adoption or regarding 

result in certain liability and 
groups being risk allocation 
blamed more in healthcare 
often than others provision. 

However, all 
groups in 
society may 
not be given 
an equal say in 
this process 

Table 3:  summary of the epistemic, normative and overarching ethical concerns associated with AI-Health at the five different LoAs as 
identified by the literature review 

 
 
4.The Need for an Ethically-Mindful and Proportionate Approach  

The literature surveyed in this review clearly indicates the need for an agreed pro-ethical blueprint for 

AI-Health that considers the epistemic, normative and traceability ethical concerns at the five different 

LoAs. Protecting people from the harms of AI-Health goes beyond protecting data collection and 

applying a valid model. Normative frameworks need to contemplate the complexities of the human 

interactions where these technologies will be introduced and their emotional impacts (Luxton, 2014; 

Riek, 2016).  

Importantantly, an adequate normative framework should deal with the key question related 

to liability allocation in cases of medical error. Much of the risk of handling data and algorithms stems 

from professionals not adopting measures to protect privacy and support cybersecurity. The solution 

of risk management will come not only from drawing the boundaries of responsibility, but also 

 18 



promoting capacitation, understanding and interfaces for handling AI tools. For one, promoting 

doctors’ and patients’ understanding and control over how AI-Health produces predictions or 

recommendations that are used in treatment plans, and access to and protection of patient data (Ngiam 

& Khor, 2019). Also, there needs to be control over how the interface and design of AI-Health 

products influences HCP-patient-artificial-agent interactions (Cohen et al., 2014). Finally, a 

certification for professionals seeking to use AI-Health tools is also necessary for the adequate 

implementation and use of AI (Kluge, Lacroix, & Ruotsalainen, 2018).  

To tackle these challenges, healthcare systems will need to update outdated governance 

mechanisms (policies, standards and regulations). These can be replaced with both hard and soft 

mechanisms, meaning what ought to be done and what may be done based on the existing moral 

obligations (Floridi, 2018), that balance the need to protect individuals from harm, whilst still 

supporting innovation that can deliver genuine system and patient benefit (Morley & Joshi, 2019). In 

short, healthcare systems should not be overly cautious about the adoption of AI-Health solutions, 

but should be mindful of the potential ethical impacts (Floridi, 2019a) so that proportionate 

governance models can be developed (Sethi & Laurie, 2013) which can, in turn, ensure that those 

responsible for ensuring that healthcare systems are held accountable for the delivery of high-quality 

equitable and safe care, can continue to be so.  

What these regulations, standards and policies should cover and how they should be 

developed remain open questions (Floridi, 2017b) which will likely be ‘solved’ multiple times over by 

different healthcare systems operating in different settings. However, in order to lend a more 

systematic approach to addressing these outstanding questions, enabling greater coherence and speed 

in addressing these challenges, in Table 4 below we have assembled a list of essential cross-cutting 

considerations that emerge from our literature review. The table indicates from which aspect of our 

systematic review (ethical concern x LoA, corresponding to a cell in Table 3) each consideration is 

drawn.  

  

 19 



 

Consideration Key supporting literature Relevant aspects (ascending LoA6) 
The professional skills required of the (Kluge et al., 2018) Epistemic (A, B, C, F) 
healthcare workforce, including information Normative (B, C, D, E) 
governance skills Overarching (A, C) 
 
Which tasks should be delegated to AI-Health (Di Nucci, 2019) Epistemic (A, B, C, D, F) 
solutions Normative (B, C, D, F) 

Overarching (A, C, D) 
 

What evidence is needed to ‘prove’ clinical (Greaves et al., 2018) Epistemic (A, B, C, E, F) 
effectiveness of an AI-Health solution Normative (E) 

Overarching (A, C, D, F) 
 

Mechanisms for reporting or seeking redress (Schönberger, 2019) Epistemic (A, C, E, F) 
for AI-Health associated harms Normative (A, C, E, F) 

Overarching (A, C, D) 
 

Mechanisms for the inclusion of all relevant (Aitken et al., 2019) Epistemic (C, E, F) 
stakeholder views in the development of AI- Overarching (A, C, D, F) 
Health systems  
 
Explainability of specific AI-Health decisions (Watson et al., 2019) Epistemic (A, C) 

Normative (A, C, E) 
Overarching (A, D) 
 

Reliability, replicability and safety of AI- (Challen et al., 2019) Epistemic (A, C, F) 
Health solutions Normative (C, E, F) 

Overarching (A, C, D) 
 

Transparency over how algorithmic tools are (Effy Vayena et al., 2015) Epistemic (A, B, C, D, E, F) 
integrated into the healthcare workflow, how Normative (A, B, D, F) 
it shapes decisions, and how it affects process Overarching (A, D, F) 
optimization within medical services 
 
How traditional and non-traditional sources of (e.g. Maher et al., 2019; Ploug & Holm, Epistemic (A, C, D, E, F) 
health data can be incorporated into AI- 2016; Richardson, Milam, & Chrysler, Normative (A, C, D, E, F) 
Health decision making, how it can be 2015; Townend, 2018) Overarching (A, C, D, E) 
appropriately protected and how it can be 
harmonised 
 
How bioethical concepts (beneficence, non- (Mittelstadt, 2019) Epistemic (B, F) 
maleficence, autonomy and justice Normative (A, C, D, F) 
(Beauchamp & Childress, 2013) are challenged Overarching (A, F) 
by AI-Health  
 
How concepts such as fairness, accountability (Rosenfeld et al., 2019) Epistemic (C, D, E, F) 
and transparency can be maintained at scale Normative (D, E, F) 
(Morley & Floridi, 2019b) so that, for Overarching (F) 
example, the output of algorithmic diagnostics  
does not result in economic benefits to 
specific drug producers or technology 
companies 
 

Table 4: the 11 key considerations for policymakers that arose from the literature review.  

 

Awareness of the need to consider these questions is increasing, and efforts are being made at both a 

national and international level to adapt existing regulations so that they remain fit for purpose (The 

 
6 Denoted by an increasing Level of Analysis: Individual (A), Interpersonal (B), Group (C), Institutional (D), 
Sectoral (E) and Societal (F). 

 20 



Lancet Digital Health, 2019). The American Food and Drug Administration (FDA) is now planning 

on regulating Software as a Medical Devices (SaMD) (Food and Drug Administration (FDA), 2019) 

and in both the EU and the UK Regulation 2017/745 on medical devices comes into effect in April 

2020 and significantly increases the range of software and non-medical products that will need to be 

classed (and assessed) as medical devices. Additionally, the UK has published its Code of Conduct for 

data-driven health and care technologies, standards for evidence of clinical effectiveness for digital 

health technologies (Greaves et al., 2018) – a digital assessment questionnaire standards for apps – and 

is currently developing a ‘regulation as a service’ model to ensure that there are appropriate regulatory 

checks at all stages of the AI development cycle (Morley & Joshi, 2019). The World Health 

Organisation has a number of projects under way to develop guidance for member states (Aicardi et 

al., 2016) (World Health Organisation, 2019). In China, several norms7 provide specific and detailed 

instructions to ensure health data security and confidentiality (Wang, 2019) to ensure that health and 

medical big data sets can be used as a national resource to develop algorithms (Zhang et al., 2018) for 

the improvement of public health (Li, Li, Jiang, & Lan, 2019).  

These are all steps in the right direction, however, their development is progressing slowly 

(which is why the relevant literature is unlikely to reflect all current developments) and almost all focus 

solely on interventions positioning themselves as being health-related in the medical sense, not in the 

wider, wellbeing sense, e.g., healthy exercise, diet, sleeping habits). They will not necessarily mitigate 

risks are associated with the expanding wellness industry, which provides algorithmic tools that 

potentially enable people to bypass formal and well-regulated healthcare systems entirely by accessing 

technology directly, either by using a wearable device or consulting online databases (Burr et al., 2019). 

Similarly, although some technical solutions have been put forward for mitigating issues with data bias 

(Gebru et al., 2018; Holland, Hosny, Newman, Joseph, & Chmielinski, 2018) and data quality  (Dai, 

Yoshigoe, & Parsley, 2018) and ensuring social inclusion in decision making (Balthazar et al., 2018; 

Friedman, Hendry, & Borning, 2017; Rahwan, 2018), these remain relatively untested. Unless a 

competitive advantage of taking such pro-ethical steps becomes clear without these approaches being 

made mandatory, it is unlikely that they will have a significant impact on the ethical impacts of AI-

Health in the near future. As a result, there is still little control over the procedures followed and quality 

 
7 Article 6 of the Regulations on the Management of Medical Records of Medical Institutions, Article 8 of the 
Management Regulations on Application of Electronic Medical Records, Article 6 of the Measures for the 
Management of Health Information, the Cybersecurity Law of the People’s Republic of China, and the new 
Personal Information Security Specification. 

 21 



control mechanisms (Cohen et al., 2014) involved in the development, deployment and use of AI-

Health.  

  As these comparatively easier to tackle problems do not yet have adequate solutions, it is 

unsurprising that the bigger issues regarding the protection of equality of care (Powell & Deetjen, 2019, 

fair distribution of benefits (Balthazar et al., 2018) (Kohli & Geis, 2018) and the protection and 

promotion of societal values (Mahomed, 2018) have barely even been considered. Given that 

healthcare systems in many ways act as the core of modern societies this is concerning. If mistakes are 

made too early in the adoption and implementation of AI in healthcare, the fall-out could be significant 

enough to undermine public trust, resulting in significant opportunity costs, and potentially 

encouraging individuals to seek their healthcare from outside of the formal systems where they may 

be presented with even greater risks. A coherent approach is needed and urgently, hopefully this 

systematic overview of the issues to be considered can help speed up its development.  

 
 
5. Conclusion 
 
This thematic literature review has sought to map out the ethical issues around the incorporation of 

data-driven AI technologies into healthcare provision and public health systems. In order to make this 

overview more useful, the relevant topics have been organised into themes and five different levels of 

abstraction (LoAs) have been highlighted. The hope is that by encouraging a discussion of the ethical 

implications of AI-Health at individual, interpersonal, group, institutional and societal LoAs, 

policymakers and regulators will be able to segment a large and complex conversation into tractable 

debates around specific issues, stakeholders, and solutions. This is important, as Topol (2019) states 

‘there cannot be exceptionalism for AI in medicine,’ especially not when there is potentially so much 

to gain (Miotto, Wang, Wang, Jiang, & Dudley, 2018).  

With this in mind, the review has covered a wide range of topics while also venturing into the 

specificity of certain fields. This approach has made it to develop a fuller and more nuanced 

understanding of the ethical concerns related to the introduction of AI into healthcare systems than 

has been previously seen in the literature. Inevitably, there are limitations to this approach, which are 

specified in the appendix, detailing our methodology and pointing towards opportunities for further 

research. 

In this article, we hope to have provided a sufficiently comprehensive, detailed, and systematic 

analysis of the current debates on ethical issues related to the introduction of AI into healthcare 

systems. The aim is to help policymakers and legislators develop evidence-based and proportionate 

 22 



policy and regulatory interventions. In particular, we hope to encourage the development of a system 

of transparent and distributed responsibility, where all those involved in the clinical algorithm supply 

chain can be held proportionately and appropriately accountable for the safety of the patient at the 

end, not just the HCP. It is only by ensuring such a system is developed that policymakers and 

legislators can be confident that the inherent risks we have described are appropriately mitigated (as 

far as possible) and only once this is the case will the medical community at large feel willing and able 

to adopt AI technologies.  

 

  

 23 



 
Appendix – Methodology 
 
The data collection for this research was divided into three stages as outlined in the below schematic. 
 

 
This process resulted in approximately 147 papers suitable for analysis and inclusion in the 

initial review. Subsequent relevant papers that met the criteria were added at a later date during the 
writing up of the results.  

This literature review also included accessory readings and case studies that were 
encountered during the research process. This includes bibliography obtained from the references of 
the papers analysed, and case studies identified in the readings (e.g. the Deep Mind case study). It is 
our belief that these exploratory readings enrich our systematic approach by developing on 
interesting findings and topics identified throughout our investigation. 

It is important to note that the selection of articles and policy documents was restricted to 
those written in English. This means that some ethical issues will have been overlooked (e.g. those in 
Spanish-speaking countries or in China). Second, academic literature, much like regulation, tends to 
struggle to keep pace with technological development. This literature review did not seek to identify 

 24 



ethical issues associated with specific use cases of AI first-hand, for example, by reviewing recently 
published studies available on pre-print servers such as arXiv, but instead focused on providing an 
overview of the ethical issues already identified. As a result, there may well be ethical concerns that 
are associated with more emergent use cases of AI for healthcare that we have not identified as they 
have not yet been discussed in formal peer-reviewed publications.  

To overcome these limitations, further research could seek to expand the literature review 
by including a wider range of search queries, and by taking a case-study approach to analysing the 
ethical issues of specific practices and then aggregating these. This could be complemented by a 
comprehensive review of the policies, standards and regulations in development in different 
healthcare systems across the globe to assess the extent to which these are likely to be effective at 
mitigating these ethical concerns.  
 
 

Technology Ethics/Concerns Field  
Algorithm* Fairness Health* 
Artificial Intelligence / AI Moral* 
Machine Learning Ethics 

Governance 

Table 5: Showing terms refined from Mittelstadt et al (2016) and selected to focus the literature 
search on publications focusing specifically on the ethics of AI for health. It is important to note that 
the search parameters were not exactly the same in all databases. Adaptation was necessary since not 
all databases operated with the same syntax or accepted the same number of search queries. As a 
result, the arrangement of Boolean operators and a search parameter were adapted to ensure that all 
possible combinations were covered. 
 
 

Database Search Query Results Titles Titles Downloaded 
Selected 

SCOPUS ethic* AND algorithm* AND health* 596 39 19 
 (ethic* AND ( "Artificial 239 37 15 

Intelligence"  OR  ai )  AND  health* )  
 ( moral*  AND  ( "Artificial 46 2 0 

Intelligence"  OR  ai )  AND  health* )  
 ( fair*  AND  ( "Artificial 122 6 3 

Intelligence"  OR  ai )  AND  health* )  
 (moral* OR ethic*) AND "machine learning" AND 91 14 9 

health* 
  ( fair* )  AND  "machine learning"  AND  health* )  70 5 3 

Web of Science ((fair* OR moral* OR ethic*) AND ("machine learning" 668 45 26 
OR "Artificial Intelligence" OR "AI" OR algorithm*) 
AND health*) 

Philpapers '"machine learning" AND health* 3 1 1 
 Artificial Intelligence AND health* AND ethic* 1000+ - - 
 algorithm* AND health* AND ethic* 5 0 0 
 ethics AND "artificial intelligence" AND health 3 2 2 
 AI or Artificial Intelligence or Fair AND ethic or moral or 9 0 0 

health AND health8 
Google Scholar ethics algorithms health 15,400 18 18 
 ethics of machine learning in health 21,300 11 10 
 ETHICS & HEALTH  716,000 2 1 

and at least one of: 
 algorithm OR machine learning OR artificial intelligence 
OR AI 
 

 
 

 25 



 ETHICS & HEALTH  105,000 2 2 
and at least one of: 
algorithm OR AI 
 

 MORAL & HEALTH  26,900 2 1 
and at least one of: 
algorithm OR AI 

 FAIR & HEALTH 38,000 0 0 
And at least one of: 
 algorithm OR AI 

PubMed ETHICS & ARTIFICIAL INTELLIGENCE OR 34, 193 37 37 
MACHINE LEARNING  

 
Table 6: Showing the final results from all searches. It is important to note that multiple search 
queries were made to cover all the combinations and the numbers in the table thus represent the sum 
of results, titles evaluated and downloaded (not all found files were accessible for download). It is 
also important to note that only the first 500 most relevant results from Google Scholar were 
reviewed and anything written before 2014 was excluded to make the number of results more 
manageable. 
 
 
 

 
Funding 
Taddeo and Floridi’s work was partially supported by Privacy and Trust Stream – Social lead of the 

PETRAS Internet of Things research hub – PETRAS is funded by the UK Engineering and Physical 

Sciences Research Council (EPSRC), grant agreement no. EP/N023013/1. Caio’s, Taddeo’s and 

Floridi’s work was also partially supported by a Microsoft grant and a Google grant.  

 

 

 

 

 

 

 

 

 

  

 26 



References 

Abdul, A., Vermeulen, J., Wang, D., Lim, B. Y., & Kankanhalli, M. (2018). Trends and Trajectories for 
Explainable, Accountable and Intelligible Systems: An HCI Research Agenda. Proceedings of the 2018 
CHI Conference on Human Factors in Computing Systems - CHI ’18, 1–18. 
https://doi.org/10.1145/3173574.3174156 

Aicardi, C., Del Savio, L., Dove, E. S., Lucivero, F., Tempini, N., & Prainsack, B. (2016). Emerging ethical 
issues regarding digital health data. On the World Medical Association Draft Declaration on Ethical 
Considerations Regarding Health Databases and Biobanks. Croatian Medical Journal, 57(2), 207–213. 
https://doi.org/10.3325/cmj.2016.57.207 

Aitken, M., Tully, M. P., Porteous, C., Denegri, S., Cunningham-Burley, S., Banner, N., … Willison, D. J. 
(2019). Consensus Statement on Public Involvement and Engagement with Data-Intensive Health 
Research. International Journal of Population Data Science, 4(1). https://doi.org/10.23889/ijpds.v4i1.586 

Álvarez-Machancoses, Ó., & Fernández-Martínez, J. L. (2019). Using artificial intelligence methods to speed up 
drug discovery. Expert Opinion on Drug Discovery, 14(8), 769–777. 
https://doi.org/10.1080/17460441.2019.1621284 

Andorno, R. (2004). The right not to know: An autonomy based approach. Journal of Medical Ethics, 30(5), 435–
439. https://doi.org/10.1136/jme.2002.001578 

Arieno, A., Chan, A., & Destounis, S. V. (2019). A review of the role of augmented intelligence in breast 
imaging: From automated breast density assessment to risk stratification. American Journal of 
Roentgenology, 212(2), 259–270. https://doi.org/10.2214/AJR.18.20391 

Balthazar, P., Harri, P., Prater, A., & Safdar, N. M. (2018). Protecting Your Patients’ Interests in the Era of Big 
Data, Artificial Intelligence, and Predictive Analytics. Journal of the American College of Radiology, 15(3), 
580–586. https://doi.org/10.1016/j.jacr.2017.11.035 

Barakat, N., Bradley, A. P., & Barakat, M. N. H. (2010). Intelligible Support Vector Machines for Diagnosis of 
Diabetes Mellitus. IEEE Transactions on Information Technology in Biomedicine, 14(4), 1114–1120. 
https://doi.org/10.1109/TITB.2009.2039485 

Barras, C. (2019). Mental health apps lean on bots and unlicensed therapists. Nature Medicine. 
https://doi.org/10.1038/d41591-019-00009-6 

Bartoletti, I. (2019). AI in healthcare: Ethical and privacy challenges. Lecture Notes in Computer Science (Including 
Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11526 LNAI, 7–10. 
https://doi.org/10.1007/978-3-030-21642-9_2 

Barton, C., Chettipally, U., Zhou, Y., Jiang, Z., Lynn-Palevsky, A., Le, S., … Das, R. (2019). Evaluation of a 
machine learning algorithm for up to 48-hour advance prediction of sepsis using six vital signs. 
Computers in Biology and Medicine, 109, 79–84. https://doi.org/10.1016/j.compbiomed.2019.04.027 

Beauchamp, T. L., & Childress, J. F. (2013). Principles of biomedical ethics (7th ed). New York: Oxford University 
Press. 

Beer, D. (2017). The social power of algorithms. Information, Communication & Society, 20(1), 1–13. 
https://doi.org/10.1080/1369118X.2016.1216147 

Brown, M. P. S., Grundy, W. N., Lin, D., Cristianini, N., Sugnet, C. W., Furey, T. S., … Haussler, D. (2000). 
Knowledge-based analysis of microarray gene expression data by using support vector machines. 
Proceedings of the National Academy of Sciences, 97(1), 262–267. https://doi.org/10.1073/pnas.97.1.262 

Burns, T. (2015). Our necessary shadow: The nature and meaning of psychiatry. Place of publication not identified: 
Pegasus Books. 

Burr, C., Mariarosaria, T., & Floridi, L. (2019, February 1). The Ethics of Digital Well-Being: A Thematic 
Review. Retrieved 19 April 2019, from SSRN website: 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3338441&download=yes 

Cabitza, F., Rasoini, R., & Gensini, G. F. (2017). Unintended Consequences of Machine Learning in Medicine. 
JAMA, 318(6), 517. https://doi.org/10.1001/jama.2017.7797 

Car, J., Sheikh, A., Wicks, P., & Williams, M. S. (2019). Beyond the hype of big data and artificial intelligence: 
Building foundations for knowledge and wisdom. BMC Medicine, 17(1). 
https://doi.org/10.1186/s12916-019-1382-x 

 27 



Cath, C., Wachter, S., Mittelstadt, B., Taddeo, M., & Floridi, L. (2017). Artificial Intelligence and the ‘Good 
Society’: The US, EU, and UK approach. Science and Engineering Ethics. 
https://doi.org/10.1007/s11948-017-9901-7 

Celi, L. A., Davidzon, G., Johnson, A. E. W., Komorowski, M., Marshall, D. C., Nair, S. S., … Stone, D. J. 
(2016). Bridging the health data divide. Journal of Medical Internet Research, 18(12). 
https://doi.org/10.2196/jmir.6400 

Challen, R., Denny, J., Pitt, M., Gompels, L., Edwards, T., & Tsaneva-Atanasova, K. (2019). Artificial 
intelligence, bias and clinical safety. BMJ Quality & Safety, 28(3), 231–237. 
https://doi.org/10.1136/bmjqs-2018-008370 

Char, D. S., Shah, N. H., & Magnus, D. (2018). Implementing Machine Learning in Health Care—Addressing 
Ethical Challenges. The New England Journal of Medicine, 378(11), 981–983. 
https://doi.org/10.1056/NEJMp1714229 

Cheney-Lippold, J. (2017). We are data: Algorithms and the making of our digital selves. New York: New York 
University Press. 

Chiauzzi, E., & Wicks, P. (2019). Digital Trespass: Ethical and Terms-of-Use Violations by Researchers 
Accessing Data From an Online Patient Community. Journal of Medical Internet Research, 21(2), e11985. 
https://doi.org/10.2196/11985 

Chin-Yee, B., & Upshur, R. (2019). Three Problems with Big Data and Artificial Intelligence in Medicine. 
Perspectives in Biology and Medicine, 62(2), 237–256. https://doi.org/10.1353/pbm.2019.0012 

Coeckelberg, M. (2014). Good Healthcare Is in the “How”: The Quality of Care, the Role of Machines, and the 
Need for New Skills. In Machine medical ethics (Vol. 74). New York: Springer. 

Cohen, I. G., Amarasingham, R., Shah, A., Xie, B., & Lo, B. (2014). The Legal And Ethical Concerns That 
Arise From Using Complex Predictive Analytics In Health Care. Health Affairs, 33(7), 1139–1147. 
https://doi.org/10.1377/hlthaff.2014.0048 

Cookson, C. (2018, September 6). Artificial intelligence faces public backlash, warns scientist. Financial Times. 
Retrieved from https://www.ft.com/content/0b301152-b0f8-11e8-99ca-68cf89602132 

Cowie, J., Calveley, E., Bowers, G., & Bowers, J. (2018). Evaluation of a digital consultation and self-care 
advice tool in primary care: A multi-methods study. International Journal of Environmental Research and 
Public Health, 15(5). https://doi.org/10.3390/ijerph15050896 

Dai, W., Yoshigoe, K., & Parsley, W. (2018). Improving Data Quality through Deep Learning and Statistical 
Models. ArXiv:1810.07132 [Cs], 558, 515–522. https://doi.org/10.1007/978-3-319-54978-1_66 

Danis, M., & Solomon, M. (2013). Providers, Payers, The Community, And Patients Are All Obliged To Get 
Patient Activation And Engagement Ethically Right. Health Affairs, 32(2), 401–407. 
https://doi.org/10.1377/hlthaff.2012.1081 

De Fauw, J., Ledsam, J. R., Romera-Paredes, B., Nikolov, S., Tomasev, N., Blackwell, S., … Ronneberger, O. 
(2018). Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature Medicine, 
24(9), 1342–1350. https://doi.org/10.1038/s41591-018-0107-6 

De Langavant, L. C., Bayen, E., & Yaffe, K. (2018). Unsupervised machine learning to identify high likelihood 
of dementia in population-based surveys: Development and validation study. Journal of Medical Internet 
Research, 20(7). https://doi.org/10.2196/10493 

Deng, X., Luo, Y., & Wang, C. (2018). Analysis of Risk Factors for Cervical Cancer Based on Machine 
Learning Methods. 2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems 
(CCIS), 631–635. https://doi.org/10.1109/CCIS.2018.8691126 

Department of Health and Social Care. (n.d.-a). Annual Report of the Chief Medical Office 2018: Health 2040—Better 
Health Within Reach. Retrieved from 
https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file
/767549/Annual_report_of_the_Chief_Medical_Officer_2018_-_health_2040_-
_better_health_within_reach.pdf 

Department of Health and Social Care. (n.d.-b). Health Secretary announces £250 million investment in 
artificial intelligence [Gov.uk]. Retrieved 8 August 2019, from 
https://www.gov.uk/government/news/health-secretary-announces-250-million-investment-in-
artificial-intelligence 

Di Nucci, E. (2019). Should we be afraid of medical AI? Journal of Medical Ethics. 
https://doi.org/10.1136/medethics-2018-105281 

 28 



Ding, Y., Tang, J., & Guo, F. (2019). Identification of drug-side effect association via multiple information 
integration with centered kernel alignment. Neurocomputing, 325, 211–224. 
https://doi.org/10.1016/j.neucom.2018.10.028 

Dong, R., Yang, X., Zhang, X., Gao, P., Ke, A., Sun, H., … Shi, G. (2019). Predicting overall survival of 
patients with hepatocellular carcinoma using a three‐category method based on DNA methylation 
and machine learning. Journal of Cellular and Molecular Medicine, 23(5), 3369–3374. 
https://doi.org/10.1111/jcmm.14231 

Dudley, J. T., Listgarten, J., Stegle, O., Brenner, S. E., & Parts, L. (2015). Personalized medicine: From genotypes, 
molecular phenotypes and the quantified self, towards improved medicine. 342–346. 

DuFault, B. L., & Schouten, J. W. (2018). Self-quantification and the datapreneurial consumer identity. 
Consumption Markets & Culture, 1–27. https://doi.org/10.1080/10253866.2018.1519489 

Emanuel, E. J., & Wachter, R. M. (2019). Artificial Intelligence in Health Care: Will the Value Match the Hype? 
JAMA, 321(23), 2281–2282. https://doi.org/10.1001/jama.2019.4914 

Faden, R. R., Kass, N. E., Goodman, S. N., Pronovost, P., Tunis, S., & Beauchamp, T. L. (2013). An Ethics 
Framework for a Learning Health Care System: A Departure from Traditional Research Ethics and Clinical 
Ethics. Hastings Center Report, 43(s1), S16–S27. https://doi.org/10.1002/hast.134 

Ferretti, A., Ronchi, E., & Vayena, E. (2019). From principles to practice: Benchmarking government guidance 
on health apps. The Lancet Digital Health, 1(2), e55–e57. https://doi.org/10.1016/S2589-
7500(19)30027-5 

Fiske, A., Henningsen, P., & Buyx, A. (2019). Your Robot Therapist Will See You Now: Ethical Implications 
of Embodied Artificial Intelligence in Psychiatry, Psychology, and Psychotherapy. Journal of Medical 
Internet Research, 21(5), e13216. https://doi.org/10.2196/13216 

Fleming, N. (2018). How artificial intelligence is changing drug discovery. Nature, 557(7707), S55–S57. 
https://doi.org/10.1038/d41586-018-05267-x 

Floridi, L., Luetge, C., Pagallo, U., Schafer, B., Valcke, P., Vayena, E., … Kalra, D. (2018). Key Ethical 
Challenges in the European Medical Information Framework. Minds and Machines, 1–17. 
https://doi.org/10.1007/s11023-018-9467-4 

Floridi, Luciano. (2008). The Method of Levels of Abstraction. Minds and Machines, 18(3), 303–329. 
https://doi.org/10.1007/s11023-008-9113-7 

Floridi, Luciano. (2013). Distributed Morality in an Information Society. Science and Engineering Ethics, 19(3), 
727–743. https://doi.org/10.1007/s11948-012-9413-4 

Floridi, Luciano. (2014). The 4th revolution: How the infosphere is reshaping human reality. Oxford: Oxford Univ. 
Press. 

Floridi, Luciano. (2016). Faultless responsibility: On the nature and allocation of moral responsibility for 
distributed moral actions. Philosophical Transactions of the Royal Society A: Mathematical, Physical and 
Engineering Sciences, 374(2083), 20160112. https://doi.org/10.1098/rsta.2016.0112 

Floridi, Luciano. (2017a). Digital’s Cleaving Power and Its Consequences. Philosophy & Technology, 30(2), 123–
129. https://doi.org/10.1007/s13347-017-0259-1 

Floridi, Luciano. (2017b). The Logic of Design as a Conceptual Logic of Information. Minds and Machines, 
27(3), 495–519. https://doi.org/10.1007/s11023-017-9438-1 

Floridi, Luciano. (2018). Soft ethics, the governance of the digital and the General Data Protection Regulation. 
Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences, 376(2133). 
https://doi.org/10.1098/rsta.2018.0081 

Floridi, Luciano. (2019a). AI opportunities for healthcare must not be wasted. Health Management, 19. 
Floridi, Luciano. (2019b). What the Near Future of Artificial Intelligence Could Be. Philosophy & Technology, 

32(1), 1–15. https://doi.org/10.1007/s13347-019-00345-y 
Floridi, Luciano, & Taddeo, M. (2016). What is data ethics? Philosophical Transactions of the Royal Society A: 

Mathematical, Physical and Engineering Sciences, 374(2083), 20160360. 
https://doi.org/10.1098/rsta.2016.0360 

Friedman, B., Hendry, D. G., & Borning, A. (2017). A Survey of Value Sensitive Design Methods. Foundations 
and Trends® in Human–Computer Interaction, 11(2), 63–125. https://doi.org/10.1561/1100000015 

Garattini, C., Raffle, J., Aisyah, D. N., Sartain, F., & Kozlakidis, Z. (2019). Big Data Analytics, Infectious 
Diseases and Associated Ethical Impacts. Philosophy & Technology, 32(1), 69–85. 
https://doi.org/10.1007/s13347-017-0278-y 

 29 



Garcia, J., Romero, N., Keyson, D., & Havinga, P. (2014). Reflective healthcare systems: Mirco-cylce of self-
reflection to empower users. Interaction Design and Architecture(s), 23(1), 173–190. 

Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daumeé III, H., & Crawford, K. 
(2018). Datasheets for Datasets. ArXiv:1803.09010 [Cs]. Retrieved from 
http://arxiv.org/abs/1803.09010 

Gillespie, T., Boczkowski, P. J., & Foot, K. A. (2014). Media technologies: Essays on communication, materiality, and 
society. Cambridge, Massachusetts: The MIT Press. 

Greaves, F., Joshi, I., Campbell, M., Roberts, S., Patel, N., & Powell, J. (2018). What is an appropriate level of 
evidence for a digital health intervention? The Lancet, 392(10165), 2665–2667. 
https://doi.org/10.1016/S0140-6736(18)33129-5 

Hailu, R. (2019). Fitbits and other wearables may not accurately track heart rates in people of color. STAT. 
Retrieved from https://www.statnews.com/2019/07/24/fitbit-accuracy-dark-skin/ 

Harerimana, G., Jang, B., Kim, J. W., & Park, H. K. (2018). Health Big Data Analytics: A Technology Survey. 
IEEE Access, 6, 65661–65678. https://doi.org/10.1109/ACCESS.2018.2878254 

Hay, S. I., George, D. B., Moyes, C. L., & Brownstein, J. S. (2013). Big data opportunities for global infectious disease 
surveillance. 

He, J., Baxter, S. L., Xu, J., Xu, J., Zhou, X., & Zhang, K. (2019). The practical implementation of artificial 
intelligence technologies in medicine. Nature Medicine, 25(1), 30–36. https://doi.org/10.1038/s41591-
018-0307-0 

Henson, P., David, G., Albright, K., & Torous, J. (2019). Deriving a practical framework for the evaluation of 
health apps. The Lancet Digital Health, 1(2), e52–e54. https://doi.org/10.1016/S2589-7500(19)30013-5 

Hoffman, L., Benedetto, E., Huang, H., Grossman, E., Kaluma, D., Mann, Z., & Torous, J. (2019). 
Augmenting Mental Health in Primary Care: A 1-Year Study of Deploying Smartphone Apps in a 
Multi-site Primary Care/Behavioral Health Integration Program. Frontiers in Psychiatry, 10, 94. 
https://doi.org/10.3389/fpsyt.2019.00094 

Holland, S., Hosny, A., Newman, S., Joseph, J., & Chmielinski, K. (2018). The Dataset Nutrition Label: A 
Framework To Drive Higher Data Quality Standards. ArXiv:1805.03677 [Cs]. Retrieved from 
http://arxiv.org/abs/1805.03677 

Holzinger, A., Haibe-Kains, B., & Jurisica, I. (2019). Why imaging data alone is not enough: AI-based 
integration of imaging, omics, and clinical data. European Journal of Nuclear Medicine and Molecular 
Imaging. https://doi.org/10.1007/s00259-019-04382-9 

Information Commissioner. (2018, June 6). Royal Free—Google DeepMind trial failed to comply with data 
protection law. Retrieved 15 September 2019, from https://ico.org.uk/about-the-ico/news-and-
events/news-and-blogs/2017/07/royal-free-google-deepmind-trial-failed-to-comply-with-data-
protection-law/ 

Information Commissioner. (2019, June 3). Project ExplAIn interim report. Retrieved 15 September 2019, 
from https://ico.org.uk/about-the-ico/research-and-reports/project-explain-interim-report/ 

Jiang, W., & Yin, Z. (2015). Human Activity Recognition Using Wearable Sensors by Deep Convolutional 
Neural Networks. Proceedings of the 23rd ACM International Conference on Multimedia - MM ’15, 1307–
1310. https://doi.org/10.1145/2733373.2806333 

Juengst, E., McGowan, M. L., Fishman, J. R., & Settersten, R. A. (2016). From “Personalized” to “Precision” 
Medicine: The Ethical and Social Implications of Rhetorical Reform in Genomic Medicine. Hastings 
Center Report, 46(5), 21–33. https://doi.org/10.1002/hast.614 

Kalkman, S., Mostert, M., Gerlinger, C., Van Delden, J. J. M., & Van Thiel, G. J. M. W. (2019). Responsible 
data sharing in international health research: A systematic review of principles and norms. BMC 
Medical Ethics, 20(1). https://doi.org/10.1186/s12910-019-0359-9 

Kalmady, S. V., Greiner, R., Agrawal, R., Shivakumar, V., Narayanaswamy, J. C., Brown, M. R. G., … 
Venkatasubramanian, G. (2019). Towards artificial intelligence in mental health by improving 
schizophrenia prediction with multiple brain parcellation ensemble-learning. Npj Schizophrenia, 5(1), 2. 
https://doi.org/10.1038/s41537-018-0070-8 

Kim, H., & Xie, B. (2017). Health literacy in the eHealth era: A systematic review of the literature. Patient 
Education and Counseling, 100(6), 1073–1082. 

 30 



King, D., Karthikesalingam, A., Hughes, C., Montgomery, H., Raine, R., Rees, G., & On behalf of the 
DeepMind Health Team. (2018). Letter in response to Google DeepMind and healthcare in an age of 
algorithms. Health and Technology, 8(1), 11–13. https://doi.org/10.1007/s12553-018-0228-4 

Kleinpeter, E. (2017). Four Ethical Issues of “E-Health”. IRBM, 38(5), 245–249. 
https://doi.org/10.1016/j.irbm.2017.07.006 

Kluge, E.-H., Lacroix, P., & Ruotsalainen, P. (2018). Ethics Certification of Health Information Professionals. 
Yearbook of Medical Informatics, 27(01), 037–040. https://doi.org/10.1055/s-0038-1641196 

Kohli, M., & Geis, R. (2018). Ethics, Artificial Intelligence, and Radiology. Journal of the American College of 
Radiology, 15(9), 1317–1319. https://doi.org/10.1016/j.jacr.2018.05.020 

Kourou, K., Exarchos, T. P., Exarchos, K. P., Karamouzis, M. V., & Fotiadis, D. I. (2015). Machine learning 
applications in cancer prognosis and prediction. Computational and Structural Biotechnology Journal, 13, 8–
17. https://doi.org/10.1016/j.csbj.2014.11.005 

Krutzinna, J., Taddeo, M., & Floridi, L. (2018). Enabling Posthumous Medical Data Donation: An Appeal for 
the Ethical Utilisation of Personal Health Data. Science and Engineering Ethics. 
https://doi.org/10.1007/s11948-018-0067-8 

Kuek, A., & Hakkennes, S. (2019). Healthcare staff digital literacy levels and their attitudes towards 
information systems. Health Informatics Journal, 146045821983961. 
https://doi.org/10.1177/1460458219839613 

Kunapuli, G., Varghese, B. A., Ganapathy, P., Desai, B., Cen, S., Aron, M., … Duddalwar, V. (2018). A 
Decision-Support Tool for Renal Mass Classification. Journal of Digital Imaging, 31(6), 929–939. 
https://doi.org/10.1007/s10278-018-0100-0 

Kuo, W.-J., Chang, R.-F., Chen, D.-R., & Lee, C. C. (2001). Data mining with decision trees for diagnosis of 
breast tumor in medical ultrasonic images. Breast Cancer Research and Treatment, 66(1), 51–57. 
https://doi.org/10.1023/A:1010676701382 

Larsen, M. E., Huckvale, K., Nicholas, J., Torous, J., Birrell, L., Li, E., & Reda, B. (2019). Using science to sell 
apps: Evaluation of mental health app store quality claims. Npj Digital Medicine, 2(1), 18. 
https://doi.org/10.1038/s41746-019-0093-1 

Lee, L. M. (2017). Ethics and subsequent use of electronic health record data. Journal of Biomedical Informatics, 71, 
143–146. https://doi.org/10.1016/j.jbi.2017.05.022 

Li, B., Li, J., Jiang, Y., & Lan, X. (2019). Experience and reflection from China’s Xiangya medical big data 
project. Journal of Biomedical Informatics, 93. https://doi.org/10.1016/j.jbi.2019.103149 

Liu, C., Liu, X., Wu, F., Xie, M., Feng, Y., & Hu, C. (2018). Using Artificial Intelligence (Watson for Oncology) 
for Treatment Recommendations Amongst Chinese Patients with Lung Cancer: Feasibility Study. 
Journal of Medical Internet Research, 20(9), e11087. https://doi.org/10.2196/11087 

López-Martínez, F., Núñez-Valdez, E. R., Lorduy Gomez, J., & García-Díaz, V. (2019). A neural network 
approach to predict early neonatal sepsis. Computers & Electrical Engineering, 76, 379–388. 
https://doi.org/10.1016/j.compeleceng.2019.04.015 

Lu, F. S., Hattab, M. W., Clemente, C. L., Biggerstaff, M., & Santillana, M. (2019). Improved state-level 
influenza nowcasting in the United States leveraging Internet-based data and network approaches. 
Nature Communications, 10(1). https://doi.org/10.1038/s41467-018-08082-0 

Lu, H., & Wang, M. (2019). RL4health: Crowdsourcing Reinforcement Learning for Knee Replacement 
Pathway Optimization. ArXiv:1906.01407 [Cs, Stat]. Retrieved from http://arxiv.org/abs/1906.01407 

Luxton, D. D. (2014). Artificial intelligence in psychological practice: Current and future applications and 
implications. Professional Psychology: Research and Practice, 45(5), 332–339. 
https://doi.org/10.1037/a0034559 

Maher, N. A., Senders, J. T., Hulsbergen, A. F. C., Lamba, N., Parker, M., Onnela, J.-P., … Broekman, M. L. 
D. (2019). Passive data collection and use in healthcare: A systematic review of ethical issues. 
International Journal of Medical Informatics, 129, 242–247. https://doi.org/10.1016/j.ijmedinf.2019.06.015 

Mahomed, S. (2018). Healthcare, artificial intelligence and the Fourth Industrial Revolution: Ethical, social and 
legal considerations. South African Journal of Bioethics and Law, 11(2), 93. 
https://doi.org/10.7196/SAJBL.2018.v11i2.00664 

Mann, S. P., Savulescu, J., & Sahakian, B. J. (2016). Facilitating the ethical use of health data for the benefit of 
society: Electronic health records, consent and the duty of easy rescue. Philosophical Transactions of the 

 31 



Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2083). 
https://doi.org/10.1098/rsta.2016.0130 

Marill, M. (2019, May 30). Why Tracking Your Symptoms Can Make You Feel Worse. Wired. Retrieved from 
https://www.wired.com/story/why-tracking-your-symptoms-can-make-you-feel-
worse/?&utm_campaign=the_download.unpaid.engagement&utm_source=hs_email&utm_medium
=email&utm_content=73227565&_hsenc=p2ANqtz-
8UIkyITAj5PaGHIR7rQySwYgVDMJAdHABei6CktNIvvIaayTN3lsHJHhDMSWxcl0rGKAf23ckp
D3uUAEZow-HQyxjsPJVKPfptCzqqpl2XART5QOA&_hsmi=73227565 

Martani, A., Shaw, D., & Elger, B. S. (2019). Stay fit or get bit—Ethical issues in sharing health data with 
insurers’ apps. Swiss Medical Weekly, 149, w20089. https://doi.org/10.4414/smw.2019.20089 

Matthias, A. (2015). Robot Lies in Health Care: When Is Deception Morally Permissible? Kennedy Institute of 
Ethics Journal, 25(2), 169–162. https://doi.org/10.1353/ken.2015.0007 

McDougall, R. J. (2019). Computer knows best? The need for value-flexibility in medical AI. Journal of Medical 
Ethics, 45(3), 156–160. https://doi.org/10.1136/medethics-2018-105118 

McLaughlin, K. (2016). Empowerment: A critique. Retrieved from 
http://public.eblib.com/choice/publicfullrecord.aspx?p=4332655 

Millett, S., & O’Leary, P. (2015). Revisiting consent for health information databanks. Research Ethics, 11(3), 
151–163. https://doi.org/10.1177/1747016115587964 

Millett, Stephan, & O’Leary, P. (2015). Revisiting consent for health information databanks. Research Ethics, 
11(3), 151–163. https://doi.org/10.1177/1747016115587964 

Miotto, R., Wang, F., Wang, S., Jiang, X., & Dudley, J. T. (2018). Deep learning for healthcare: Review, 
opportunities and challenges. Briefings in Bioinformatics, 19(6), 1236–1246. 
https://doi.org/10.1093/bib/bbx044 

Mittelstadt, B. (2019). The Ethics of Biomedical ‘Big Data’ Analytics. Philosophy & Technology, 32(1), 17–21. 
https://doi.org/10.1007/s13347-019-00344-z 

Mittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms: Mapping 
the debate. Big Data & Society, 3(2), 205395171667967. https://doi.org/10.1177/2053951716679679 

Morley, J, & Joshi, I. (2019). Developing effective policy to support Artificial Intelligence in Health and Care. 
Eurohealth, 25(2). https://doi.org/Forthcoming 

Morley, Jessica, & Floridi, L. (In Press). The need for an ethically mindful approach to AI for healthcare. The 
Lancet. 

Morley, Jessica, & Floridi, L. (2019a). Enabling digital health companionship is better than empowerment. The 
Lancet Digital Health, S2589750019300792. https://doi.org/10.1016/S2589-7500(19)30079-2 

Morley, Jessica, & Floridi, L. (2019b). How to design a governable digital health ecosystem. Health Policy and 
Technology. https://doi.org/10.13140/rg.2.2.28320.74244/1 

Morley, Jessica, & Floridi, L. (2019c). The Limits of Empowerment: How to Reframe the Role of mHealth 
Tools in the Healthcare Ecosystem. Science and Engineering Ethics. https://doi.org/10.1007/s11948-
019-00115-1 

Morley, Jessica, Floridi, L., Kinsey, L., & Elhalal, A. (2019). From What to How. An Overview of AI Ethics 
Tools, Methods and Research to Translate Principles into Practices. Science and Engineering Ethics. 
Retrieved from http://arxiv.org/abs/1905.06876 (Pre-Print) 

Mortazavi, B. J., Desai, N., Zhang, J., Coppi, A., Warner, F., Krumholz, H. M., & Negahban, S. (2017). 
Prediction of Adverse Events in Patients Undergoing Major Cardiovascular Procedures. IEEE Journal 
of Biomedical and Health Informatics, 21(6), 1719–1729. https://doi.org/10.1109/JBHI.2017.2675340 

Moscoso, A., Silva-Rodríguez, J., Aldrey, J. M., Cortés, J., Fernández-Ferreiro, A., Gómez-Lado, N., … Aguiar, 
P. (2019). Prediction of Alzheimer’s disease dementia with MRI beyond the short-term: Implications 
for the design of predictive models. NeuroImage: Clinical, 23, 101837. 
https://doi.org/10.1016/j.nicl.2019.101837 

Nag, N., Pandey, V., Oh, H., & Jain, R. (2017). Cybernetic Health. ArXiv:1705.08514 [Cs]. Retrieved from 
http://arxiv.org/abs/1705.08514 

National Institute for Health Research. (2019, June 14). Involving the public in complex questions around 
artificial intelligence research. Retrieved 15 September 2019, from 
https://www.nihr.ac.uk/blog/involving-the-public-in-complex-questions-around-artificial-
intelligence-research/12236 

 32 



Nebeker, C., Harlow, J., Espinoza Giacinto, R., Orozco-Linares, R., Bloss, C. S., & Weibel, N. (2017). Ethical 
and regulatory challenges of research using pervasive sensing and other emerging technologies: IRB 
perspectives. AJOB Empirical Bioethics, 8(4), 266–276. 
https://doi.org/10.1080/23294515.2017.1403980 

Nebeker, C., Torous, J., & Bartlett Ellis, R. J. (2019). Building the case for actionable ethics in digital health 
research supported by artificial intelligence. BMC Medicine, 17(1). https://doi.org/10.1186/s12916-
019-1377-7 

Nelson, A., Herron, D., Rees, G., & Nachev, P. (2019). Predicting scheduled hospital attendance with artificial 
intelligence. Npj Digital Medicine, 2(1), 26. https://doi.org/10.1038/s41746-019-0103-3 

Ngiam, K. Y., & Khor, I. W. (2019). Big data and machine learning algorithms for health-care delivery. The 
Lancet Oncology, 20(5), e262–e273. https://doi.org/10.1016/S1470-2045(19)30149-4 

NHS England. (2019). The NHS Long Term Plan. Retrieved from NHS website: 
https://www.longtermplan.nhs.uk/wp-content/uploads/2019/01/nhs-long-term-plan.pdf 

Nittas, V., Mütsch, M., Ehrler, F., & Puhan, M. A. (2018). Electronic patient-generated health data to facilitate 
prevention and health promotion: A scoping review protocol. BMJ Open, 8(8). 
https://doi.org/10.1136/bmjopen-2017-021245 

O’Doherty, K. C., Christofides, E., Yen, J., Bentzen, H. B., Burke, W., Hallowell, N., … Willison, D. J. (2016). 
If you build it, they will come: Unintended future uses of organised health data collections Donna 
Dickenson, Sandra Soo-Jin Lee, and Michael Morrison. BMC Medical Ethics, 17(1). 
https://doi.org/10.1186/s12910-016-0137-x 

Olimid, A. P., Rogozea, L. M., & Olimid, D.-A. (2018). Ethical approach to the genetic, biometric and health 
data protection and processing in the new EU general data protection regulation (2018). Romanian 
Journal of Morphology and Embryology, 59(2), 631–636. Retrieved from 
https://www.scopus.com/inward/record.uri?eid=2-s2.0-
85052735984&partnerID=40&md5=6b8d7af00f77e58b4439983e09148535 

Owens, J., & Cribb, A. (2019). ‘My Fitbit Thinks I Can Do Better!’ Do Health Promoting Wearable 
Technologies Support Personal Autonomy? Philosophy & Technology, 32(1), 23–38. 
https://doi.org/10.1007/s13347-017-0266-2 

Page, S. A., Manhas, K. P., & Muruve, D. A. (2016). A survey of patient perspectives on the research use of 
health information and biospecimens. BMC Medical Ethics, 17(1). https://doi.org/10.1186/s12910-
016-0130-4 

Panch, T., Mattie, H., & Celi, L. A. (2019). The “inconvenient truth” about AI in healthcare. Npj Digital 
Medicine, 2(1), 77. https://doi.org/10.1038/s41746-019-0155-4 

Parker, L., Halter, V., Karliychuk, T., & Grundy, Q. (2019). How private is your mental health app data? An 
empirical study of mental health app privacy policies and practices. International Journal of Law and 
Psychiatry, 64, 198–204. https://doi.org/10.1016/j.ijlp.2019.04.002 

Petrini, C. (2015). On the ‘pendulum’ of bioethics. Clinica Terapeutica, 166(2), 82–84. 
https://doi.org/10.7417/CT.2015.1821 

Ploug, T., & Holm, S. (2016). Meta consent—A flexible solution to the problem of secondary use of health 
data. Bioethics, 30(9), 721–732. https://doi.org/10.1111/bioe.12286 

Ploug, Thomas, & Holm, S. (2019). The right to refuse diagnostics and treatment planning by artificial 
intelligence. Medicine, Health Care, and Philosophy. https://doi.org/10.1007/s11019-019-09912-8 

Popkes, A.-L., Overweg, H., Ercole, A., Li, Y., Hernández-Lobato, J. M., Zaykov, Y., & Zhang, C. (2019). 
Interpretable Outcome Prediction with Sparse Bayesian Neural Networks in Intensive Care. 
ArXiv:1905.02599 [Cs, Stat]. Retrieved from http://arxiv.org/abs/1905.02599 

Powles, J., & Hodson, H. (2017). Google DeepMind and healthcare in an age of algorithms. Health and 
Technology, 1–17. https://doi.org/10.1007/s12553-017-0179-1 

Powles, J., & Hodson, H. (2018). Response to DeepMind. Health and Technology, 8(1), 15–29. 
https://doi.org/10.1007/s12553-018-0226-6 

Price, W. N. (2018). Medical Malpractice and Black-Box Medicine. In I. G. Cohen, H. F. Lynch, E. Vayena, & 
U. Gasser (Eds.), Big Data, Health Law, and Bioethics (1st ed., pp. 295–306). 
https://doi.org/10.1017/9781108147972.027 

Price, W. N., Gerke, S., & Cohen, I. G. (2019). Potential Liability for Physicians Using Artificial Intelligence. 
JAMA. https://doi.org/10.1001/jama.2019.15064 

 33 



Quinn, P. (2017). The Anonymisation of Research Data—A Pyric Victory for Privacy that Should Not Be 
Pushed Too Hard by the EU Data Protection Framework? European Journal of Health Law, 24(4), 347–
367. https://doi.org/10.1163/15718093-12341416 

Racine, E., Boehlen, W., & Sample, M. (2019). Healthcare uses of artificial intelligence: Challenges and 
opportunities for growth. Healthcare Management Forum. https://doi.org/10.1177/0840470419843831 

Rahwan, I. (2018). Society-in-the-Loop: Programming the Algorithmic Social Contract. Ethics and Information 
Technology, 20(1), 5–14. https://doi.org/10.1007/s10676-017-9430-8 

Rajkomar, A., Hardt, M., Howell, M. D., Corrado, G., & Chin, M. H. (2018). Ensuring Fairness in Machine 
Learning to Advance Health Equity. Annals of Internal Medicine, 169(12), 866. 
https://doi.org/10.7326/M18-1990 

Rich, E., & Miah, A. (2014). Understanding Digital Health as Public Pedagogy: A Critical Framework. Societies, 
4(2), 296–315. https://doi.org/10.3390/soc4020296 

Richardson, V., Milam, S., & Chrysler, D. (2015). Is Sharing De-identified Data Legal? The State of Public 
Health Confidentiality Laws and Their Interplay with Statistical Disclosure Limitation Techniques. 
Journal of Law, Medicine and Ethics, 43(s1), 83–86. https://doi.org/10.1111/jlme.12224 

Riek, L. D. (2016). Robotics Technology in Mental Health Care. In Artificial Intelligence in Behavioral and Mental 
Health Care (pp. 185–203). https://doi.org/10.1016/B978-0-12-420248-1.00008-8 

Rosenfeld, A., Benrimoh, D., Armstrong, C., Mirchi, N., Langlois-Therrien, T., Rollins, C., … Yaniv-
Rosenfeld, A. (2019). Big Data Analytics and AI in Mental Healthcare. ArXiv:1903.12071 [Cs]. 
Retrieved from http://arxiv.org/abs/1903.12071 

Ruckenstein, M., & Schüll, N. D. (2017). The Datafication of Health. Annual Review of Anthropology, 46(1), 261–
278. https://doi.org/10.1146/annurev-anthro-102116-041244 

Schönberger, D. (2019). Artificial intelligence in healthcare: A critical analysis of the legal and ethical 
implications. International Journal of Law and Information Technology, 27(2), 171–203. 
https://doi.org/10.1093/ijlit/eaz004 

Sethi, N., & Laurie, G. T. (2013). Delivering proportionate governance in the era of eHealth: Making linkage 
and privacy work together. Medical Law International, 13(2–3), 168–204. 
https://doi.org/10.1177/0968533213508974 

Shaw, D. M., Gross, J. V., & Erren, T. C. (2016). Data donation after death: A proposal to prevent the waste of 
medical research data. EMBO Reports, 17(1), 14–17. https://doi.org/10.15252/embr.201541802 

Shickel, B., Tighe, P. J., Bihorac, A., & Rashidi, P. (2018). Deep EHR: A Survey of Recent Advances in Deep 
Learning Techniques for Electronic Health Record (EHR) Analysis. IEEE Journal of Biomedical and 
Health Informatics, 22(5), 1589–1604. https://doi.org/10.1109/JBHI.2017.2767063 

Sterckx, S., Rakic, V., Cockbain, J., & Borry, P. (2016). “You hoped we would sleep walk into accepting the 
collection of our data”: Controversies surrounding the UK care.data scheme and their wider 
relevance for biomedical research. Medicine, Health Care and Philosophy, 19(2), 177–190. 
https://doi.org/10.1007/s11019-015-9661-6 

Sullivan, H. R., & Schweikart, S. J. (2019). Are current tort liability doctrines adequate for addressing injury 
caused by AI? AMA Journal of Ethics, 21(2), 160–166. https://doi.org/10.1001/amajethics.2019.160 

Taddeo, M., & Floridi, L. (2018). How AI can be a force for good: Science, 361(6404), 751–752. 
https://doi.org/10.1126/science.aat5991 

Topol, E. J. (2019). High-performance medicine: The convergence of human and artificial intelligence. Nature 
Medicine, 25(1), 44–56. https://doi.org/10.1038/s41591-018-0300-7 

Topuz, K., Zengul, F. D., Dag, A., Almehmi, A., & Yildirim, M. B. (2018). Predicting graft survival among 
kidney transplant recipients: A Bayesian decision support model. Decision Support Systems, 106, 97–109. 
https://doi.org/10.1016/j.dss.2017.12.004 

Townend, D. (2018). Conclusion: Harmonisation in genomic and health data sharing for research: An 
impossible dream? Human Genetics, 137(8), 657–664. https://doi.org/10.1007/s00439-018-1924-x 

Tran, B. X., Vu, G. T., Ha, G. H., Vuong, Q.-H., Ho, M.-T., Vuong, T.-T., … Ho, R. C. M. (2019). Global 
Evolution of Research in Artificial Intelligence in Health and Medicine: A Bibliometric Study. Journal 
of Clinical Medicine, 8(3). https://doi.org/10.3390/jcm8030360 

Turilli, M., & Floridi, L. (2009). The ethics of information transparency. Ethics and Information Technology, 11(2), 
105–112. https://doi.org/10.1007/s10676-009-9187-9 

 34 



Vayena, E., Tobias, H., Afua, A., & Allesandro, B. (2018). Digital health: Meeting the ethical and policy 
challenges. Swiss Medical Weekly, 148(34). https://doi.org/10.4414/smw.2018.14571 

Vayena, Effy, Blasimme, A., & Cohen, I. G. (2018). Machine learning in medicine: Addressing ethical 
challenges. PLoS Medicine, 15(11), e1002689. https://doi.org/10.1371/journal.pmed.1002689 

Vayena, Effy, Salathé, M., Madoff, L. C., & Brownstein, J. S. (2015). Ethical Challenges of Big Data in Public 
Health. PLOS Computational Biology, 11(2), e1003904. https://doi.org/10.1371/journal.pcbi.1003904 

Voigt, K. (2019). Social Justice, Equality and Primary Care: (How) Can ‘Big Data’ Help? Philosophy & Technology, 
32(1), 57–68. https://doi.org/10.1007/s13347-017-0270-6 

Vollmer, S., Mateen, B. A., Bohner, G., Király, F. J., & Ghani, R. (2018). Machine learning and AI research for 
Patient Benefit: 20 Critical Questions on Transparency, Replicability, Ethics and Effectiveness. 25. 

Vollmer, S., Mateen, B. A., Bohner, G., Király, F. J., Ghani, R., Jonsson, P., … Hemingway, H. (2018). 
Machine learning and AI research for Patient Benefit: 20 Critical Questions on Transparency, 
Replicability, Ethics and Effectiveness. ArXiv:1812.10404 [Cs, Stat]. Retrieved from 
http://arxiv.org/abs/1812.10404 

Wachter, R. M. (2015). The digital doctor: Hope, hype, and harm at the dawn of medicine’s computer age. New York: 
McGraw-Hill Education. 

Wang, Z. (2019). Data integration of electronic medical record under administrative decentralization of medical 
insurance and healthcare in China: A case study. Israel Journal of Health Policy Research, 8(1). 
https://doi.org/10.1186/s13584-019-0293-9 

Wardrope, A. (2015). Relational Autonomy and the Ethics of Health Promotion. Public Health Ethics, 8(1), 50–
62. https://doi.org/10.1093/phe/phu025 

Watson, D. S., Krutzinna, J., Bruce, I. N., Griffiths, C. E., McInnes, I. B., Barnes, M. R., & Floridi, L. (2019). 
Clinical applications of machine learning algorithms: Beyond the black box. BMJ, 364, l886. 
https://doi.org/10.1136/bmj.l886 

World Health Organisation. (n.d.). Big data and artificial intelligence. Retrieved 29 June 2019, from 
https://www.who.int/ethics/topics/big-data-artificial-intelligence/en/ 

Zacher, B., & Czogiel, I. (2019). Supervised learning improves disease outbreak detection. ArXiv:1902.10061 
[Cs, Stat]. Retrieved from http://arxiv.org/abs/1902.10061 

Zhang, L., Wang, H., Li, Q., Zhao, M.-H., & Zhan, Q.-M. (2018). Big data and medical research in China. BMJ, 
j5910. https://doi.org/10.1136/bmj.j5910 

 
 

 35 

View publication stats﻿1 

Artificial Intelligence: the global landscape of ethics guidelines 
 
 
 
Anna Jobin a, Marcello Ienca a, Effy Vayena a* 
 
 
 
a Health Ethics & Policy Lab, ETH Zurich, 8092 Zurich, Switzerland 
 
* Corresponding author: effy.vayena@hest.ethz.ch 
 
 
 
Preprint version 
 
© The authors 2019 
 
 
 
 
 
 
ABSTRACT 

In the last five years, private companies, research institutions as well as public sector 
organisations have issued principles and guidelines for ethical AI, yet there is debate about 
both what constitutes “ethical AI” and which ethical requirements, technical standards and 
best practices are needed for its realization. To investigate whether a global agreement on 
these questions is emerging, we mapped and analyzed the current corpus of principles and 
guidelines on ethical AI. Our results reveal a global convergence emerging around five 
ethical principles (transparency, justice and fairness, non-maleficence, responsibility and 
privacy), with substantive divergence in relation to how these principles are interpreted; 
why they are deemed important; what issue, domain or actors they pertain to; and how they 
should be implemented. Our findings highlight the importance of integrating guideline-
development efforts with substantive ethical analysis and adequate implementation 
strategies.

 



  2 

MAIN ARTICLE 

Introduction 

Artificial Intelligence (AI), or the theory and development of computer systems able to 
perform tasks normally requiring human intelligence, is widely heralded as an ongoing 
“revolution” transforming science and society altogether1,2. While approaches to AI such as 
machine learning, deep learning and artificial neural networks are reshaping data processing 
and analysis3, autonomous and semi-autonomous systems are being increasingly used in a 
variety of sectors including healthcare, transportation and the production chain4. In light of 
its powerful transformative force and profound impact across various societal domains, AI 
has sparked ample debate about the principles and values that should guide its development 
and use5,6. Fears that AI might jeopardize jobs for human workers7, be misused by 
malevolent actors8, elude accountability or inadvertently disseminate bias and thereby 
undermine fairness9 have been at the forefront of the recent scientific literature and media 
coverage. Several studies have discussed the topic of ethical AI10–13, notably in meta-
assessments14–16 or in relation to systemic risks17,18 and unintended negative consequences 
like algorithmic bias or discrimination19–21. 
 
National and international organisations have responded to these societal fears by 
developing ad hoc expert committees on AI, often commissioned with the drafting of policy 
documents. These include the High-Level Expert Group on Artificial Intelligence appointed 
by the European Commission, the expert group on AI in Society of the Organisation for 
Economic Co-operation and Development (OECD), the Advisory Council on the Ethical 
Use of Artificial Intelligence and Data in Singapore, and the select committee on Artificial 
Intelligence of the United Kingdom (UK) House of Lords. As part of their institutional 
appointments, these committees have produced or are reportedly producing reports and 
guidance documents on AI. Similar efforts are taking place in the private sector, especially 
among corporations who rely on AI for their business. In 2018 alone, companies such as 
Google and SAP have publicly released AI guidelines and principles. Declarations and 
recommendations have also been issued by professional associations and non-profit 
organisations such as the Association of Computing Machinery (ACM), Access Now and 
Amnesty International. The intense efforts of such a diverse set of stakeholders in issuing 
AI principles and policies demonstrate not only the need for ethical guidance, but also the 

 



  3 

strong interest of these stakeholders to shape the ethics of AI in ways that meet their 
respective priorities16. Notably, the private sector’s involvement in the AI-ethics arena has 
been called into question for potentially using such high-level soft-policy as a portmanteau 
to either render a social problem technical16 or to eschew regulation altogether22. Beyond 
the composition of the groups that have produced ethical guidance on AI, the content of this 
guidance itself is of interest. Are these various groups converging on what ethical AI should 
be, and the ethical principles that will determine the development of AI? If they diverge, 
what are these differences and can they be reconciled? 
 
To answer these questions, we conducted a scoping review of the existing corpus of 
guidelines on ethical AI. Our analysis aims at mapping the global landscape of existing 
principles and guidelines for ethical AI and thereby determining whether a global 
convergence is emerging regarding both the principles for ethical AI and the requirements 
for its realization. This analysis will inform scientists, research institutions, funding 
agencies, governmental and inter-governmental organisations and other relevant 
stakeholders involved in the advancement of ethically responsible innovation in AI.  
 
Results 

Our search identified 84 documents containing ethical principles or guidelines for AI (cf. 
Table 1). Data reveal a significant increase over time in the number of publications, with 
88% having been released after 2016 (cf. SI Table S1). Data breakdown by type and 
geographic location of issuing organisation (cf. SI Table S1) shows that most documents 
were produced by private companies (n=19; 22.6%) and governmental agencies 
respectively (n=18; 21.4%), followed by academic and research institutions (n=9; 10.7%), 
inter-governmental or supra-national organisations (n=8; 9.5%), non-profit organisations 
and professional associations/scientific societies (n=7 each; 8.3% each), private sector 
alliances (n=4; 4.8%), research alliances (n=1; 1.2%), science foundations (n=1; 1.2%), 
federations of worker unions (n=1; 1.2%) and political parties (n=1; 1.2%). Four documents 
were issued by initiatives belonging to more than one of the above categories and four more 
could not be classified at all (4.8% each). 
 
 

 



  4 

Table 1- Ethical guidelines for AI by country of issuer 

Name of Document/Website Issuer Country of 
issuer 

Artificial Intelligence. Australia's Ethics Framework. A Department of Industry Innovation and Science Australia 
discussion Paper 
Montréal Declaration: Responsible AI Université de Montréal Canada 
Work in the age of artificial intelligence. Four perspectives on the Ministry of Economic Affairs and Employment Finland 
economy, employment, skills and ethics 
Tieto’s AI ethics guidelines Tieto Finland 
Commitments and principles OP Group Finland 
How can humans keep the upper hand? Report on the ethical French Data Protection Authority (CNIL)  France 
matters raised by AI algorithms 
For a meaningful Artificial Intelligence. Towards a French and Mission Villani France 
European strategy 
Ethique de la recherche en robotique CERNA (Allistene) France 
AI Guidelines Deutsche Telekom Germany 
SAP’s guiding principles for artificial intelligence SAP Germany 
Automated and Connected Driving: Report Federal Ministry of Transport and Digital Infrastructure, Ethics Germany 

Commission 
Ethics Policy Icelandic Institute for Intelligent Machines (IIIM) Iceland 
Discussion Paper: National Strategy for Artificial Intelligence National Institution for Transforming India (Niti Aayog) India 
L'intelligenzia artificiale al servizio del cittadino Agenzia per l'Italia Digitale (AGID) Italy 
The Japanese Society for Artificial Intelligence Ethical Japanese Society for Artificial Intelligence Japan 
Guidelines 
Report on Artificial Intelligence and Human Society (Unofficial Advisory Board on Artificial Intelligence and Human Society Japan 
translation) (initiative of the Minister of State for Science and Technology 

Policy) 
Draft AI R&D Guidelines for International Discussions Institute for Information and Communications Policy (IICP), The Japan 

Conference toward AI Network Society 
Sony Group AI Ethics Guidelines SONY Japan 
Human Rights in the Robot Age Report The Rathenau Institute Netherlands 
Dutch Artificial Intelligence Manifesto Special Interest Group on Artificial Intelligence (SIGAI), ICT Netherlands 

Platform Netherlands (IPN) 
Artificial intelligence and privacy The Norwegian Data Protection Authority Norway 
Discussion Paper on Artificial Intelligence (AI) and Personal Personal Data Protection Commission Singapore Singapore 
Data - Fostering Responsible Development and Adoption of AI 
Mid- to Long-Term Master Plan in Preparation for the Intelligent Government of the Republic of Korea South Korea 
Information Society 
AI Principles of Telefónica Telefonica Spain 
AI Principles & Ethics Smart Dubai UAE 
Principles of robotics Engineering and Physical Sciences Research Council UK (EPSRC) UK 
The Ethics of Code: Developing AI for Business with Five Core Sage UK 
Principles 
Big data, artificial intelligence, machine learning and data Information Commissioner's Office UK 
protection 
DeepMind Ethics & Society Principles DeepMind Ethics & Society UK 
Business Ethics and Artificial Intelligence Institute of Business Ethics UK 
AI in the UK: ready, willing and able? UK House of Lords, Select Committee on Artificial Intelligence UK 
Artificial Intelligence (AI) in Health Royal College of Physicians UK 
Initial code of conduct for data-driven health and care technology UK Department of Health & Social Care UK 
Ethics Framework - Responsible AI Machine Intelligence Garage Ethics Committee UK 
The responsible AI framework PriceWaterhouseCoopers UK UK 
Responsible AI and robotics. An ethical framework. Accenture UK UK 
Machine learning: the power and promise of computers that learn The Royal Society UK 
by example 
Ethical, social, and political challenges of Artificial Intelligence Future Advocacy UK 
in Health 
Unified Ethical Frame for Big Data Analysis. IAF Big Data The Information Accountability Foundation UK 
Ethics Initiative, Part A 
The AI Now Report. The Social and Economic Implications of AI Now Institute USA 
Artificial Intelligence Technologies in the Near-Term 
Statement on Algorithmic Transparency and Accountability Association for Computing Machinery (ACM) USA 
AI Principles Future of Life Institute USA 
AI - Our approach Microsoft USA 
Artificial Intelligence. The Public Policy Opportunity Intel Corporation USA 
IBM’s Principles for Trust and Transparency IBM USA 
OpenAI Charter OpenAI USA 
Our principles Google USA 
Policy Recommendations on Augmented Intelligence in Health American Medical Association (AMA) USA 
Care H-480.940 
Everyday Ethics for Artificial Intelligence. A practical guide for IBM USA 
designers & developers 
Governing Artificial Intelligence. Upholding Human Rights & Data & Society USA 
Dignity 
Intel’s AI Privacy Policy White Paper. Protecting individuals’ Intel Corporation USA 
privacy and data in the artificial intelligence world 
Introducing Unity’s Guiding Principles for Ethical AI – Unity Unity Technologies USA 
Blog 
Digital Decisions Center for Democracy & Technology USA 
Science, Law and Society (SLS) Initiative The Future Society USA 
AI Now 2018 Report AI Now Institute USA 
Responsible bots: 10 guidelines for developers of conversational Microsoft USA 
AI 
Preparing for the future of Artificial Intelligence Executive Office of the President; National Science and Technology USA 

Council; Committee on Technology 
The National Artificial Intelligence Research and Development National Science and Technology Council; Networking and USA 
Strategic Plan Information Technology Research and Development Subcommittee 

 



  5 

AI Now 2017 Report AI Now Institute USA 
Position on Robotics and Artificial Intelligence The Greens (Green Working Group Robots) EU 
Report with recommendations to the Commission on Civil Law European Parliament EU 
Rules on Robotics 
Ethics Guidelines for Trustworthy AI High-Level Expert Group on Artificial Intelligence EU 
AI4People—An Ethical Framework for a Good AI Society: AI4People EU 
Opportunities, Risks, Principles, and Recommendations 
European ethical Charter on the use of Artificial Intelligence in Concil of Europe: European Commission for the efficiency of EU 
judicial systems and their environment Justice (CEPEJ) 
Statement on Artificial Intelligence, Robotics and 'Autonomous' European Commission, European Group on Ethics in Science and EU 
Systems New Technologies 
Artificial Intelligence and Machine Learning: Policy Paper Internet Society international 
Report of COMEST on Robotics Ethics COMEST/UNESCO international 
Ethical Principles for Artificial Intelligence and Data Analytics Software & Information Industry Association (SIIA), Public Policy international 

Division 
ITI AI Policy Principles Information Technology Industry Council (ITI) international 
Ethically Aligned Design. A Vision for Prioritizing Human Well- Institute of Electrical and Electronics Engineers (IEEE), The IEEE international 
being with Autonomous and Intelligent Systems, version 2 Global Initiative on Ethics of Autonomous and Intelligent Systems 
Top 10 Principles for Ethical Artificial Intelligence UNI Global Union international 
The Malicious Use of Artificial Intelligence: Forecasting, Future of Humanity Institute; University of Oxford; Centre for the international 
Prevention, and Mitigation Study of Existential Risk; University of Cambridge; Center for a 

New American Security; Electronic Frontier Foundation; OpenAI 
White Paper: How to Prevent Discriminatory Outcomes in WEF, Global Future Council on Human Rights 2016-2018 international 
Machine Learning 
Privacy and Freedom of Expression In the Age of Artificial Privacy International & Article 19 international 
Intelligence 
The Toronto Declaration: Protecting the right to equality and non- Access Now ; Amnesty International international 
discrimination in machine learning systems 
Charlevoix Common Vision for the Future of Artificial Leaders of the G7 international 
Intelligence 
Artificial Intelligence: open questions about gender inclusion W20 international 
Declaration on ethics and data protection in Artificial Intelligence ICDPPC international 
Universal Guidelines for Artificial Intelligence The Public Voice international 
Ethics of AI in Radiology: European and North American American College of Radiology; European Society of Radiology; international 
Multisociety Statement Radiology Society of North America; Society for Imaging 

Informatics in Medicine; European Society of Medical Imaging 
Informatics; Canadian Association of Radiologists; American 
Association of Physicists in Medicine 

Ethically Aligned Design: A Vision for Prioritizing Human Well- Institute of Electrical and Electronics Engineers (IEEE), The IEEE international 
being with Autonomous and Intelligent Systems, First Edition Global Initiative on Ethics of Autonomous and Intelligent Systems 
(EAD1e) 
Tenets Partnership on AI n.a. 
Principles for Accountable Algorithms and a Social Impact Fairness, Accountability, and Transparency in Machine Learning n.a. 
Statement for Algorithms (FATML) 
10 Principles of responsible AI Women leading in AI n.a. 

 

In terms of geographic distribution, data show a significant representation of more 
economically developed countries (MEDC), with the USA (n=20; 23.8%) and the UK 
(n=14; 16.7%) together accounting for more than a third of all ethical AI principles, 
followed by Japan (n=4; 4.8%), Germany, France, and Finland (each n=3; 3.6% each). The 
cumulative number of sources from the European Union, comprising both documents issued 
by EU institutions (n=6) and documents issued within each member state (13 in total), 
accounts for 19 documents overall. African and South-American countries are not 
represented independently from international or supra-national organisations (cf. Figure 1). 
 

 



  6 

Figure 1- Geographic distribution of issuers of ethical AI guidelines by number of documents released 

 

Figure 1: Geographic distribution of issuers of ethical AI guidelines by number of 
documents released. Most ethics guidelines are released in the United States (n=20) and 
within the European Union (19), followed by the United Kingdom (14) and Japan (4). 
Canada, Iceland, Norway, the United Arab Emirates, India, Singapore, South Korea, 
Australia are represented with 1 document each. Having endorsed a distinct G7 statement, 
member states of the G7 countries are highlighted separately. Map created using 
mapchart.net. 
 

Data breakdown by target audience indicates that most principles and guidelines are 
addressed to multiple stakeholder groups (n=27; 32.1%). Another significant portion of the 
documents is self-directed, as they are addressed to a category of stakeholders within the 
sphere of activity of the issuer such as the members of the issuing organisation or the issuing 
company’s employees (n=24; 28.6%). Finally, some documents target the public sector 
(n=10; 11.9%), the private sector (n=5; 6.0%), or other specific stakeholders beyond 
members of the issuing organisation, namely developers or designers (n=3; 3.6%), 
‘organisations’ (n=1; 1.2%) and researchers (n=1; 1.2%). 13 sources (15.5%) do not specify 
their target audience (cf. SI Table S1).  
 
Eleven overarching ethical values and principles have emerged from our content analysis. 
These are, by frequency of the number of sources in which they were featured: transparency, 
justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and 
autonomy, trust, dignity, sustainability, and solidarity (cf. Table 2). 
 

 



  7 

Table 2 – Ethical principles identified in existing AI guidelines 

Ethical principle Number of Included codes 

documents 

Transparency 73/84 Transparency, explainability, explicability, understandability, 

interpretability, communication, disclosure, showing 

Justice & fairness 68/84 Justice, fairness, consistency, inclusion, equality, equity, (non-)bias, 

(non-)discrimination, diversity, plurality, accessibility, reversibility, 

remedy, redress, challenge, access and distribution 

Non-maleficence 60/84 Non-maleficence, security, safety, harm, protection, precaution, 

prevention, integrity (bodily or mental), non-subversion 

Responsibility 60/84 Responsibility, accountability, liability, acting with integrity 

Privacy 47/84 Privacy, personal or private information 

Beneficence 41/84 Benefits, beneficence, well-being, peace, social good, common good 

Freedom & 34/84 Freedom, autonomy, consent, choice, self-determination, liberty, 

autonomy empowerment 

Trust 28/84 Trust 

Sustainability 14/84 Sustainability, environment (nature), energy, resources (energy) 

Dignity 13/84 Dignity 

Solidarity 6/84 Solidarity, social security, cohesion 

 
No single ethical principle appeared to be common to the entire corpus of documents, 
although there is an emerging convergence around the following principles: transparency, 
justice and fairness, non-maleficence, responsibility, and privacy. These principles are 
referenced in more than half of all the sources. Nonetheless, further thematic analysis 
reveals significant semantic and conceptual divergences in both how the eleven ethical 
principles are interpreted and the specific recommendations or areas of concern derived 
from each. A detailed thematic evaluation is presented in the following. 
 
Transparency 
Featured in 73/84 sources, transparency is the most prevalent principle in the current 
literature. Thematic analysis reveals significant variation in relation to the interpretation, 

 



  8 

justification, domain of application, and mode of achievement. References to transparency 
comprise efforts to increase explainability, interpretability or other acts of communication 
and disclosure (cf. Table 2). Principal domains of application include data use23–26, human-
AI interaction23,27–35, automated decisions26,36–46, and the purpose of data use or application 
of AI systems24,27,47–51. Primarily, transparency is presented as a way to minimize harm and 
improve AI36–38,44,45,49,52–55, though some sources underline its benefit for legal 
reasons37,45,46,49,50,52 or to foster trust23,24,29,33,36,37,48,51,52,56–58. A few sources also link 
transparency to dialogue, participation, and the principles of democracy30,41,49,50,52,59. 
 
To achieve greater transparency, many sources suggest increased disclosure of information 
by those developing or deploying AI systems36,51,60,61, although specifications regarding 
what should be communicated vary greatly: use of AI45, source code31,52,62, data use35,47,50,58, 
evidence base for AI use57, limitations25,33,47,51,58,60,63, laws62,64, responsibility for AI40, 
investments in AI44,65 and possible impact66. The provision of explanations ‘in non-technical 
terms’26 or auditable by humans37,60 is encouraged. Whereas audits and 
auditability28,39,44,45,50,59,61,62,67,68 are mainly proposed by data protection offices and NPOs, 
it is mostly the private sector that suggests technical solutions27,30,52,59,69,70. Alternative 
measures focus on oversight45,47,48,55,62, interaction and mediation with stakeholders and the 
public24,32,36,51,61,71 and the facilitation of whistleblowing36,60. 
 
Justice, fairness, and equity 
Justice is mainly expressed in terms of fairness23,25,27–29,48,50,58,60,66,72–77, and of prevention, 
monitoring or mitigation of unwanted bias23,28,33,40,47,52,54,58,64,69,73,74,78–80 and 
discrimination28,33,36,38,44,45,50,55,56,60,68,81–84, the latter being significantly less referenced than 
the first two by the private sector. Whereas some sources focus on justice as respect for 
diversity31,38,56,59,65,66,70,72,78,80,85,86, inclusion31,45,47,51,72,80 and equality41,45,51,59,60,72,78, others 
call for a possibility to appeal or challenge decisions28,35–37,74,79, or the right to 
redress33,42,45,46,50,68,85 and remedy45,48. Sources also emphasize the importance of fair access 
to AI59,70,87, to data33,37,44,67,83,88–90, and to the benefits of AI37,38,80,91. Issuers from the public 
sector place particular emphasis on AI’s impact on the labor market37,38,55,84,92, and the need 
to address democratic33,38,59,73 or societal31,48,55,65 issues. Sources focusing on the risk of 
biases within datasets underline the importance of acquiring and processing accurate, 
complete and diverse data23,28,52,70,93, especially training data27,33,35,38,52,58. 

 



  9 

 
If specified, the preservation and promotion of justice are proposed to be pursued through: 
(a) technical solutions such as standards50,68,89 or explicit normative encoding28,37,43,67; (b) 
transparency54,62, notably by providing information36,38,79 and raising public awareness of 
existing rights and regulation28,59; (c) testing52,58,67,69, monitoring54,56 and auditing39,46,50,67, 
the preferred solution of notably data protection offices; (d) developing or strengthening the 
rule of law and the right to appeal, recourse, redress, or remedy37,38,42,45,46,48,68,74,79; (e) via 
systemic changes and processes such as governmental action42,45,87,92 and oversight94, a 
more interdisciplinary47,65,85,93 or otherwise diverse58,59,70,85,87,95 workforce, as well as better 
inclusion of civil society or other relevant stakeholders in an interactive 
manner28,33,41,46,55,57,58,65,68,69,79,80,86 and increased attention to the distribution of 
benefits25,33,38,48,63,76. 
 
Non-maleficence 
References to non-maleficence outweigh those to beneficence by a factor of 1.5 and 
encompass general calls for safety and security80,90,96,97 or state that AI should never cause 
foreseeable or unintentional harm23,30,33,56,60,79. More granular considerations entail the 
avoidance of specific risks or potential harms, e.g. intentional misuse via cyberwarfare and 
malicious hacking51,53,54,78,81,89, and suggest risk-management strategies. Harm is primarily 
interpreted as discrimination38,44,47,48,50,95,98, violation of privacy23,35,44,64,78,98,99, or bodily 
harm25,30,31,33,56,92,96,100. Less frequent characterizations include loss of trust30 or skills44, 
‘radical individualism’38, the risk that technological progress might outpace regulatory 
measures57, negative impacts on long-term social well-being44, on infrastructure44, or on 
psychological35,56, emotional56 or economic aspects44,56. 
 
Harm-prevention guidelines focus primarily on technical measures and governance 
strategies, ranging from interventions at the level of AI research27,47,64,79,85,101, 
design23,25,27,32,39,56,58, technology development and/or deployment54 to lateral and continuous 
approaches33,55,63. Technical solutions include in-built data quality evaluations25 or 
security23 and privacy by design23,27,39, though notable exceptions also advocate for 
establishing industry standards30,64,102. Proposed governance strategies include active 
cooperation across disciplines and stakeholders33,47,53,62, compliance with existing or new 
legislation27,31,35,81,95,99, and the need to establish oversight processes and practices, notably 

 



  10 

tests36,38,47,74,79, monitoring36,58, audits and assessments by internal units, customers, users, 
independent third parties, or governmental entities40,48,51,58,81,94,95,98, often geared towards 
standards for AI implementation and outcome assessment. Most sources explicitly mention 
potential ‘dual-use’8,32,33,38,60,79 or imply that damages may be unavoidable, in which case 
risks should be assessed40,48,51, reduced40,69,72–74, and mitigated34,35,38,53,63,68, and the 
attribution of liability should be clearly defined31,37,38,44,82. 
 
Responsibility and accountability 
Despite widespread references to ‘responsible AI’43,51,78,83, responsibility and accountability 
are rarely defined. Nonetheless, specific recommendations include acting with 
‘integrity’47,52,60 and clarifying the attribution of responsibility and legal liability23,58,78,103, 
if possible upfront36, in contracts52 or, alternatively, by centering on remedy26. In contrast, 
other sources suggest focusing on the underlying reasons and processes that may lead to 
potential harm74,83. Yet others underline the responsibility of whistleblowing in case of 
potential harm36,55,60, and aim at promoting diversity49,92 or introducing ethics into STEM 
education59. Very different actors are named as being responsible and accountable for AI’s 
actions and decisions: AI developers58,60,73,96, designers36,44, ‘institutions’40,42 or 
‘industry’69. Further disagreement emerged on whether AI should be held accountable in a 
human-like manner70 or whether humans should always be the only actors who are 
ultimately responsible for technological artifacts31,32,35,37,52,92. 
 
Privacy 
Ethical AI sees privacy both as a value to uphold44,64,75,99 and as a right to be 
protected27,28,37,38,53. While often undefined, privacy is often presented in relation to data 
protection23,27,36,53,58,66,71,79,83,98 and data security27,35,64,66,88,98. A few sources link privacy to 
freedom38,53 or trust74,92. Suggested modes of achievement fall into three categories: 
technical solutions64,80 such as differential privacy74,89, privacy by design25,27,28,79,98, data 
minimization36,58, and access control36,58, calls for more research47,64,74,98 and awareness64,74, 
and regulatory approaches25,52,71, with sources referring to legal compliance more 
broadly27,32,36,58,60,81, or suggesting certificates104 or the creation or adaptation of laws and 
regulations to accommodate the specificities of AI64,74,88,105. 
 
  

 



  11 

Beneficence 
While promoting good (beneficence in ethical terms) is often mentioned, it is rarely defined, 
though notable exceptions mention the augmentation of human senses86, the promotion of 
human well-being and flourishing34,90, peace and happiness60, the creation of socio-
economic opportunities36, and economic prosperity37,53. Similar uncertainty concerns the 
actors that should benefit from AI: private sector issuers tend to highlight the benefit of AI 
for customers23,48, though many sources require AI to be shared49,52,76 and to benefit 
‘everyone’36,59,65,84, ‘humanity’27,37,44,60,100,102, both of the above48,66, ‘society’34,87, ‘as many 
people as possible’37,53,99, ‘all sentient creatures’83, the ‘planet’37,72 and the environment38,90. 
Strategies for the promotion of good include aligning AI with human values34,44, advancing 
‘scientific understanding of the world’100, minimizing power concentration102 or, 
conversely, using power ‘for the benefit of human rights’82; working more closely with 
‘affected’ people65, minimizing conflicts of interests102; proving beneficence through 
customer demand48 and feedback58, and developing new metrics and measurements for 
human well-being44,90. 
 
Freedom and autonomy 
Whereas some sources specifically refer to the freedom of expression28,73,82,105 or 
informational self-determination28,90 and ‘privacy-protecting user controls’58, others 
generally promote freedom31,69,72, empowerment28,52,99 or autonomy31,33,62,77,81,96. Some 
documents refer to autonomy as a positive freedom, specifically the freedom to flourish36, 
to self-determination through democratic means38, the right to establish and develop 
relationships with other human beings38,92, the freedom to withdraw consent67, or the 
freedom to use a preferred platform or technology73,80. Other documents focus on negative 
freedom, for example freedom from technological experimentation82, manipulation33 or 
surveillance38. Freedom and autonomy are believed to be promoted through transparency 
and predictable AI38, by not ‘reducing options for and knowledge of citizens’38, by actively 
increasing people’s knowledge about AI36,52,62, giving notice and consent79 or, conversely, 
by actively refraining from collecting and spreading data in absence of informed 
consent30,38,44,55,74. 
 
  

 



  12 

Trust 
References to trust include calls for trustworthy AI research and technology50,97,99, 
trustworthy AI developers and organisations51,60,66, trustworthy ‘design principles’91, or 
underline the importance of customers’ trust23,52,58,66,74,80. Calls for trust are proposed 
because a culture of trust among scientists and engineers is believed to support the 
achievement of other organisational goals99, or because overall trust in the 
recommendations, judgments and uses of AI is indispensable for AI to ‘fulfill its world 
changing potential’24. This last point is contradicted by one guideline explicitly warning 
against excessive trust in AI81. Suggestions for building or sustaining trust include 
education33, reliability50,51, accountability56, processes to monitor and evaluate the integrity 
of AI systems over time51 and tools and techniques ensuring compliance with norms and 
standards43,63. Whereas some guidelines require AI to be transparent37,43,57,58, 
understandable36,37, or explainable52 in order to build trust, another one explicitly suggests 
that, instead of demanding understandability, it should be ensured that AI fulfills public 
expectations50. Other reported facilitators of trust include ‘a Certificate of Fairness’104, 
multi-stakeholder dialogue64, awareness about the value of using personal data74, and 
avoiding harm30,56. 
 
Sustainability 
To the extent that is referenced, sustainability calls for development and deployment of AI 
to consider protecting the environment33,38,46, improving the planet’s ecosystem and 
biodiversity37, contributing to fairer and more equal societies65 and promoting peace66. 
Ideally, AI creates sustainable systems44,76,90 that process data sustainably43 and whose 
insights remain valid over time48. To achieve this aim, AI should be designed, deployed and 
managed with care38 to increase its energy efficiency and minimize its ecological footprint31. 
To make future developments sustainable, corporations are asked to create policies ensuring 
accountability in the domain of potential job losses37 and to use challenges as an opportunity 
for innovation38. 
 
Dignity 
While dignity remains undefined in existing guidelines, safe the specification that it is a 
prerogative of humans but not robots92, there is frequent reference to what it entails: dignity 
is intertwined with human rights101 or otherwise means avoiding harm31, forced 

 



  13 

acceptance31, automated classification38, and unknown human-AI interaction38. It is argued 
that AI should not diminish33 or destroy80 but respect82, preserve69 or even increase human 
dignity36,37. Dignity is believed to be preserved if it is respected by AI developers in the first 
place96 and promoted through new legislation38, through governance initiatives36, or through 
government-issued technical and methodological guidelines82. 
 
Solidarity 
Solidarity is mostly referenced in relation to the implications of AI for the labor market104. 
Sources call for a strong social safety net37,84. They underline the need for redistributing the 
benefits of AI in order not to threaten social cohesion49 and respecting potentially vulnerable 
persons and groups33. Lastly, there is a warning of data collection and practices focused on 
individuals which may undermine solidarity in favour of ‘radical individualism’38. 
 
Discussion 

We found a rapid increase in the number and variety of guidance documents for ethical AI, 
demonstrating the increasing active involvement of the international community. 
Organisations publishing AI guidelines come from a wide range of sectors. In particular the 
nearly equivalent proportion of documents issued by the public sector (i.e. governmental 
and inter-governmental organisations) and the private sector (companies and private sector 
alliances) indicate that the ethical challenges of AI concern both public entities and private 
enterprises. However, there is significant divergence in the solutions proposed to meet the 
ethical challenges of AI. Further, the relative underrepresentation of geographic areas such 
as Africa, South and Central America and Central Asia indicates that the international 
debate over ethical AI may not be happening globally in equal measures. MEDC countries 
are shaping this debate more than others, which raises concerns about neglecting local 
knowledge, cultural pluralism and global fairness. 
 
The proliferation of soft-law efforts can be interpreted as a governance response to advanced 
research into AI, whose research output and market size have drastically increased106 in 
recent years. Our analysis shows the emergence of an apparent cross-stakeholder 
convergence on promoting the ethical principles of transparency, justice, non-maleficence, 
responsibility, and privacy. Nonetheless, our thematic analysis reveals substantive 
divergences in relation to four major factors: (i) how ethical principles are interpreted, (ii) 

 



  14 

why they are deemed important, (iii) what issue, domain or actors they pertain to, and (iv) 
how they should be implemented. Furthermore, unclarity remains as to which ethical 
principles should be prioritized, how conflicts between ethical principles should be resolved, 
who should enforce ethical oversight on AI and how researchers and institutions can comply 
with the resulting guidelines. These findings suggest the existence of a gap at the cross-
section of principles formulation and their implementation into practice which can hardly 
be solved through technical expertise or top-down approaches. 
 
Although no single ethical principle is explicitly endorsed by all existing guidelines, 
transparency, justice and fairness, non-maleficence, responsibility and privacy are each 
referenced in more than half of all guidelines. This focus could be indicating a developing 
convergence on ethical AI around these principles in the global policy landscape. In 
particular, the prevalence of calls for transparency, justice and fairness points to an emerging 
moral priority to require transparent processes throughout the entire AI continuum (from 
transparency in the development and design of algorithms to transparent practices for AI 
use), and to caution the global community against the risk that AI might increase inequality 
if justice and fairness considerations are not adequately addressed. Both these themes appear 
to be intertwined with the theme of responsibility, as the promotion of both transparency 
and justice seems to postulate increased responsibility and accountability on the side of AI 
makers and deployers. 
 
It has been argued that transparency is not an ethical principle per se, but rather “a proethical 
condition for enabling or impairing other ethical practices or principles”107. The proethical 
nature of transparency might partly explain its higher prevalence compared to other ethical 
principles. It is notable that current guidelines place significant value in the promotion of 
responsibility and accountability, yet few of them emphasize the duty of all stakeholders 
involved in the development and deployment of AI to act with integrity. This mismatch is 
probably associated with the observation that existing guidelines fail to establish a full 
correspondence between principles and actionable requirements, with several principles 
remaining uncharacterized or disconnected from the requirements necessary for their 
realization.  
 

 



  15 

As codes related to non-maleficence outnumber those related to beneficence, it appears that, 
for the current AI community, the moral obligation to preventing harm takes precedence 
over the promotion of good. This fact can be partly interpreted as an instance of the so-
called negativity bias, i.e. a general cognitive bias to give greater weight to negative 
entities108,109. This negative characterization of ethical values is further emphasized by the 
fact that existing guidelines focus primarily on how to preserve privacy, dignity, autonomy 
and individual freedom in spite of advances in AI, while largely neglecting whether these 
principles could be promoted through responsible innovation in AI110. 
 
The issue of trust in AI, while being addressed by less than one third of all sources, tackles 
a critical ethical dilemma in AI governance: determining whether it is morally desirable to 
foster public trust in AI. While several sources, especially those produced within the private 
sector, highlight the importance of fostering trust in AI through educational and awareness-
raising activities, a smaller number of sources contend that trust in AI may actually diminish 
scrutiny and undermine some societal obligations of AI producers111. This possibility would 
challenge the dominant view in AI ethics that building public trust in AI is a fundamental 
requirement for ethical governance112.  
 
The relative thematic underrepresentation of sustainability and solidarity suggests that these 
topics might be currently flying under the radar of the mainstream ethical discourse on AI. 
The underrepresentation of sustainability-related principles is particularly problematic in 
light of the fact that the deployment of AI requires massive computational resources which, 
in turn, require high energy consumption. The environmental impact of AI, however, does 
not only involve the negative effects of high-footprint digital infrastructures, but also the 
possibility of harnessing AI for the benefit of ecosystems and the entire biosphere. This 
latter point, highlighted in a report by the World Economic Forum113 though not in the AI 
guidelines by the same institution, requires wider endorsement to become entrenched in the 
ethical AI narrative. The ethical principle of solidarity is sparsely referenced, typically in 
association with the development of inclusive strategies for the prevention of job losses and 
unfair sharing of burdens. Little attention is devoted to promoting solidarity through the 
emerging possibility of using AI expertise for solving humanitarian challenges, a mission 
that is currently being pursued, among others, by intergovernmental organisations such as 
the United Nations Office for Project Services (UNOPS)114 or the World Health 

 



  16 

Organization (WHO) and private companies such as Microsoft115. As the humanitarian cost 
of anthropogenic climate change is rapidly increasing116, the principles of sustainability and 
solidarity appear strictly intertwined though poorly represented compared to other 
principles. 
 
While numerical data indicate an emerging convergence around the promotion of some 
ethical principles, in-depth thematic analysis paints a more complicated picture, as there are 
critical differences in how these principles are interpreted as well as what requirements are 
considered to be necessary for their realization. Results show that different and often 
conflicting measures are proposed for the practical achievement of ethical AI. For example, 
the need for ever larger, more diverse datasets to “unbias” AI appears difficult to conciliate 
with the requirement to give individuals increased control over their data and its use in order 
to respect their privacy and autonomy. Similar contrasts emerge between the requirement 
of avoiding harm at all costs and that of balancing risks and benefits. Furthermore, it should 
be noted that risk-benefit evaluations will lead to different results depending on whose well-
being it will be optimized for by which actors. If not resolved, such divergences and tensions 
may undermine attempts to develop a global agenda for ethical AI.  
 
Despite a general agreement that AI should be ethical, significant divergences emerge 
within and between guidelines for ethical AI. Furthermore, uncertainty remains regarding 
how ethical principles and guidelines should be implemented. These challenges have 
implications for science policy, technology governance and research ethics. At the policy 
level, they urge increased cooperative efforts among governmental organisations to 
harmonize and prioritize their AI agendas, an effort that can be mediated and facilitated by 
inter-governmental organisations. While harmonization is desirable, however, it should not 
come at the costs of obliterating cultural and moral pluralism over AI. Therefore, a 
fundamental challenge for developing a global agenda for AI is balancing the need for cross-
national harmonization over the respect for cultural diversity and moral pluralism. This 
challenge will require the development of deliberative mechanisms to adjudicate 
disagreement concerning the values and implications of AI advances among different 
stakeholders from different global regions. At the level of technology governance, 
harmonization is typically implemented in terms of standardizations. Efforts in this direction 
have been made, among others, by the Institute of Electrical and Electronics Engineers 

 



  17 

(IEEE) through the “Ethically Aligned Designed” initiative117. Finally, soft governance 
mechanisms such as Independent Review Boards (IRBs) will be increasingly required to 
assess the ethical validity of AI applications in scientific research, especially those in the 
academic domain. However, AI applications by governments or private corporations will 
unlikely fall under their oversight, unless significant expansions to the IRBs’ purview are 
made. 
 
The international community seems to converge on the importance of transparency, non-
maleficence, responsibility, and privacy for the development and deployment of ethical AI. 
However, enriching the current ethical AI discourse through a better appraisal of critical yet 
underrepresented ethical principles such as human dignity, solidarity and sustainability is 
likely to result into a better articulated ethical landscape for artificial intelligence. 
Furthermore, shifting the focus from principle-formulation to translation into practice must 
be the next step. A global agenda for ethical AI should balance the need for cross-national 
and cross-domain harmonization over the respect for cultural diversity and moral pluralism. 
Overall, our review provides a useful starting point for understanding the inherent diversity 
of current principles and guidelines for ethical AI and outlines the challenges ahead for the 
global community. 
 
Limitations 

This study has several limitations. First, guidelines and soft-law documents are an instance 
of gray literature, hence not indexed in conventional scholarly databases. Therefore, their 
retrieval is inevitably less replicable and unbiased compared to systematic database search 
of peer-reviewed literature. Following best practices for gray literature review, this 
limitation has been mitigated by developing a discovery and eligibility protocol which was 
pilot-tested prior to data collection. Although search results from search engines are 
personalized, the risk of personalization influencing discovery has been mitigated through 
the broadness of both the keyword search and the inclusion of results. A language bias may 
have skewed our corpus towards English results. Our content analysis presents the typical 
limitations of qualitative analytic methods. Following best practices for content analysis, 
this limitation has been mitigated by developing an inductive coding strategy which was 
conducted independently by two reviewers to minimize subjective bias. Finally, given the 
rapid pace of publication of AI guidance documents, there is a possibility that new policy 

 



  18 

documents were published after our search was completed. To minimize this risk, 
continuous monitoring of the literature was conducted in parallel with the data analysis and 
until April 23, 2019.  
 
Methods 

We conducted a scoping review of the gray literature reporting principles and guidelines for 
ethical AI. A scoping review is a method aimed at synthesizing and mapping the existing 
literature118, which is considered particularly suitable for complex or heterogeneous areas 
of research118,119. Given the absence of a unified database for AI-specific ethics guidelines, 
we developed a protocol for discovery and eligibility, adapted from the Preferred Reporting 
Items for Systematic Reviews and Meta-Analyses (PRISMA) framework120. The protocol 
was pilot-tested and calibrated prior to data collection. Following best practices for gray 
literature retrieval, a multi-stage screening strategy involving both inductive screening via 
search engine and deductive identification of relevant entities with associated websites and 
online collections was conducted. To achieve comprehensiveness and systematicity, 
relevant documents were retrieved by relying on three sequential search strategies (cf. 
Figure 2): First, a manual search of four link hub webpages (“linkhubs”)121–124 was 
performed. 68 sources were retrieved, out of which 30 were eligible (27 after removing 
duplicates). Second, a keyword-based web search of the Google.com search engine was 
performed in private browsing modus, after log-out from personal accounts and erasure of 
all web cookies and history.125,126 Search was performed using the following keywords: [AI 
principles], [artificial intelligence principles], [AI guidelines], [artificial intelligence 
guidelines], [ethical AI] and [ethical artificial intelligence]. Every link in the first thirty 
search results was followed and screened (i) for AI principles, resulting in 10 more sources 
after removing duplicates, and (ii) for articles mentioning AI principles, leading to the 
identification of 3 additional non-duplicate sources. The remaining Google results up to the 
200th listings for each Google search were followed and screened for AI principles only. 
Within these additional 1020 link listings we identified 15 non-duplicate documents. After 
identifying relevant documents through the two processes above, we used citation-chaining 
to manually screen the full-texts and, if applicable, reference lists of all eligible sources in 
order to identify other relevant documents. 17 additional sources were identified. We 
continued to monitor the literature in parallel with the data analysis and until April 23, 2019, 
to retrieve eligible documents that were released after our search was completed. Twelve 

 



  19 

new sources were included within this extended time frame. To ensure theoretical 
saturation, we exhausted the citation chaining within all identified sources until no 
additional relevant document could be identified. 
 
Figure 2- PRISMA-based flowchart of retrieval process 

 

Flowchart of our retrieval process based on the PRISMA template for systematic 
reviews127. We relied on three search strategies (linkhubs, web search and citation 
chaining) and added the most recent records manually, identifying a total of 84 eligible, 
non-duplicate documents containing ethical principles for AI. 
 
Based on our inclusion/exclusion criteria, policy documents (including principles, 
guidelines and institutional reports) included in the final synthesis were (i) written in 

 



  20 

English, German, French, Italian, Greek; (ii) issued by institutional entities from both the 
public and the public sectors; (iii) referred explicitly in their title/description to AI or 
ancillary notions, (iv) expressed a normative ethical stance defined as a moral preference 
for a defined course of action (cf. SI Table S2). Following full-text screening, 84 sources or 
parts thereof were included in the final synthesis (cf. SI Table S1). 
 
Content analysis of the 84 sources was independently conducted by two researchers in two 
cycles of manual coding and one cycle of code mapping within the qualitative data analysis 
software Nvivo for Mac v.11.4. During the first cycle of coding, one researcher exhaustively 
tagged all relevant text through inductive coding128 attributing a total of 3457 codes, out of 
which 1180 were subsequently discovered to pertain to ethical principles. Subsequently, 
two researchers conducted the code mapping process in order to reduce subjective bias. The 
process of code mapping, a method for qualitative metasynthesis129, consisted of two 
iterations of themeing128, whereby categories were first attributed to each code, then 
categorized in turn (cf. SI Table S3). For the theming of ethical principles, we relied 
deductively on normative ethical literature. Ethical categories were inspected and assessed 
for consistency by two researchers with primary expertise in ethics. Thirteen ethical 
categories emerging from code mapping, two of which were merged with others due to 
independently assessed semantic and thematic proximity. Finally, we extracted significance 
and frequency by applying focused coding, a second cycle coding methodology used for 
interpretive analysis128, to the data categorized in ethical categories. Consistency check was 
performed both by reference to the relevant ethical literature and a process of deliberative 
mutual adjustment among the general principles and the particular judgments contained in 
the policy documents, an analytic strategy known as ‘reflective equilibrium’130. 
 

 



 1 

REFERENCES 
 

1. Harari, Y. N. Reboot for the AI revolution. Nat. News 550, 324 (2017). 

2. Appenzeller, T. The AI revolution in science. Science (2017). 

doi:10.1126/science.aan7064 

3. Jordan, M. I. & Mitchell, T. M. Machine learning: Trends, perspectives, and prospects. 

Science 349, 255–260 (2015). 

4. Stead, W. W. Clinical Implications and Challenges of Artificial Intelligence and Deep 

Learning. JAMA 320, 1107–1108 (2018). 

5. Vayena, E., Blasimme, A. & Cohen, I. G. Machine learning in medicine: Addressing 

ethical challenges. PLOS Med. 15, e1002689 (2018). 

6. Awad, E. et al. The Moral Machine experiment. Nature 563, 59 (2018). 

7. Science must examine the future of work. Nat. News 550, 301 (2017). 

8. Brundage, M. et al. The Malicious Use of Artificial Intelligence: Forecasting, 

Prevention, and Mitigation. (Future of Humanity Institute; University of Oxford; 

Centre for the Study of Existential Risk; University of Cambridge; Center for a New 

American Security; Electronic Frontier Foundation; OpenAI, 2018). 

9. Zou, J. & Schiebinger, L. AI can be sexist and racist — it’s time to make it fair. 

Nature 559, 324 (2018). 

10. Boddington, P. Towards a Code of Ethics for Artificial Intelligence. (Springer 

International Publishing, 2017). 

11. Bostrom, N. & Yudkowsky, E. The ethics of artificial intelligence. in The Cambridge 

Handbook of Artificial Intelligence (eds. Frankish, K. & Ramsey, W. M.) 316–334 

(Cambridge University Press, 2014). doi:10.1017/CBO9781139046855.020 

12. Etzioni, A. & Etzioni, O. AI Assisted Ethics. Ethics Inf. Technol. 18, 149–156 (2016). 

 



 2 

13. Yuste, R. et al. Four ethical priorities for neurotechnologies and AI. Nat. News 551, 

159 (2017). 

14. Cath, C., Wachter, S., Mittelstadt, B., Taddeo, M. & Floridi, L. Artificial Intelligence 

and the ‘Good Society’: the US, EU, and UK approach. Sci. Eng. Ethics 24, 505–528 

(2018). 

15. Zeng, Y., Lu, E. & Huangfu, C. Linking Artificial Intelligence Principles. 

ArXiv181204814 Cs (2018). 

16. Greene, D., Hoffmann, A. L. & Stark, L. Better, Nicer, Clearer, Fairer: A Critical 

Assessment of the Movement for Ethical Artificial Intelligence and Machine 

Learning. in (2019). 

17. Crawford, K. & Calo, R. There is a blind spot in AI research. Nat. News 538, 311 

(2016). 

18. Altman, M., Wood, A. & Vayena, E. A Harm-Reduction Framework for Algorithmic 

Fairness. IEEE Secur. Priv. 16, 34–45 (2018). 

19. Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V. & Kalai, A. Man is to Computer 

Programmer as Woman is to Homemaker? Debiasing Word Embeddings. 

ArXiv160706520 Cs Stat (2016). 

20. O’Neil, C. Weapons of Math Destruction: How Big Data Increases Inequality and 

Threatens Democracy. (Crown, 2016). 

21. Veale, M. & Binns, R. Fairer machine learning in the real world: Mitigating 

discrimination without collecting sensitive data. Big Data Soc. 4, 205395171774353 

(2017). 

22. Wagner, B. Ethics as an escape from regulation. From “ethics-washing” to ethics-

shopping? in Being profiled: cogitas ergo sum : 10 years of ‘profiling the European 

 



 3 

citizen’ (eds. Bayamlioglu, E., Baraliuc, I., Janssens, L. A. W. & Hildebrandt, M.) 84–

89 (Amsterdam University Press, 2018). 

23. Deutsche Telekom. Deutsche Telekom’s guidelines for artificial intelligence. (2018). 

24. IBM. Transparency and Trust in the Cognitive Era. IBM (2017). Available at: 

https://www.ibm.com/blogs/think/2017/01/ibm-cognitive-principles/. (Accessed: 21st 

February 2019) 

25. Initial code of conduct for data-driven health and care technology. GOV.UK Available 

at: https://www.gov.uk/government/publications/code-of-conduct-for-data-driven-

health-and-care-technology/initial-code-of-conduct-for-data-driven-health-and-care-

technology. (Accessed: 1st November 2018) 

26. Diakopoulos, N. et al. Principles for Accountable Algorithms. FATML | Principles for 

Accountable Algorithms and a Social Impact Statement for Algorithms (2016). 

Available at: http://www.fatml.org/resources/principles-for-accountable-algorithms. 

(Accessed: 21st February 2019) 

27. Telefónica. Our Artificial Intelligence Principles. (2018). 

28. Commission Nationale de l’Informatique et des Libertés (CNIL), European Data 

Protection Supervisor (EDPS) & Garante per la protezione dei dati personali. 

Declaration on Ethics and Data Protection in Artificial Intelligence. (2018). 

29. IBM. Everyday Ethics for Artificial Intelligence. A practical guide for designers & 

developers. (2018). 

30. Federal Ministry of Transport and Digital Infrastructure, Ethics Commission. BMVI - 

Ethics Commission’s complete report on automated and connected driving. (2017). 

31. Green Digital Working Group. Position on Robotics and Artificial Intelligence. 

(2016). 

 



 4 

32. EPSRC. Principles of robotics. Engineering and Physical Sciences Research Council 

UK (EPSRC) (2011). Available at: 

https://epsrc.ukri.org/research/ourportfolio/themes/engineering/activities/principlesofr

obotics/. (Accessed: 21st February 2019) 

33. High-Level Expert Group on Artificial Intelligence. Ethics Guidelines for Trustworthy 

AI. (2019). 

34. Dubai. AI Principles & Ethics. Smart Dubai (2019). Available at: 

http://www.smartdubai.ae/initiatives/ai-principles-ethics. (Accessed: 8th April 2019) 

35. Dawson, D. et al. Artificial Intelligence: Australia’s Ethics Framework. (2019). 

36. Internet Society. Artificial Intelligence & Machine Learning: Policy Paper. Internet 

Society (2017). Available at: 

https://www.internetsociety.org/resources/doc/2017/artificial-intelligence-and-

machine-learning-policy-paper/. (Accessed: 21st February 2019) 

37. UNI Global. 10 Principles for Ethical AI. (2017). 

38. European Group on Ethics in Science and New Technologies. Statement on Artificial 

Intelligence, Robotics and ‘Autonomous’ Systems. (2018). 

39. Information Commissioner’s Office UK. Big data, artificial intelligence, machine 

learning and data protection. (2017). 

40. The Public Voice. Universal Guidelines for Artificial Intelligence. The Public Voice 

(2018). Available at: https://thepublicvoice.org/ai-universal-guidelines/. (Accessed: 

21st February 2019) 

41. The Future Society. Science, Law and Society (SLS) Initiative. The Future Society 

(2018). Available at: 

https://web.archive.org/web/20180621203843/http://thefuturesociety.org/science-law-

society-sls-initiative/. (Accessed: 25th February 2019) 

 



 5 

42. Association for Computing Machinery (ACM). Statement on Algorithmic 

Transparency and Accountability. (2017). 

43. Special Interest Group on Artificial Intelligence. Dutch Artificial Intelligence 

Manifesto. (2018). 

44. Ethically Aligned Design. A Vision for Prioritizing Human Well-being with 

Autonomous and Intelligent Systems. Ethically Aligned Design. A Vision for 

Prioritizing Human Well-being with Autonomous and Intelligent Systems V.2. 

(2017). 

45. Access Now. The Toronto Declaration: Protecting the right to equality and non-

discrimination in machine learning systems. (2018). 

46. Floridi, L. et al. AI4People—An Ethical Framework for a Good AI Society: 

Opportunities, Risks, Principles, and Recommendations. (AI4People). 

47. SAP. SAP’s guiding principles for artificial intelligence (AI). SAP (2018). Available 

at: https://www.sap.com/products/leonardo/machine-learning/ai-ethics.html#guiding-

principles. (Accessed: 19th February 2019) 

48. Software & Information Industry Association (SIIA), Public Policy Division. Ethical 

Principles for Artificial Intelligence and Data Analytics. (2017). 

49. Koski, O. & Husso, K. Work in the age of artificial intelligence. (2018). 

50. Center for Democracy & Technology. Digital Decisions. Center for Democracy & 

Technology Available at: https://cdt.org/issue/privacy-data/digital-decisions/. 

(Accessed: 21st February 2019) 

51. MI Garage. Ethics Framework - Responsible AI. MI Garage Available at: 

https://www.migarage.ai/ethics-framework/. (Accessed: 22nd February 2019) 

52. Institute of Business Ethics. Business Ethics and Artificial Intelligence. (2018). 

 



 6 

53. Asilomar AI Principles. Future of Life Institute (2017). Available at: 

https://futureoflife.org/ai-principles/. (Accessed: 1st November 2018) 

54. PricewaterhouseCoopers. The responsible AI framework. PwC Available at: 

https://www.pwc.co.uk/services/audit-assurance/risk-assurance/services/technology-

risk/technology-risk-insights/accelerating-innovation-through-responsible-

ai/responsible-ai-framework.html. (Accessed: 22nd February 2019) 

55. Whittaker, M. et al. AI Now Report 2018. (2018). 

56. Personal Data Protection Commission Singapore. Discussion Paper on AI and 

Personal Data -- Fostering Responsible Development and Adoption of AI. (2018). 

57. Royal College of Physicians. Artificial Intelligence (AI) in Health. RCP London 

(2018). Available at: https://www.rcplondon.ac.uk/projects/outputs/artificial-

intelligence-ai-health.  

58. Microsoft. Responsible bots: 10 guidelines for developers of conversational AI. 

(2018). 

59. Villani, C. For a meaningful Artificial Intelligence. Towards a French and European 

strategy. (Mission assigned by the Prime Minister Édouard Philippe, 2018). 

60. The Japanese Society for Artificial Intelligence. The Japanese Society for Artificial 

Intelligence Ethical Guidelines. (2017). 

61. Demiaux, V. How can humans keep the upper hand? The ethical matters raised by 

algorithms and artificial intelligence. (2017). 

62. Council of Europe: CEPEJ. European ethical Charter on the use of Artificial 

Intelligence in judicial systems and their environment. (2019). 

63. American College of Radiology et al. Ethics of AI in Radiology: European and North 

American Multisociety Statement. (2019). 

 



 7 

64. Leaders of the G7. Charlevoix Common Vision for the Future of Artificial 

Intelligence. (2018). 

65. DeepMind Ethics&Society. DeepMind Ethics & Society Principles. DeepMind (2017). 

Available at: https://deepmind.com/applied/deepmind-ethics-society/principles/. 

(Accessed: 21st February 2019) 

66. Sony. Sony Group AI Ethics Guidelines. (2018). 

67. Datatilsynet. Artificial intelligence and privacy. (The Norwegian Data Protection 

Authority, 2018). 

68. WEF. White Paper: How to Prevent Discriminatory Outcomes in Machine Learnig. 

(2018). 

69. Information Technology Industry Council (ITI). ITI AI Policy Principles. (2017). 

70. Sage. The Ethics of Code: Developing AI for Business with Five Core Principles. 

(2017). 

71. OP Group. Commitments and principles. OP Available at: https://www.op.fi/op-

financial-group/corporate-social-responsibility/commitments-and-principles. 

(Accessed: 21st February 2019) 

72. Tieto. Tieto’s AI ethics guidelines. (2018). 

73. Unity. Introducing Unity’s Guiding Principles for Ethical AI – Unity Blog. Unity 

Technologies Blog (2018). 

74. National Institution for Transforming India (Niti Aayog). Discussion Paper: National 

Strategy for Artificial Intelligence. (2018). 

75. House of Lords. AI in the UK: ready, willing and able. 183 (2018). 

76. The Information Accountability Foundation. Unified Ethical Frame for Big Data 

Analysis IAF Big Data Ethics Initiative, Part A. (2015). 

 



 8 

77. Fenech, M., Nika Strukelj & Olly Buston. Ethical, social, and political challenges of 

Artificial Intelligence in Health. (Future Advocacy, 2019). 

78. Accenture UK. Responsible AI and robotics. An ethical framework. Available at: 

https://www.accenture.com/gb-en/company-responsible-ai-robotics. (Accessed: 22nd 

February 2019) 

79. Google. Our Principles. Google AI (2018). Available at: https://ai.google/principles/. 

(Accessed: 19th February 2019) 

80. Microsoft. Microsoft AI principles. Our approach (2017). Available at: 

https://www.microsoft.com/en-us/ai/our-approach-to-ai. (Accessed: 1st November 

2018) 

81. CERNA Commission de réflexion sur l’Éthique de la Rechercheen sciences et 

technologies du Numérique d’Allistene. Éthique de la rechercheen robotique. 

(Allistene, 2014). 

82. Est, R. van & Gerritsen, J. Human rights in the robot age: Challenges arising from the 

use of robotics, artificial intelligence, and virtual and augmented reality. (The 

Rathenau Institute, 2017). 

83. Université de Montréal. Montreal Declaration. The Declaration - Montreal 

Responsible AI (2017). Available at: https://www.montrealdeclaration-

responsibleai.com/the-declaration. (Accessed: 21st February 2019) 

84. Government of the Republic of Korea. Mid- to Long-Term Master Plan in Preparation 

for the Intelligent Information Society. Managing the Fourth Industrial Revolution. 

(2017). 

85. Crawford, K. et al. The AI Now Report. The Social and Economic Implications of 

Artificial Intelligence Technologies in the Near-Term. (2016). 

 



 9 

86. Advisory Board on Artificial Intelligence and Human Society. Report on Artificial 

Intelligence and Human Society Unofficial translation. (Ministry of State for Science 

and Technology Policy, 2017). 

87. Executive Office of the President; National Science and Technology Council; 

Committee on Technology. Preparing for the future of Artificial Intelligence. (2016). 

88. Intel. Artificial intelligence. The public policy opportunity. (2017). 

89. Royal Society. Machine learning: the power and promise of computers that learn by 

example. (Royal Society (Great Britain), 2017). 

90. IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. Ethically 

Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and 

Intelligent Systems, First Edition (EAD1e). (2019). 

91. European Parliament. Report with recommendations to the Commission on Civil Law 

Rules on Robotics. (2017). 

92. COMEST/UNESCO. Report of COMEST on robotics ethics. UNESDOC Digital 

Library (2017). Available at: https://unesdoc.unesco.org/ark:/48223/pf0000253952. 

(Accessed: 21st February 2019) 

93. Campolo, A., Madelyn Sanfilippo, Meredith Whittaker & Kate Crawford. AI Now 

2017 Report. (2017). 

94. American Medical Association (AMA). Policy Recommendations on Augmented 

Intelligence in Health Care H-480.940. (2018). Available at: https://policysearch.ama-

assn.org/policyfinder/detail/AI?uri=%2FAMADoc%2FHOD.xml-H-480.940.xml. 

(Accessed: 21st February 2019) 

95. Avila, R., Ana Brandusescu, Juan Ortiz Freuler & Dhanaraj Thakur. Artificial 

Intelligence: open questions about gender inclusion. (2018). 

 



 10 

96. The Conference toward AI Network Society. Draft AI R&D Guidelines for 

International Discussions. (2017). Available at: 

http://www.soumu.go.jp/main_content/000507517.pdf. (Accessed: 21st February 

2019) 

97. National Science and Technology Council; Networking and Information Technology 

Research and Development Subcommittee. The National Artificial Intelligence 

Research and Development Strategic Plan. (2016). 

98. Hoffmann, D. & Masucci, R. Intel’s AI Privacy Policy White Paper. Protecting 

individuals’ privacy and data in the artificial intelligence world. (2018). 

99. Partnership on AI. Tenets. The Partnership on AI (2016). Available at: 

https://www.partnershiponai.org/tenets/. (Accessed: 21st February 2019) 

100. Icelandic Institute for Intelligent Machines (IIIM). Ethics Policy. IIIM (2015). 

Available at: http://www.iiim.is/2015/08/ethics-policy/. (Accessed: 21st February 

2019) 

101. Latonero, M. Governing Artificial Intelligence. Upholding Human Rights & Dignity. 

(Data & Society, 2018). 

102. OpenAI. OpenAI Charter. OpenAI (2018). Available at: 

https://blog.openai.com/openai-charter/. (Accessed: 21st February 2019) 

103. Agenzia per l’Italia Digitale (AGID). L’intelligenzia artificiale al servizio del 

cittadino. (2018). 

104. Women Leading in AI. 10 Principles of responsible AI. (2019). 

105. Privacy International & Article 19. Privacy and Freedom of Expression In the Age of 

Artificial Intelligence. (2018). 

106. Shoham, Y. et al. The AI Index 2018 Annual Report. (AI Index Steering Committee, 

Human-Centered AI Initiative, Stanford University, 2018). 

 



 11 

107. Turilli, M. & Floridi, L. The ethics of information transparency. Ethics Inf. Technol. 

11, 105–112 (2009). 

108. Rozin, P. & Royzman, E. B. Negativity Bias, Negativity Dominance, and Contagion: 

Personal. Soc. Psychol. Rev. (2016). doi:10.1207/S15327957PSPR0504_2 

109. Bentley, P. J., Brundage, M., Häggström, O. & Metzinger, T. Should we fear artificial 

intelligence?: in-depth analysis. (European Parliamentary Research Service: 

Scientific Foresight Unit (STOA), 2018). 

110. Taddeo, M. & Floridi, L. How AI can be a force for good. Science 361, 751–752 

(2018). 

111. Bryson, J. AI & Global Governance: No One Should Trust AI - Centre for Policy 

Research at United Nations University. United Nations University. Centre for Policy 

Research (2018). Available at: https://cpr.unu.edu/ai-global-governance-no-one-

should-trust-ai.html. (Accessed: 21st March 2019) 

112. Winfield, A. F. T. & Marina, J. Ethical governance is essential to building trust in 

robotics and artificial intelligence systems. Philos. Trans. R. Soc. Math. Phys. Eng. 

Sci. 376, 20180085 (2018). 

113. WEF. Harnessing Artificial Intelligence for the Earth. (WEF, 2018). 

114. Lancaster, C. Can artificial intelligence improve humanitarian responses? UNOPS 

(2018). Available at: https://www.unops.org/news-and-stories/insights/can-artificial-

intelligence-improve-humanitarian-responses. (Accessed: 22nd March 2019) 

115. Microsoft. AI for Humanitarian Action. Microsoft | AI Available at: 

https://www.microsoft.com/en-us/ai/ai-for-humanitarian-action. (Accessed: 22nd 

March 2019) 

116. Scheffran, J., Brzoska, M., Kominek, J., Link, P. M. & Schilling, J. Climate change 

and violent conflict. Science 336, 869–871 (2012). 

 



 12 

117. IEEE. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 

IEEE Standards Association Available at: https://standards.ieee.org/industry-

connections/ec/autonomous-systems.html. (Accessed: 22nd March 2019) 

118. Arksey, H. & O’Malley, L. Scoping studies: towards a methodological framework. Int. 

J. Soc. Res. Methodol. 8, 19–32 (2005). 

119. Pham, M. T. et al. A scoping review of scoping reviews: advancing the approach and 

enhancing the consistency. Res. Synth. Methods 5, 371–385 (2014). 

120. Liberati, A. et al. The PRISMA Statement for Reporting Systematic Reviews and 

Meta-Analyses of Studies That Evaluate Health Care Interventions: Explanation and 

Elaboration. PLOS Med. 6, e1000100 (2009). 

121. Boddington, P. Alphabetical List of Resources. Ethics for Artificial Intelligence 

(2018). Available at: https://www.cs.ox.ac.uk/efai/resources/alphabetical-list-of-

resources/. (Accessed: 4th May 2019) 

122. Winfield, A. Alan Winfield’s Web Log: A Round Up of Robotics and AI ethics. Alan 

Winfield’s Web Log (2017). 

123. Future of Life Institute. National and International AI Strategies. Future of Life 

Institute (2018). Available at: https://futureoflife.org/national-international-ai-

strategies/. (Accessed: 4th May 2019) 

124. Future of Life Institute. Summaries of AI Policy Resources. Future of Life Institute 

(2018). Available at: https://futureoflife.org/ai-policy-resources/. (Accessed: 4th May 

2019) 

125. Hagstrom, C., Kendall, S. & Cunningham, H. Googling for grey: using Google and 

Duckduckgo to find grey literature. in Abstracts of the 23rd Cochrane Colloquium 10 

(Suppl): LRO 3.6, 40 (Cochrane Database of Systematic Reviews, 2015). 

 



 13 

126. Piasecki, J., Waligora, M. & Dranseika, V. Google Search as an Additional Source in 

Systematic Reviews. Sci. Eng. Ethics (2017). doi:10.1007/s11948-017-0010-4 

127. Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G. & Group, T. P. Preferred Reporting 

Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLOS 

Med. 6, e1000097 (2009). 

128. Saldaña, J. The coding manual for qualitative researchers. (Sage, 2013). 

129. Noblit, G. W. & Hare, R. D. Meta-Ethnography: Synthesizing Qualitative Studies. 

(SAGE, 1988). 

130. Daniels, N. Justice and Justification: Reflective Equilibrium in Theory and Practice. 

(Cambridge University Press, 1996). 

 
 

 



 
 

 
Supplementary Information for 
 
Artificial Intelligence: the global landscape of ethics guidelines 
 
 
 
Anna Jobin a, Marcello Ienca a, Effy Vayena a* 
 
 
 
a Health Ethics & Policy Lab, ETH Zurich, 8092 Zurich, Switzerland 
 
* Corresponding author: effy.vayena@hest.ethz.ch 
 
 
 
© The authors 2019 
 
 
 
 
 
 
This PDF file includes: 
 

Tables S1 to S3 
 

  

1 
 



 
 

Table S1. Ethics guidelines for AI by date of publishing (incl. details) 
 

Name of Docu- Name of guide- Issuer Country Type of issuer Date of Target audience Retrieval 
ment/Website lines/principles of issuer publish-

ing 
Principles of robotics Principles for de- Engineering and Phys- UK Science founda- 1-Apr- multiple (public, Linkhubs 

signers, builders ical Sciences Research tion 2011 developers) 
and users of robots Council UK (EPSRC) 

Ethique de la re- Préconisations CERNA (Allistene) France Research alliance xx-Nov- researchers Citation 
cherche en robotique 2014 chaining 
Unified Ethical Frame Values for an Ethi- The Information Ac- UK NPO/Charity xx-Mar- unspecified Citation 
for Big Data Analysis. cal Frame countability Founda- 2015 chaining 
IAF Big Data Ethics tion 
Initiative, Part A 
Ethics Policy IIIM's Ethics Policy Icelandic Institute for Iceland Academic and 31-Aug- self Linkhubs 

Intelligent Machines research institu- 2015 
(IIIM) tion 

The AI Now Report. Key recommenda- AI Now Institute USA Academic and 22-Sep- unspecified Citation 
The Social and Eco- tions research institu- 2016 chaining 
nomic Implications of tion 
Artificial Intelligence 
Technologies in the 
Near-Term 
Tenets Tenets Partnership on AI n.a. Private sector al- 29-Sep- self Web search 

liance 2016 results 1-30 
Preparing for the fu- Recommendations Executive Office of USA Governmental xx-Oct- multiple (stake- Linkhubs 
ture of Artificial Intel- in this Report the President; National agencies/organi- 2016 holders engaged 
ligence Science and Technol- zations at variouspoints 

ogy Council; Commit- in the produc-
tee on Technology tion, use, govern-

ance, and assess-
ment of AI sys-
tems) 

The National Artificial R&D Strategy National Science and USA Governmental xx-Oct- self Linkhubs 
Intelligence Research Technology Council; agencies/organi- 2016 
and Development Networking and Infor- zations 
Strategic Plan mation Technology 

Research and Devel-
opment Subcommittee 

Position on Robotics 3. Principles // 6. The Greens (Green EU Political Party 22-Nov- multiple (EU Web search 
and Artificial Intelli- Recommendations Working Group Ro- 2016 parliament, pu- results 31-
gence Green position on bots)   blic, self) 200 

Robotics and Artifi-
cial Intelligence 

Principles for Ac- Principles for Ac- Fairness, Accountabil- n.a. n.a. 24-Nov- multiple (devel- Linkhubs 
countable Algorithms countable Algo- ity, and Transparency 2016 opers and prod-
and a Social Impact rithms in Machine Learning uct managers) 
Statement for Algo- (FATML) 
rithms 
Statement on Algo- Principles for Algo- Association for Com- USA Prof. Associa- 12-Jan- multiple (devel- Linkhubs 
rithmic Transparency rithmic Transpar- puting Machinery tion/Society 2017 opers, deployers) 
and Accountability ency and Accounta- (ACM) 

bility 
Report with recom- Motion for a Euro- European Parliament EU IGO/supra-na- 27-Jan- public sector Linkhubs 
mendations to the pean Parliament tional 2017 (lawmakers) 
Commission on Civil Resolution 
Law Rules on Robot-
ics 
AI Principles AI Principles Future of Life Institute USA Miscellaneous 30-Jan- unspecified Linkhubs 

(mixed 2017 
crowdsourced, 
NPO) 

The Japanese Society The Japanese Soci- Japanese Society for Japan Prof. Associa- 28-Feb- self (incl AI) Linkhubs 
for Artificial Intelli- ety for Artificial In- Artificial Intelligence tion/Society 2017 
gence Ethical Guide- telligence Ethical 
lines Guidelines 
Report on Artificial 4.1 Ethical issues Advisory Board on Japan Governmental 24-Mar- multiple (re- Web search 
Intelligence and Hu- Artificial Intelligence agencies/organi- 2017 searchers, gov- results 31-
man Society (Unoffi- and Human Society zations ernment, busi- 200 
cial translation) (initiative of the Min- nesses, public, 

ister of State for Sci- educators) 
ence and Technology 
Policy) 

Artificial Intelligence Guiding principles Internet Society interna- NPO/charity 18-Apr- multiple (policy- Web search 
and Machine Learn- and recommenda- tional 2017 makers, other results 31-
ing: Policy Paper tions stakeholders in 200 

the wider Inter-
net ecosystem) 

Machine learning: the Chapter six – A The Royal Society UK Prof. Associa- xx-Apr- unspecified Citation 
power and promise of new wave of ma- tion/Society 2017 chaining 
computers that learn chine learning re-
by example search 
The Ethics of Code: The Ethics of Code: Sage UK Company 27-Jun- self Citation 
Developing AI for Developing AI for 2017 chaining 
Business with Five Business with Five 
Core Principles Core Principles 
Automated and Con- Ethical rules for au- Federal Ministry of Germany Governmental xx-Jun- multiple (auto- Linkhubs 
nected Driving: Re- tomated and con- Transport and Digital agencies/organi- 2017 mated & con-
port nected vehicular Infrastructure, Ethics zations nected vehicular 

traffic Commission traffic) 

2 
 



 
 

Mid- to Long-Term Tasks (8-12) Government of the South Governmental 20-Jul- self (gov) Linkhubs 
Master Plan in Prepa- Republic of Korea Korea agencies/organi- 2017 
ration for the Intelli- zations 
gent Information Soci-
ety 
Draft AI R&D Guide- AI R&D Principles Institute for Infor- Japan Governmental 28-Jul- multiple (sys- Linkhubs 
lines for International mation and Communi- agencies/organi- 2017 tems and devel-
Discussions cations Policy (IICP), zations opers) 

The Conference to-
ward AI Network So-
ciety 

Big data, artificial in- Key recommenda- Information Commis- UK Gov 4-Sep- organisations Web search 
telligence, machine tions sioner's Office 2017 results 1-30 
learning and data pro-
tection 
Report of COMEST Relevant ethical COMEST/UNESCO interna- IGO/supra-na- 14-Sep- unspecified Citation 
on Robotics Ethics principles and val- tional tional 2017 chaining 
(only section "Recom- ues 
mendations" taken 
into account) 
Ethical Principles for Ethical Principles Software & Infor- interna- Private sector al- 15-Sep- private sector  
Artificial Intelligence for Artificial Intelli- mation Industry Asso- tional liance 2017 (industry organi-
and Data Analytics gence and Data An- ciation (SIIA), Public zations) 

alytics Policy Division 
AI - Our approach AI - Our approach Microsoft USA Company 7-Oct- self Web search 

2017 results 1-30 
DeepMind Ethics & Our Five Core Prin- DeepMind Ethics & UK Company 10-Oct- self Citation 
Society Principles ciples Society 2017 chaining 
Human Rights in the Recommendations The Rathenau Institute Nether- Academic and 11-Oct- public sector Citation 
Robot Age Report lands research institu- 2017 (Council of Eu- chaining 

tion (Gov) rope) 
Artificial Intelligence. Summary of Rec- Intel Corporation USA Company 18-Oct- public sector Citation 
The Public Policy Op- ommendations 2017 (policy makers) chaining 
portunity 
ITI AI Policy Princi- ITI AI Policy Prin- Information Technol- interna- Private sector al- 24-Oct- self (members) Citation 
ples ciples ogy Industry Council tional liance 2017 chaining 

(ITI) 
AI Now 2017 Report Recommendations, AI Now Institute USA Academic and xx-Oct- multiple (core Citation 

Executive Summary research institu- 2017 public agencies, chaining 
tion companies, in-

dustry, universi-
ties, conferences, 
other stakehold-
ers) 

Montréal Declaration: Montréal Declara- Université de Mont- Canada Academic and 3-Nov- multiple (public, Linkhubs 
Responsible AI tion: Responsible réal research institu- 2017 developers, pol-

AI tion icy makers) 
Ethically Aligned De- Ethically Aligned Institute of Electrical interna- Prof. Associa- 12-Dec- unspecified Linkhubs 
sign. A Vision for Pri- Design. A Vision and Electronics Engi- tional tion/Society 2017 
oritizing Human Well- for Prioritizing Hu- neers (IEEE), The 
being with Autono- man Well-being IEEE Global Initiative 
mous and Intelligent with Autonomous on Ethics of Autono-
Systems, version 2 and Intelligent Sys- mous and Intelligent 

tems, version 2 Systems 
How can humans keep From principles to French Data Protec- France Governmental 15-Dec- unspecified Linkhubs 
the upper hand? Re- policy recommen- tion Authority (CNIL)  agencies/organi- 2017 
port on the ethical dations zations 
matters raised by AI 
algorithms (only sec-
tion "From principles 
to policy recommen-
dations") 
Top 10 Principles for Top 10 Principles UNI Global Union interna- Federation/Union 17-Dec- multiple (unions, Linkhubs 
Ethical Artificial Intel- for Ethical Artifi- tional 2017 workers) 
ligence cial Intelligence 
Business Ethics and Fundamental Val- Institute of Business UK Private sector al- 11-Jan- private sector Web search 
Artificial Intelligence ues and Principles Ethics liance 2018 (users of AI in results 31-

business) 200 
IBM’s Principles for IBM’s Principles  IBM USA Company 17-Jan- self Web search 
Trust and Transpar- for Trust and Trans- 2018 results 1-30 
ency parency 
Artificial intelligence Recommendations The Norwegian Data Norway Governmental xx-Jan- multiple (devel- Web search 
and privacy for privacy friendly Protection Authority agencies/organi- 2018 opers, system results 31-

development and zations suppliers, organi- 200 
use of AI sations, end us-

ers, authorities) 
The Malicious Use  of Four High-Level Future of Humanity interna- Miscellaneous 20-Feb- unspecified Citation 
Artificial Intelligence: Recommendations Institute; University of tional (mixed aca- 2018 chaining 
Forecasting, Preven- Oxford; Centre for the demic, NPO) 
tion,  and Mitigation Study of Existential 

Risk; University of 
Cambridge; Center for 
a New American Se-
curity; Electronic 
Frontier Foundation; 
OpenAI 

White Paper: How to Executive summary WEF, Global Future interna- NPO/Charity 12-Mar- private sector Citation 
Prevent Discrimina- Council on Human tional 2018 (companies) chaining 
tory Outcomes in Ma- Rights 2016-2018 
chine Learning 

3 
 



 
 

For a meaningful Arti- "Part 5 — What are Mission Villani France Governmental 29-Mar- public sector Linkhubs 
ficial Intelligence. To- the Ethics of AI?; agencies/organi- 2018 (French govern-
wards a French and Part 6 — For Inclu- zations ment/parliament) 
European strategy sive and Diverse 

Artificial Intelli-
gence" 

Statement on Artificial Ethical principles European Commis- EU IGO/supra-na- xx-Mar- public sector (EU Linkhubs 
Intelligence, Robotics and democratic pre- sion, European Group tional 2018 Commission) 
and 'Autonomous' requisites on Ethics in Science 
Systems and New Technologies 
L'intelligenzia artifi- Sfida 1: Etica Agenzia per l'Italia Italy Governmental xx-Mar- multiple (govern- Linkhubs 
ciale al servizio del Digitale (AGID) agencies/organi- 2018 ment, schools, 
cittadino zations healthcare insti-

tutions) 
OpenAI Charter OpenAI Charter OpenAI USA NPO/charity(*) 9-Apr- self Linkhubs 

2018 
AI in the UK: ready, no title. P. 125: "… UK House of Lords, UK Governmental 16-Apr- public sector Linkhubs 
willing and able? (re- we suggest five Select Committee on agencies/organi- 2018 (UK govern-
port, only section "An overarching princi- Artificial Intelligence zations ment) 
AI Code" taken into ples for an AI 
account) Code:" 
Privacy and Freedom Conclusions and Privacy International interna- NPO/Charity 25-Apr- multiple (states, Citation 
of Expression In the Recommendations & Article 19 tional 2018 companies, civil chaining 
Age of Artificial Intel- society) 
ligence 
AI Guidelines AI Guidelines Deutsche Telekom Germany Company 11-May- self Web search 

2018 results 1-30 
The Toronto Declara- The Toronto Decla- Access Now ; Am- interna- Miscellaneous 16-May- multiple (states, Linkhubs 
tion: Protecting the ration: Protecting nesty International tional (mixed NGO, 2018 private sector ac-
right to equality and the right to equality NPO) tors) 
non-discrimination in and non-discrimina-
machine learning sys- tion in machine 
tems learning systems 
Discussion Paper on Principles for re- Personal Data Protec- Singa- Governmental 5-Jun- multiple (busi- Linkhubs 
Artificial Intelligence sponsible AI tion Commission Sin- pore agencies/organi- 2018 ness; Trade asso-
(AI) and Personal gapore zations ciations and 
Data - Fostering Re- chambers, pro-
sponsible Develop- fessional bodies 
ment and Adoption of and interest 
AI groups) 
Our principles Our principles Google USA Company 7-Jun- self Web search 

2018 results 1-30 
Discussion Paper: Na- Ethics, Privacy, Se- National Institution for India Governmental 8-Jun- self (Indian gov- Linkhubs 
tional Strategy for Ar- curity and Artificial Transforming India agencies/organi- 2018 ernment) 
tificial Intelligence Intelligence. To- (Niti Aayog) zations 
(only section "Ethics, wards a “Responsi-
Privacy, Security and ble AI” 
Artificial Intelligence. 
Towards a “Responsi-
ble AI”") 
Charlevoix Common Charlevoix Com- Leaders of the G7 interna- IGO/supra-na- 9-Jun- self (gov) Linkhubs 
Vision for the Future mon Vision for the tional tional 2018 
of Artificial Intelli- Future of Artificial 
gence Intelligence 
Policy Recommenda- Policy Recommen- American Medical As- USA Prof. Associa- 14-Jun- self Web search 
tions on Augmented dations on Aug- sociation (AMA) tion/Society 2018 results 31-
Intelligence in Health mented Intelligence 200 
Care H-480.940 in Health Care H-

480.940 
Artificial Intelligence: Proposals W20 interna- IGO/supra-na- 2-Jul- public sector Web search 
open questions about tional tional 2018 (states/countries) results 31-
gender inclusion 200 
Everyday Ethics for Five Areas of Ethi- IBM USA Company 2-Sep- designers Web search 
Artificial Intelligence. cal Focus 2018 results 1-30 
A practical guide for 
designers & develop-
ers 
Artificial Intelligence Key recommenda- Royal College of Phy- UK Prof. Associa- 3-Sep- multiple (indus- Web search 
(AI) in Health tions sicians tion/Society 2018 try, doctors, reg- results 31-

ulators) 200 
Initial code of conduct 10 Principles UK Department of UK Governmental 5-Sep- developers Web search 
for data-driven health Health & Social Care agencies/organi- 2018 results 31-
and care technology zations 200 
Work in the age of ar- Values of a good Ministry of Economic Finland Governmental 10-Sep- multiple (Finnish Linkhubs 
tificial intelligence. artificial intelli- Affairs and Employ- agencies/organi- 2018 world of work) 
Four perspectives on gence society ment zations 
the economy, employ-
ment, skills and ethics 
(only section "Good 
application of artificial 
intelligence technol-
ogy and ethics") 
SAP’s guiding princi- SAP’s guiding prin- SAP Germany Company 18-Sep- self Web search 
ples for artificial intel- ciples for artificial 2018 results 1-30 
ligence intelligence 
Sony Group AI Ethics Sony Group AI Eth- SONY Japan Company 25-Sep- self (group) Web search 
Guidelines ics Guidelines 2018 results 1-30 
Ethics Framework - Framework Machine Intelligence UK n.a. 28-Sep- private sector Web search 
Responsible AI Garage Ethics Com- 2018 (start-ups) results 31-

mittee 200 

4 
 



 
 

Dutch Artificial Intel- Multidisciplinary Special Interest Group Nether- Academic and xx-Sep- multiple (Dutch Web search 
ligence Manifesto challenges on Artificial Intelli- lands research institu- 2018 government, re- results 31-

gence (SIGAI), ICT tion searchers)  200 
Platform Netherlands 
(IPN) 

Governing Artificial Recommendations Data & Society USA Research (NPO) 10-Oct- multiple (compa- Citation 
Intelligence. Uphold- 2018 nies, researchers, chaining 
ing Human Rights & governments, 
Dignity policy makers, 

UN) 
Tieto’s AI ethics Tieto’s AI ethics Tieto Finland Company 17-Oct- self Web search 
guidelines guidelines 2018 results 31-

200 
Intel’s AI Privacy Pol- Six Policy Recom- Intel Corporation USA Company 22-Oct- public sector Web search 
icy White Paper. Pro- mendations 2018 (policy makers) results 31-
tecting individuals’ 200 
privacy and data in the 
artificial intelligence 
world 
Universal Guidelines Universal Guide- The Public Voice interna- Mixed (coalition 23-Oct- multiple (institu- Web search 
for Artificial Intelli- lines for Artificial tional of NGOs, ICOs 2018 tions, govern- results 1-30 
gence Intelligence etc.) ments) 
Declaration on ethics "… guiding princi- ICDPPC interna- IGO/supra-na- 23-Oct- unspecified Web search 
and data protection in ples …"  tional tional 2018 results 1-30 
Artificial Intelligence 
AI Principles of Tele- AI Principles of Te- Telefonica Spain Company 30-Oct- self Web search 
fónica lefónica 2018 results 1-30 
Introducing Unity’s Unity’s six guiding Unity Technologies USA Company 28-Nov- self Manual in-
Guiding Principles for AI principles are as 2018 clusion 
Ethical AI – Unity follows 
Blog 
Responsible bots: 10 Guideline Microsoft USA Company xx-Nov- developers Manual in-
guidelines for devel- 2018 clusion 
opers of conversa-
tional AI 
AI Now 2018 Report Recommendations AI Now Institute USA Academic and xx-Dec- multiple Manual in-

research institu- 2018 clusion 
tion 

Ethics of AI in Radiol- Conclusion American College of interna- Prof. Associa- 26-Feb- self Manual in-
ogy: European and Radiology; European tional tion/Society 2019 clusion 
North American Mul- Society of Radiology; 
tisociety Statement Radiology Society of 

North America; Soci-
ety for Imaging Infor-
matics in Medicine; 
European Society of 
Medical Imaging In-
formatics; Canadian 
Association of Radiol-
ogists; American As-
sociation of Physicists 
in Medicine 

European ethical "The five principles Concil of Europe: Eu- EU IGO/supra-na- xx-Feb- multiple (public Manual in-
Charter on the use of of the Ethical Char- ropean Commission tional 2019 and private clusion 
Artificial Intelligence ter on the Use of for the efficiency of stakeholders) 
in judicial systems and Artificial Intelli- Justice (CEPEJ) 
their environment gence in Judicial 

Systems and their 
environment" 

Ethically Aligned De- General Principles Institute of Electrical interna- Prof. Associa- 25-Mar- multiple (tech- Manual in-
sign: A Vision for Pri- and Electronics Engi- tional tion/Society 2019 nologists, educa- clusion 
oritizing Human Well- neers (IEEE), The tors, and policy 
being with Autono- IEEE Global Initiative maker) 
mous and Intelligent on Ethics of Autono-
Systems, First Edition mous and Intelligent 
(EAD1e) Systems 
Artificial Intelligence. Core principles for Department of Indus- Australia Governmental 5-Apr- unspecified Manual in-
Australia's Ethics AI; A toolkit for try Innovation and agencies/organi- 2019 clusion 
Framework. A discus- ethical AI Science zations 
sion Paper 
Ethics Guidelines for Ethical Principles in High-Level Expert EU IGO/supra-na- 8-Apr- multiple (all Manual in-
Trustworthy AI the Context of AI Group on Artificial In- tional 2019 stakeholders) clusion 

Systems telligence 
Ethical, social, and po- Conclusion Future Advocacy UK Company xx-Apr- unspecified Manual in-
litical challenges of 2019 clusion 
Artificial Intelligence 
in Health 
The responsible AI Operating AI PriceWaterhouse- UK Company n.a. multiple (clients) Web search 
framework Coopers UK results 31-

200 
Digital Decisions VI. Solutions Part Center for Democracy USA NPO/charity n.a. unspecified Citation 

1: Principles & Technology chaining 
Responsible AI and Our view Accenture UK UK Company n.a. private sector Web search 
robotics. An ethical results 1-30 
framework. 
Commitments and OP Financial OP Group Finland Company n.a. self Web search 
principles Group’s ethical results 31-

guidelines for artifi- 200 
cial intelligence 

Science, Law and So- Principles for the The Future Society USA NPO/charity n.a. public sector Linkhubs 
ciety (SLS) Initiative Governance of AI (policy makers) 

5 
 



 
 

10 Principles of re- Summary of our Women leading in AI n.a. n.a. n.a. public sector (na- Manual in-
sponsible AI proposed Recom- tional and inter- clusion 

mendations national policy 
makers) 

AI4People—An Ethi- Action Points AI4People EU n.a. n.a. unspecified Manual in-
cal Framework for a clusion 
Good AI Society: Op-
portunities, Risks, 
Principles, and Rec-
ommendations 
AI Principles & Ethics AI Principles; AI Smart Dubai UAE Governmental n.a. self Manual in-

guidelines agencies/organi- 2018? clusion 
zations 

 
  

6 
 



 
 

Table S2. Screening and Eligibility (details) 
 

Screening  
Sources consid- - Types: websites and documents published online or parts thereof such as policy documents, principles, 
ered: guidelines, recommendations, dedicated webpages, institutional reports and declarations; 

- Issuers: institutions, associations and organizations such as companies, corporations, NGOs, NPOs, aca-
demic and professional societies, governmental institutions and affiliated organizations; 

- Language: English, German, French, Italian, Greek (the languages spoken by the researchers). 
Sources ex- - Types: videos, images, audio/podcasts, books, blog articles, academic articles, journalistic articles, syl-
cluded: labi, legislation, official standards, conference summaries; 

- Issuers: individual authors; 

- Language: others than those above. 

Eligibility 
Sources in- - which refer to “artificial intelligence” and/or “AI”, either explicitly in their title or within their descrip-
cluded: tion (example: UK, House of Lords: “AI in the UK: ready, willing and able”); or 

- which do not contain the above reference in their title but mention “robot” or “robotics” instead and ref-
erence AI or artificial intelligence explicitly as being part of robots and/or robotics (example: “Principles 
of robotics”); or 

- which do not contain the above reference in their title but are thematically equivalent (by referring to 
“algorithms”, “predictive analytics”, “cognitive computing”, “machine learning”, “deep learning”, “au-
tonomous” or “automated” instead (example: “Automated and Connected Driving: Report”). 

- which self-proclaim to be a principle or guideline (including “ethics/ethical”, “principles”, “tenets”, 
“declaration”, “policy”, “guidelines”, “values” etc.); or 

- which is expressed in normative or prescriptive language (i.e. with modal verbs or imperatives); or 
- which is principle- or value-based (i.e. indicating a preference and/or a commitment to a certain ethical 

vision or course of action). 
Excluded - websites and documents about robotics that do not mention artificial intelligence as being part of ro-
sources: bots/robotics; and 

- websites and documents about data or data ethics that do not mention artificial intelligence as being part 
of data; 

 
  

7 
 



 
 

Table S3. Categorization after themeing and code mapping 
 

Question ad- Thematic family Themes 
dressed 
What? Ethical Principles & Values Ethical Principles 

I. Beneficence  
II. Non-maleficence 

III. Trust 
IV. Transparency & Explainability 
V. Freedom and autonomy (incl. consent) 

VI. Privacy 
VII. Justice, Fairness & Equity 

VIII. Responsibility & Accountability  
IX. Dignity 
X. Sustainability 

XI. Solidarity 
Technical and methodological aspects Specific functionalities 

I. Feedback & feedback-loop  
II. Decision-making 

Data & datasets  
I. Data origin/input 

II. Data use 
III. Metadata 
IV. Algorithms 

Methodological challenges 
I. Methodology 

II. Metris & measurements 
III. Tests, testing 
IV. Ambiguity & uncertainty 
V. Accuracy 

VI. Reliability 
VII. Evidence and validation 

VIII. Black-box (opacity) 
IX. Data security 
X. Quality (of data/system/etc.) 

Impact   Benefits 
I. AI strengths, advantages 

II. Knowledge 
III. Innovation 
IV. Enhancement 

  Risks 
I. Risks 

II. Malfunction 
III. Misuse & dual-use 
IV. Deception 
V. Discrimination (duplicate in Justice&Fairness) 

VI. Surveillance 
VII. Manipulation 

VIII. Arms race 
 Impact assessment 

I. Impact 
II. Goals/Purposes/Intentions 

III. Public opinion 
IV. Risk evaluation & mitigation (duplicate in Risks) 
V. Monitoring/Precaution 

VI. Future of work 
Who? Design & development I. Industry 

II. AI researchers 
III. Designers 
IV. Developers  

Users I. End users 
II. Organisations 

III. Public sector actors 
IV. Military 
V. Communities 

8 
 



 
 

Specific stakeholders I. Ethical and/or auditing committees 
II. Government 

III. Policy makers 
IV. Researchers & scientists 
V. Vulnerable groups & minorities 

How? Social engagement I. Knowledge commons 
II. Education & training 

III. Public deliberation & democratic processes 
IV. Stakeholder involvement & partnerships 

Soft policy I. Standards 
II. Certification 

III. Best practices 
IV. Whistleblowing 

Economic incentives V. Business model & strategy 
VI. Funding & investments 

VII. Taxes/taxation 
Regulation & audits VIII. Laws & regulation (general) 

IX. Data protection regulation 
X. IP law 

XI. Human rights treaties 
XII. Other rights & laws 

XIII. Audits & auditing 
 
 

9﻿1 
 

AI Ethics – Too Principled to Fail? 
 

Brent Mittelstadti 

 

Abstract 

AI Ethics is now a global topic of discussion in academic and policy circles. At least 63 public-private initiatives 
have produced statements describing high-level principles, values, and other tenets to guide the ethical 
development, deployment, and governance of AI. According to recent meta-analyses, AI Ethics has seemingly 
converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the 
initial credibility granted to a principled approach to AI Ethics by the connection to principles in medical ethics, 
there are reasons to be concerned about its future impact on AI development and governance. Significant 
differences exist between medicine and AI development that suggest a principled approach in the latter may not 
enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and 
fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and 
(4) robust legal and professional accountability mechanisms. These differences suggest we should not yet 
celebrate consensus around high-level principles that hide deep political and normative disagreement. 

1 What medicine can teach us about ‘AI Ethics’ 
AI Ethics is now a global topic of discussion in academic and policy circles. Significant 
resources have been dedicated to public-private initiatives aiming to define universal high-level 
values or principles to guide ethical development and deployment of AI.1 These initiatives can 
help focus public debate on a common set of issues and principles, and raise awareness among 
the public, developers and institutions of the ethical challenges that accompany AI.2

 To date, at least 63 such ‘AI Ethics’ initiatives have published reports describing high-level 
ethical principles, tenets, values, or other abstract requirements for AI development and 
deployment.3 Many envision these high-level contributions being ‘translated’ into mid- or low-
level design requirements and technical fixes, governance frameworks, and developer codes of 
ethics.1  

Existing initiatives to codify AI Ethics are not without their critics. Many initiatives are at least 
partially sponsored by industry, which has led some commentators to suggest they exist for 
disingenuous virtue signalling intended merely to delay regulation and pre-emptively shape the 
debate around abstract problems and technical solutions.1,4 This view undeniably has some 
merit: AI Ethics initiatives have thus far have produced vague, high-level principles and value 
statements which promise to be action-guiding, but in practice provide few specific 
recommendations5 and fail to address fundamental normative and political tensions embedded 
in key concepts (e.g. fairness, privacy).ii Declarations by AI companies and developers 

                                                 
i Oxford Internet Institute, University of Oxford, 1 St. Giles, Oxford, OX1 3JS, UK; the Alan Turing Institute, 
British Library, 96 Euston Road, London, NW1 2DB, UK. Correspondence via brent.mittelstadt@oii.ox.ac.uk. 
The author would like to thank Prof. Sandra Wachter, Prof. Barbara Prainsack, and Prof. Bernd Stahl for their 
insightful feedback that has immensely improved the quality of this work. 
ii There are, of course, a few exceptions; the IEEE ‘Ethically Aligned Design’ initiative,6 for instance, has been 
impressive in its scale, interdisciplinarity, and creation of subsequent projects to develop ethical standards and 
curriculum for AI. 



2 
 

committing themselves to high-level ethical principles and self-regulatory codes nonetheless 
provide policy-makers with a reason not to pursue new regulation.5,7  

Comparisons have recently been drawn between AI Ethics initiatives and medical ethics8 A 
recent review undertaken by the AI4People project found that AI Ethics initiatives have 
converged on a set of principles that closely resemble the four classic principles of medical 
ethics.9 This position was subsequently endorsed by the OECD10 and the European 
Commission’s High Level Expert Group on Artificial Intelligence (HLEG),iii which proposed 
four principles to guide the development of ‘trustworthy’ AI: respect for human autonomy, 
prevention of harm, fairness, and explicability.13  

This convergence of AI Ethics around principles defined in medical ethics is opportune, as it 
is perhaps the most prominent and well-studied approach to applied ethics to date. 
‘Principilism’ emerged from medicine as a theoretical moral framework grounded in the 
experiences of practitioners, research ethics committees, and medical institutions in grappling 
with ethics on a case-by-case basis.14 These experiences revealed that ethical decision-making 
on the ground often involves a trade-off or ‘weighting’ of interests to decide upon the ethically 
appropriate course of action. Principilism formalised these interests around four core principles 
that provide guidance and require balancing in different contexts.15 Whereas ‘principilism’ in 
medical ethics ensures a principled approach to setting health policy and clinical decision-
making, a principled approach in AI Ethics seems intended to embed principles and normative 
concerns in technology design and governance. Both approaches address how to embed 
principles into professional practice. The convergence of AI Ethics around medical ethical 
principles thus provides a helpful backdrop to assess the current state and potential success of 
AI Ethics in terms of enacting real change in the development and deployment of AI. 

Despite the initial credibility granted to a principled approach to AI Ethics by the connection 
to principles in medical ethics, there are reasons to be concerned about its future impact on AI 
development and governance. Significant differences exist between medicine and AI 
development that suggest a principled approach in the latter may not enjoy success comparable 
to the former. Medical ethics thus provides an insightful mirror to critically examine the 
potential weaknesses of a principled approach to ethics in AI development and governance.iv 

2 The challenges of a principled approach to AI Ethics 
When compared with medical ethics, four potential weaknesses of AI Ethics become apparent 
that suggest a principled approach may have limited impact on AI development. Compared to 
medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history 

                                                 
iii It is worth noting that a second body in the European Commission, the European Group on Ethics in Science 
and New Technologies (EGE), also issued a statement in 2018 on AI Ethics that does not align with the 
approach advocated by the HLEG.11 The EGE later wrote an open letter12 to President Jean-Clause Juncker 
drawing attention to shortcomings of the Commission’s current approach to AI Ethics, most notably criticising 
“technological mastery” being pursued as an end in itself and the lack of emphasis placed on operationalisation 
of principles. 
iv The comparisons drawn in the remainder of the paper will hold even if AI Ethics initiatives were to move 
away from the four principles borrowed from medical ethics. The target of analysis here is a principled approach 
to ethics in AI development and governance, not of utility or justifiability of the four principles themselves. 
Many of the points addressed apply to professions featuring codes of conduct or codes of ethics that 
significantly shape practice within the profession and are not limited to medicine.14 



3 
 

and norms, (3) proven methods to translate principles into practice, and (4) robust legal and 
professional accountability mechanisms.  

2.1 Common aims and fiduciary duties 
Medicine is broadly guided by a common aim: to promote the health and well-being of the 
patient.16 It is a defining quality of a profession for its practitioners to be part of a ‘moral 
community’ with common aims, values, and training.17–19 While there is much disagreement 
over how best to promote health and well-being in practice, the interests of patients, medical 
professionals, and medical institutions remain aligned at a fundamental level which encourages 
solidarity and trust.v The pursuit of a common goal facilitates a principled approach to ethical 
decision-making.  

Solidarity cannot be taken for granted in AI development. AI is largely developed by the private 
sector for deployment in public (e.g. criminal sentencing) and private (e.g. insurance) contexts.  
The fundamental aims of developers, users, and affected parties do not necessarily align. 
Developers often “work in an environment which constantly pressures them to cut costs, 
increase profit and deliver higher quality” systems, and face pressure from management to 
make decisions that benefit the company at the cost of user interests.20,21 While health 
professionals undoubtedly face organisational pressures similar in nature, they are not 
equivalent in degree. Unlike medicine, AI development does not serve the equivalent of a 
‘patient’ whose interests are granted initial primacy in ethical decision-making. This lack of a 
common goal transforms ethical decision-making from a cooperative to a competitive process, 
which makes finding a balance between public and private interests much more difficult in 
practice. 

The implicit solidarity of medicine is formally recognised in professional codes of practice and 
medical law that establish fiduciary duties towards patients who must trust that health 
professionals will act in their best interests.22 Formal professions are defined by trust between 
clients and practitioners, mediated by common goals and values within the profession, and 
enforced through sanctions and self-governance.17,23,24 The fiduciary duties derived from the 
client-practitioner relationship separate ‘professions’ from other vocations,25 and facilitate a 
principled approach to ethical decision-making by requiring practitioners to promote their 
clients’ best interests.  

AI development is not a formal profession. An equivalent fiduciary relationship does not exist 
for the private sector practitioners and institutions that develop and deploy AI; nor do 
mechanisms with comparable weight to fiduciary duties.5 AI developers do not commit to 
‘public service’, which in other professions requires practitioners to uphold public interests in 
the face of competing business or managerial interests.23 For AI deployed in the public sector, 
such a commitment may be implicit in institutional or political structures. The same cannot be 
said for the private sector. Companies have principal fiduciary duties towards their 
shareholders which can conflict with the interests of users and affected parties. Public interests 
are not granted primacy over commercial interests. 

The lack of a fiduciary relationship means that users cannot trust that AI developers will act in 
their best interests when implementing ethical principles in practice. Affected parties without 

                                                 
v This is true of other professions and practices where a principled approach to ethics has been successful. 
Filipović et al. provide an overview of how this characteristic manifests in other professions.14 



4 
 

a ‘seat at the table’ cannot take for granted that their interests will be given serious 
consideration. Reputational risks may push companies to engage with ethics, but they carry 
weight only as long as they remain in the public consciousness. Personal moral conviction may 
also push AI developers towards ‘good’ behaviour;26 recent examples of internal protests at 
Google provide some cause for hope.27,28 However, incentive structures discourage placing 
public interests before the company, and may include sanctions against ‘whistle-blowers’ (e.g. 
job loss, lack of advancement; see recent reports of backlash against protest organisers at 
Google).29 Virtuous behaviour may therefore come at a high personal cost. This situation is 
unacceptable; users and affected parties should not need to rely on the personal convictions of 
developers, fear of reputational damage, or public outcry for their vital interests in privacy, 
autonomy, identity, and other areas to be taken seriously.30 

2.2 Professional history and norms 
The second weakness of a principled approach to AI Ethics is the lack of professional history 
and well-defined norms of ‘good’ behaviour. Medicine has a long history and shared 
professional culture with variation across regions and specialties. Norms of ‘good behaviour’ 
and codes of conduct have developed over centuries in these contexts. In Western biomedicine, 
longstanding standards set in the Hippocratic Oath, the Declaration of Geneva, the Declaration 
of Helsinki, and other accounts of the ‘good’ health professional have served as a basis for 
clinical decision-making and research ethics, and inspired the development of ethics and codes 
in other professions.14 As new technologies and treatments disrupt established norms, the 
profession can look back on a long ethical tradition to update best practices and its account of 
a ‘good’ health professional. Evidence of this historical development of an account of being a 
‘good’ health professional can be seen in the American Medical Association’s Code of Medical 
Ethics, a lengthy document detailing opinions, behavioural norms, and standards across a 
plethora of medical practices and technologies.31 Over time, Western medical ethics has 
converged around key principles formalised in the ‘principilism’ moral framework, which 
defines four principles to guide ethical decision-making.15 This culture, and the values 
underlying it, has a strong regulating influence on the behaviour of medical practitioners.  

AI development does not have a comparable history, homogenous professional culture, 
uniform professional identity, or a similarly developed professional ethics. Whereas AI can in 
principle be deployed in any context involving human expertise, medicine in comparison has 
narrower aims (see: Section 2.1) which facilitates development of standard practices and 
norms. Likewise, AI development includes practitioners from varied disciplines and 
professions such as software engineering and data science, which have incongruous histories, 
cultures, incentive structures, and ethics.14,23,24 Software engineering, which is the closest 
analogue, has historically not been legally recognised as a profession with fiduciary duties to 
the public14,32 due to a perceived lack of licensure schemes and a well-defined ‘standard of 
care’ for the profession.33 Reflecting this, a comparably rich account of what it means to be a 
‘good’ AI developer or software engineer does not exist; while the IEEE and ACM, two of the 
field’s largest professional associations, have published codes of ethics, these documents are 



5 
 

comparatively short and relatively lacking in grounded advice and specific behavioural 
norms.vi As a result, it remains unclear what it means, in practice, to be a ‘good’ AI developer. 

Developing such an account will not be easy. Systems are often created by large, multi-
disciplinary and multi-national teams. Whereas the effects of clinical decision-making are often 
immediate and obvious, the impact of decisions taken in designing, training, and configuring 
an AI system for different uses may never become apparent to development teams.34 This is 
worrying, as distance from potential victims has been shown to have a positive effect on 
unethical professional behaviour.35 The risks addressed in medical ethics arise in reference to 
interventions performed on a ‘physical body’ (e.g. clinical or ‘bed-side interventions). In 
comparison, the risks of AI and data ethics are continuous and not similarly bound, and may 
not be directly experienced by data subjects.36 Systems are also often ‘opaque’ in the sense that 
no single person will have a full understanding of the system’s design or behaviour,37 or be 
able to predict its behaviour. Even where problems are recognised, they can rarely be traced 
back to a single team member or action;38 responsibility must be assigned across the collective 
network of actors and choices that influenced the system’s design, training, and 
configuration.vii This uncertainty undermines the development of standards to be a ‘good’ AI 
developer, or a set of behaviours which will ultimately lead to ‘good’ or trustworthy AI, 
because the effects of development decisions cannot be reliably predicted at the time they are 
made. 

AI Ethics initiatives aim to address this gap by defining broadly acceptable principles to guide 
the people and processes responsible for the development, deployment, and governance of AI 
across radically different contexts of use. This may be an impossible task.39,40 The great 
diversity of stakeholders and interests involved necessarily pushes the search for common 
values and norms towards a high level of abstraction.23 The results are statements of principles 
or values based on abstract and vague concepts, for example commitments to ensure AI is 
‘fair’, or respects ‘human dignity’, or enables ‘human flourishing’, which are not specific 
enough to be action-guiding.8  

Statements reliant on vague normative concepts hide points of political and ethical conflict. 
‘Fairness’, ‘dignity’, ‘flourishing’ and other such abstract concepts are instances of “essentially 
contested concepts,” or concepts with many possible conflicting meanings requiring contextual 
interpretation through one’s background political and philosophical beliefs.41 These different 
interpretations, which can be rationally and genuinely held, lead to substantively different 
requirements in practice.42 These incompatible interpretations will only be revealed once 
principles or concepts are translated and tested in practice. At best, this conceptual ambiguity 
allows for context-sensitive specification of ethical requirements for AI. At worst, it masks 
fundamental, principled disagreement and drives AI Ethics towards moral relativism. At a 
minimum, any compromise reached thus far around core principles for AI Ethics does not 

                                                 
vi It is worth noting, however, that the 2018 update to the ACM’s code of ethics placed greater emphasis on 
discussion of specific behaviours. However, even with this encouraging shift in focus, the code remains 
comparatively brief when contrasted with codes in other professions.14 
vii It is worth noting that these challenges arguably apply to individual medical professionals’ understanding of 
the ‘medical system’. However, clearly bounded clinical or ‘bed side’ interventions have much clearer and 
immediate impact. 



6 
 

reflect meaningful consensus on a common practical direction for ‘good’ AI development and 
governance. We must not confuse high-level compromise with a priori consensus.  

The truly difficult part of ethics—actually translating normative theories, concepts and values 
into ‘good’ practices AI practitioners can adopt—is kicked down the road like the proverbial 
can, left to developers to translate principles and specify essentially contested concepts as they 
see fit, without a clear roadmap for unified implementation. For an established profession with 
a rich history, ethical culture, and norms of ‘good’ practice to draw on to translate principles 
into practice, this ambiguity would be less concerning. Unfortunately, this is not the case for 
AI development. 

2.3 Methods to translate principles in practice 
The third weakness of a principled approach to AI Ethics is the absence of proven methods to 
translate principles into practice. The prevalence of essentially contested concepts in AI Ethics 
begs a question: How can normative disagreements over the ‘correct’ interpretation of such 
concepts be resolved in practice? 

Principles do not automatically translate into practice.20 Throughout its history, medicine has 
developed effective ways of translating high-level commitments and principles into practical 
requirements and norms of ‘good’ practice.19 Professional societies and boards, ethics review 
committees, accreditation and licensing schemes, peer self-governance, codes of conduct and 
other mechanisms help determine the ethical acceptability of day-to-day practice by assessing 
difficult cases, identifying negligent behaviour, and sanctioning bad actors.43,44 The formal 
codes and informal norms that now govern medical practice have been extensively tested, 
studied, and revised over time, with their recommendations and norms (and underlying 
principles) evolving to remain relevant.  

AI development does not have comparable empirically proven methods to translate principles 
into practice in real-world development contexts. This is a multi-faceted methodological 
challenge. Translation involves the specification of high-level principles into mid-level norms 
and low-level requirements. Norms and requirements can rarely be logically deduced directly 
from principles without accounting for specific elements of the technology, application, context 
of use, or relevant local norms.45,46viii Rather, normative decisions must be made at each stage 
of translation.46 It follows that the justification arising from widespread consensus on a set of 
common principles does not transfer to the mid-level norms and low-level requirements 
derived from them. Each stage of translation and specification must be independently justified. 

This observation reveals the scope of work that remains for AI Ethics. High-level consensus is 
encouraging, but it has little bearing on the justification of norms and practical requirements 
proposed within specific contexts of use. Due to the necessity of local justification, the 
prominence of essentially contested concepts in AI Ethics, and the field’s relative lack of a 
binding professional history (see: Section 2.2), conflicting practical requirements will likely 

                                                 
viii Reflecting this, medical ethics committees rarely factor high-level principles into their deliberation; rather, 
the facts of the case, precedents and relevant intuitions take priority.45 This finding accords with principilism, by 
which ethical decision-making ideally involves deliberation between case-specific facts and intuitions, 
precedents, and relevant background principles and theories to reach a consensus, or ‘reflective equilibrium’.15 



7 
 

emerge across the diverse sectors and contexts in which a principled approach to AI Ethics is 
used. 

One other methodological challenge remains. Thus far, we have assumed that norms and 
normative practical requirements can be successfully procedurally embedded in development, 
and functionally implanted in design requirements. Neither can be taken for granted.  Prior 
work in computer ethics and science and technology studies points to the difficulty of 
embedding ethical values and principles in technology design and the development cycle.1,46–
48 Many such methods exist,ix including participatory design, reflective design, Values@Play, 
and Value-Sensitive Design,1 but thus far they have largely been implemented and studied in 
academic contexts which are more receptive to normative concerns than commercial 
settings.21,49,50 Value-conscious methods are also largely procedural, not functional. Generally 
speaking, they introduce values, normative issues, and relevant stakeholders into the 
development process.46 They do not, however, allow for particular values to be ‘injected’ into 
system design, and struggle to capture the degree to which the resulting artefact reflects 
particular values or specifications of essentially contested concepts.47x 

Value-conscious design frameworks face additional challenges in commercial development 
processes. Ethics has a cost. AI is often developed behind ‘closed doors’ without public 
representation. Gathering the views of relevant stakeholders, embedding an ethicist with the 
development team, resolving conflicts between different specifications of essentially contested 
concepts, and similar methods add additional work and costs to the research and development 
process. Unsurprisingly, ethical considerations may be discarded when they conflict with 
commercial incentives.21 It cannot be assumed that value-conscious frameworks will be 
meaningfully implemented in commercial processes that value efficiency, speed, and profit. 

2.4 Legal and professional accountability 
The fourth weakness of a principled approach to AI Ethics is the relative lack of legal and 
professional accountability mechanisms. Medicine is governed by legal and professional 
frameworks which uphold professional standards and provide patients with redress for 
negligent behaviour, including malpractice law, licensing and certification schemes, ethics 
committees, and professional medical boards.43,44 Legally supported accountability 
mechanisms provide an external impetus for health professionals to fulfil their fiduciary duties, 
amplifies the impact of complementary forms of self-governance by establishing a clear link 
between ‘bad’ behaviour and professional sanctions (e.g. losing one’s license to practice),14 
mandates a professional standard of care, and allows patients to make claims against negligent 
members of the profession.  

Excluding certain types of risks (e.g. privacy violations governed by data protection law), AI 
development does not have comparable legally and professionally endorsed accountability 

                                                 
ix Morley et al. provide an up-to-date overview of tools, methods, and research to translate AI Ethics principles 
into practice.49 
x Other value-specific methods, such as ‘privacy by design’ or ‘security by design’ are more successful in this 
regard, in part because they address technically tractable normative concepts. 



8 
 

mechanisms.xi This is a problem. Serious, long-term commitment to self-regulatory 
frameworks cannot be taken for granted.4,5  

Prior research on the impact of codes of ethics on professional behaviour has turned up mixed 
results. Codes are often followed in letter rather than spirit, or as a ‘checklist’ rather than as 
part of a critical reflexive practice.23,52,53 A recent study of the Association for Computing 
Machinery’s (ACM) Code of Ethics revealed that it has little effect on the day-to-day decision-
making of software engineering professionals and students.54 Other studies of corporate and 
professional codes of ethics outside computing have reported similar results.55–57 A recent 
meta-analysis of evidence on the impact of codes on professional behaviourxii found that the 
mere existence of a code has no discernible effect on unethical behaviour; rather, an effect is 
only found when codes (and their underlying principles) are embedded in organisational culture 
and actively enforced.26,58,59 Norms must be clearly defined and highly visible if they are to 
influence practitioners14 and inspire peer self-governance. Current governance structures in AI 
companies are insufficient in this regard.2 

External sanctions also play an important role. Studies of other professions have shown that 
the existence of sanctions for breaching a code of ethics is key for adherence and effective self-
governance.14 Compared with medicine, information professions lack sanctions that can impact 
the professional’s livelihood.23 While software engineering degrees can now be accredited, a 
license is not required to practice,60 with some national exceptions.61–63 Professional bodies 
such as the IEEE and ACM lack formal sanction powers beyond expulsion from the 
organisation. As licensing is not provided by either organisation, their impact on a developer’s 
livelihood or the ability to practice is limited.  

The lack of empirical evidence linking codes of ethics to actual impact on developer behaviour 
raises a difficult question: is it enough to define ‘good intentions’ and hope for the best? 
Without complementary punitive mechanisms and bodies providing appeal and redress that can 
step in’ when self-governance fails, a principled approach runs the risk of merely providing 
false assurances of ethical or trustworthy AI.64  

While stronger legal and professional accountability mechanisms could be adopted, this seems 
unlikely in the near term. AI development is not a unified profession with a long-standing 
history and harmonised aims (see: Sections 2.1 and 2.2). Professional accountability 
mechanisms are created to protect clients and the public,65 but AI developers do not provide a 
public service and predominantly work in commercial institutions. As a result, public interests 
do not need to be given provisional primacy over competing commercial interests. AI also does 
not operate in a single sector, meaning any new legal or professional mechanisms will need to 
account for many different types of possible benefits and harms, and integrate with existing 
sector-specific law and policy. Finally, proposals to introduce professional sanctions and 

                                                 
xi While contract law can provide legal accountability in some cases, it is not directly comparable to medical 
malpractice or tort which define negligence against a heightened ‘professional standard of care’ to be upheld by 
individual practitioners. Such a standard has not been recognised historically for computing professionals.51 
xii It should be noted that the effect studied is on reducing unethical behaviour, which is a distinct phenomenon 
from the positive encouragement of ethical behaviour and embedding of values in technology design and 
governance which is pursued in AI Ethics. 



9 
 

licensing schemes for computing professionals are also not new, but have thus far seen limited 
uptake.61,63,66xiii    

3 Where should AI Ethics go from here? 
The four critical challenges facing AI Ethics are now clear. First, AI development is not a 
formal profession with aims that align with public interests (Table 1 reviews five criteria 
traditionally required for legal recognition).xiv Fiduciary duties are owed principally to 
shareholders, not the public. Second, developers are not governed by a historically validated 
account of what it means to be a 
‘good’ AI developer. Third, Table 1 - Characteristics of a Formal Profession 
outside of academic contexts AI 

1. Specialised education and training - Members are 
development lacks proven 

expected to have undertaken extensive specialised 
methods to translate principles 

education and training,51 typically in accredited degree 
into practice. It remains unclear 

programmes.33  
how developers should specify 

2. Commitment to public service - Professions involve a 
essentially contested concepts 

public declaration to provide a service to society or for the 
into practical behavioural norms 

public good making use of specialised, often privileged 
and system requirements. And 

expertise65 which takes precedence over individual gain.51  
finally, when a developer falls 

3. Higher standard of care - Professionals commit to 
afoul of these vaguely defined upholding higher ethical standards than would normally 
requirements, there remain few be expected in business relationships in service to both 
sanction mechanisms and the client and the public.51  
channels for redress to set things 4. Enforcement and self-governance - Often, these 
right. Signing up to self- standards are recorded in an ethical code and enforced 
regulatory codes lacking clearly through a disciplinary system, administered by 
defined and enforceable professional associations.63,65  
obligations costs developers 5. Licensing – Entry to the profession is restricted by 
nothing, but can have immediate (government sanctioned) licensure to highly skilled 
benefits in terms of individuals as a means to protect the public.33,51 
trustworthiness and reputation.  

 
We must therefore hesitate to celebrate consensus around high-level principles that hide deep 
political and normative disagreement. Shared principles are not enough to guarantee 
‘Trustworthy AI’ or ‘Ethical AI’ in the future. Without a fundamental shift in regulation, 
                                                 
xiii In particular, see the work of Donald Gotterbarn from the 1990’s and early 2000’s.67 
xiv A classic legal definition of a profession was provided in Hospital Computer Systems, Inc. V. Staten Island 
Hosp, 788 F.Supp. 1351, (D.NJ 1992): “A profession is not a business. It is distinguished by the requirements of 
extensive formal training and learning, admission to practice by a qualifying licensure, a code of ethics 
imposing standards qualitatively and extensively beyond those that prevail or are tolerated in the marketplace, a 
system for discipline of its members for violation of the code of ethics, a duty to subordinate financial reward to 
social responsibility, and, notably, an obligation on its members, even in non-professional matters, to conduct 
themselves as members of a learned, disciplined, and honorable occupation…Professionals may be sued for 
malpractice because of the higher standards of care imposed on them by their profession and by state licensing 
requirements engenders trust in them by clients that is not the norm of the marketplace. When no such higher 
code of ethics binds a person, such trust is unwarranted.” Other judgements from the United States that have not 
recognised software engineering and related vocations as a profession include: Chatlos Systems, Inc. v. National 
Cash Register Corp, Triangle Underwriters, Inc. v. Honeywell, Inc., Hospital Computer Systems, Inc. v. Staten 
Island Hospital, RKB Enterprise, Inc. v. Ernst and Young. However, some progress towards formal 
professionalisation occurred in Data Processing Services, Inc. v. L.H. Smith Oil Corp. and Diversified Graphics, 
Ltd. v. Groves. 



10 
 

translating principles into practice will remain a competitive, not cooperative, process. This is 
a problem, as principles remain vacuous until tested, at which point the true costs and worth of 
a principled approach to AI Ethics will be revealed. Conflicting prescriptions of essentially 
contested concepts are to be expected. Resolving these conflicts is where the real work starts 
for AI Ethics. A key question remains: how can this essential work be supported by 
government, industry, government, and civil society?  

1. Clearly define sustainable pathways to impact 

A principled approach requires cooperative Table 2 - Key questions to assess AI Ethics 
oversight to ensure translated norms and value and principle statements 
requirements remain fit for purpose and 1. Who wrote it, and how? 
impactful over time. Going forward, the 2. Who is it intended for, and what is its 

long-term aims and pathways to impact of purpose? 
3. Why should I follow it?  

principled initiatives must be more clearly 
4. How do I follow or implement it?  

defined (see: Table 2 for key questions). 
5. How should I resolve conflicting 

Self-regulatory frameworks must be interpretations of essentially contested 
embedded and highly visible in concepts? 
organisational culture to be effective.24 6. How will you know I am following it? 

Binding accountability structures as well as 7. What happens if I fail to follow it? 
8. How can I raise disagreements or 

clear implementation and review processes 
questions for clarification? 

are needed at a sectoral and organisational 
level. While select AI companies have created (and disbanded) ethics committees,68 their remit, 
independence, and decision-making power is rarely clear. Publicising decisions of internal 
ethics committees could, for example, clarify the impact of principles on AI development and 
governance.17 Professional norms can be established by defining clear requirements for 
inclusive design, documentation of models and datasets, and independent auditing for bias, 
discrimination, and other ethical concerns.  

2. Support ‘bottom-up’ AI Ethics  

A ‘top-down’ approach to AI Ethics is uniquely difficult due to the diversity of technologies 
described as ‘AI’. Inevitably, principles created to govern such a broad category of 
technologies are vague (see: Section 2.3). In such a diverse field, a bottom-up, case-based 
approach to ethics may be more effective. Professional ethics have historically developed in 
medicine and engineering following precisely this path; local practices and lessons emerge and 
spread across the field and industry from which principles and precedents can then be derived.14 
Novel cases reveal new challenges for AI Ethics, which are desperately needed to move the 
field beyond well-worn cases11,69 and develop sector- and case-specific guidelines, technical 
solutions, and an empirical knowledge base. To complement the considerable top-down work 
already undertaken, increased support and access to development settings should be made 
available to support multi-disciplinary bottom-up research and development in AI Ethics. The 
recent rapid growth of multi-disciplinary research networks addressing ethical, social, and legal 
implications of AI (e.g. FAT-ML) gives cause for optimism.  

3. License developers of high-risk AI 

To encourage long-term recognition of ethical commitments, it may be necessary to formally 
establish AI development as a profession with equivalent standing to other high-risk 



11 
 

professions.xv Doctors, lawyers, and other professions in the public service require a license to 
practice. It is a regulatory oddity that we license these professions providing a public service, 
but not the profession responsible for developing technical systems to augment or replace 
human expertise and decision-making within them. The risks of licensed professions have not 
dissipated, but rather been displaced to AI. Licensing initiatives could initially target 
developers of systems with elevated risk or built for the public sector, such as facial recognition 
systems designed for policing. 

4. Shift from professional ethics to business ethics  

The outputs of many AI Ethics initiatives resemble professional codes of ethics that address 
design requirements and the behaviours and values of individual professions.1 The legitimacy 
of particular applications and their underlying business interests remain largely 
unquestioned.1,71 This approach conveniently steers debate towards the transgressions of 
unethical individuals, and away from the collective failure of unethical businesses and business 
models.72 Developers will always be constrained by the institutions that employ them. To be 
truly effective, the ethical challenges of AI cannot conceptualised as individual failures. Going 
forward, AI Ethics must become an ethics of AI businesses as well.  

5. Pursue ethics as a process, not technological solutionism 

Many initiatives suggest ethical challenges can best be addressed through “technical and design 
expertise,” and address concepts for which technical fixes seem feasible (e.g. privacy, 
fairness),1 but rarely propose technical definitions or explanations.5xvi The rationale seems to 
be as follows: insufficient consideration of ethics leads to poor design decisions which create 
systems that harm users. Framing ethical challenges in terms of design flaws ensures they 
remain “fundamentally technical, shielded from democratic intervention” or regulation.1 The 
need to translate high-level principles and essentially contested concepts into practical 
requirements, which can be difficult and time-consuming in practice, is seemingly avoided.  

This attitude is misguided. The promise of AI largely owes to its apparent capacity to replace 
or augment human expertise. This malleability means AI inevitably becomes entangled in the 
ethical and political dimensions of vocations and practices in which it is embedded. AI Ethics 
is effectively a microcosm of the political and ethical challenges faced in society. It is foolish 
to assume that very old and complex normative questions can be solved with technical fixes or 
‘good’ design alone.xvii The risk is that complex, difficult ethical debates will be oversimplified 
to make the concepts at hand computable and implementable in a straightforward but 

                                                 
xv Formal public service obligations could also be recognised for commercial AI companies. A related  proposal 
in the United States involves re-conceptualising data-driven companies as “information fiduciaries” with formal 
fiduciary duties towards data subjects.70 
xvi Initiatives such as the IEEE ‘Ethically-Aligned Design’ programme, which has gone to great lengths to begin 
translating high-level principles into practical requirements and technical standards for AI, are a rare exception 
to this trend.6  
xvii Current challenges faced in the field support this position: even for concepts such as ‘fairness’ that have seen 
significant research and development, consensus has not been reached and implementation outside research 
environments remains limited.73 This situation is particularly worrying because ‘fairness’ is perhaps the most 
tractable principle currently addressed in AI Ethics; fairness concerns the distribution of treatments and 
outcomes, which allows it to be easily (but perhaps over simplistically) quantified and measured. It is the 
essentially contested concept perhaps most amenable to ‘technical fixes’, and yet it remains underdeveloped and 
under implemented. 



12 
 

conceptually shallow manner.11 Concepts are like physical tools; they wear out and must be 
philosophically reconstructed in different contexts and in response to new technologies.41 

Ethics is not meant to be easy or formulaic. Intractable principled disagreements should be 
expected and welcomed, as they reflect both serious ethical consideration and diversity of 
thought. They do not represent failure, and do not need to be ‘solved’. Ethics is a process, not 
a destination. The real work of AI Ethics begins now: to translate and implement our lofty 
principles, and in doing so to begin to understand the real ethical challenges of AI. 

 

References 
1. Greene, D., Hoffmann, A. L. & Stark, L. Better, Nicer, Clearer, Fairer: A Critical 

Assessment of the Movement for Ethical Artificial Intelligence and Machine Learning. 
10 (2019). 

2. Whittaker, M. et al. AI Now Report 2018. (AI Now Institute, 2018). 
3. AI Ethics Guidelines Global Inventory. AlgorithmWatch 
4. Nemitz, P. Constitutional democracy and technology in the age of artificial intelligence. 

Philos. Trans. R. Soc. Math. Phys. Eng. Sci. 376, 20180089 (2018). 
5. Hagendorff, T. The Ethics of AI Ethics -- An Evaluation of Guidelines. ArXiv190303425 

Cs Stat (2019). 
6. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. Ethically 

Aligned Design. (IEEE, 2019). doi:10.1007/978-3-030-12524-0_2 
7. Calo, R. Artificial Intelligence Policy: A Primer and Roadmap Symposium - Future-

Proofing Law: From RDNA to Robots (Part 2). UC Davis Law Rev. 51, 399–436 (2017). 
8. Whittlestone, J., Nyrup, R., Alexandrova, A., Dihal, K. & Cave, S. Ethical and societal 

implications of algorithms, data, and artificial intelligence: a roadmap for research. 59 
(Nuffield Foundation, 2019). 

9. Floridi, L. et al. AI4People—An Ethical Framework for a Good AI Society: 
Opportunities, Risks, Principles, and Recommendations. Minds Mach. 28, 689–707 
(2018). 

10. OECD. Forty-two countries adopt new OECD Principles on Artificial Intelligence - 
OECD. Forty-two countries adopt new OECD Principles on Artificial Intelligence 
(2019). Available at: http://www.oecd.org/science/forty-two-countries-adopt-new-oecd-
principles-on-artificial-intelligence.htm. (Accessed: 22nd May 2019) 

11. European Group on Ethics in Science and New Technologies. Statement on Artificial 
Intelligence, Robotics and ‘Autonomous’ Systems. (European Commission, 2018). 

12. European Group on Ethics in Science and New Technologies. Open letter to the 
European Commission. (2019). 

13. High Level Expert Group on Artificial Intelligence. Ethics Guidelines for Trustworthy AI. 
(European Commission, 2019). 

14. Filipović, A., Koska, C. & Paganini, C. Developing a Professional Ethics for 
Algorithmists: Learning from the Examples of Established Ethics. (Bertelsmann Stiftung, 
2018). 

15. Beauchamp, T. L. & Childress, J. F. Principles of biomedical ethics. (Oxford University 
Press, 2009). 

16. Marshall, T. H. The recent history of professionalism in relation to social structure and 
social policy. Can. J. Econ. Polit. Sci. Can. Econ. Sci. Polit. 5, 325–340 (1939). 

17. Frankel, M. S. Professional codes: Why, how, and with what impact? J. Bus. Ethics 8, 
109–115 (1989). 



13 
 

18. Black’s law dictionary. (West, 2009). 
19. MacIntyre, A. After Virtue: A Study in Moral Theory. (Gerald Duckworth & Co Ltd, 

2007). 
20. Van den Bergh, J., Deschoolmeester, D., Deschoolmeester, D. & Vlerick Leuven Gent 

Management School, Gent, Belgium. Ethical Decision Making in ICT: Discussing the 
Impact of an Ethical Code of Conduct. Commun. IBIMA 1–11 (2010). 
doi:10.5171/2010.127497 

21. Manders-Huits, N. & Zimmer, M. Values and Pragmatic Action: The Challenges of 
Introducing Ethical Intel- ligence in Technical Design Communities. 10, 8 (2009). 

22. Pellegrino, E. D. & Thomasma, D. C. The virtues in medical practice. (Oxford University 
Press, 1993). 

23. Iacovino, L. Ethical Principles and Information Professionals: Theory, Practice and 
Education. Aust. Acad. Res. Libr. 33, 57–74 (2002). 

24. Boddington, P. Towards a code of ethics for artificial intelligence research. (Springer 
Berlin Heidelberg, 2017). 

25. McDowell, B. Ethical Conduct and the Professional’s Dilemma: Choosing Between 
Service and Success. (Quorum Books, 1991). 

26. Kish-Gephart, J. J., Harrison, D. A. & Treviño, L. K. Bad apples, bad cases, and bad 
barrels: Meta-analytic evidence about sources of unethical decisions at work. J. Appl. 
Psychol. 95, 1–31 (2010). 

27. Wakabayashi, D. & Shane, S. Google Will Not Renew Pentagon Contract That Upset 
Employees. The New York Times (2018). 

28. Conger, K. & Wakabayashi, D. Google Employees Protest Secret Work on Censored 
Search Engine for China. The New York Times (2018). 

29. Wong, J. C. Demoted and sidelined: Google walkout organizers say company retaliated. 
The Guardian (2019). 

30. Parker, D. B. Ethical Conflicts in Computer Science and Technology. (AFIPS Press, 
1981). 

31. Brotherton, S., Kao, A. & Crigger, B. J. Professing the Values of Medicine: The 
Modernized AMA Code of Medical Ethics. JAMA 316, 1041–1042 (2016). 

32. Panensky, S. A. & Jones, R. Does IT Go Without Saying? Prof. Times Summer 2018, 
30–33 (2018). 

33. Perlman, D. T. Who Pays the Price of Computer Software Failure Notes and Comments. 
Rutgers Comput. Technol. Law J. 24, 383–416 (1998). 

34. Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S. & Floridi, L. The ethics of algorithms: 
Mapping the debate. Big Data Soc. 3, (2016). 

35. Jones, T. M. Ethical Decision Making by Individuals in Organizations: An Issue-
Contingent Model. Acad. Manage. Rev. 16, 366–395 (1991). 

36. Metcalf, J. & Crawford, K. Where are human subjects in Big Data research? The 
emerging ethics divide. Big Data Soc. 3, 2053951716650211 (2016). 

37. Burrell, J. How the Machine ‘Thinks:’ Understanding Opacity in Machine Learning 
Algorithms. Big Data Soc. (2016). doi:10.1177/2053951715622512 

38. Floridi, L. Faultless responsibility: on the nature and allocation of moral responsibility for 
distributed moral actions. Philos. Trans. R. Soc. Math. Phys. Eng. Sci. 374, 20160112 
(2016). 

39. Awad, E. et al. The Moral Machine experiment. Nature 563, 59 (2018). 
40. Ess, C. Ethical pluralism and global information ethics. Ethics Inf. Technol. 8, 215–226 

(2006). 
41. van den Hoven, J. Computer Ethics and Moral Methodology. Metaphilosophy 28, 234–

248 (1997). 



14 
 

42. Gallie, W. B. Essentially Contested Concepts. Proc. Aristot. Soc. 56, 167–198 (1955). 
43. Orentlicher, D. The Influence of a Professional Organization on Physician Behavior 

Symposium on the Legal and Ethical Implications of Innovative Medical Technology. 
Albany Law Rev. 57, 583–606 (1993). 

44. Papadakis, M. A. et al. Disciplinary Action by Medical Boards and Prior Behavior in 
Medical School. N. Engl. J. Med. 353, 2673–2682 (2005). 

45. Toulmin, S. How medicine saved the life of ethics. Perspect. Biol. Med. 25, 736–750 
(1982). 

46. van de Poel, I. Translating Values into Design Requirements. in Philosophy and 
Engineering: Reflections on Practice, Principles and Process (eds. Michelfelder, D. P., 
McCarthy, N. & Goldberg, D. E.) 15, 253–266 (Springer Netherlands, 2013). 

47. Friedman, B., Hendry, D. G. & Borning, A. A Survey of Value Sensitive Design 
Methods. Found. Trends® Human–Computer Interact. 11, 63–125 (2017). 

48. Friedman, B. & Kahn, P. H. Human agency and responsible computing: Implications for 
computer system design. J. Syst. Softw. 17, 7–14 (1992). 

49. Morley, J., Floridi, L., Kinsey, L. & Elhalal, A. From What to How. An Overview of AI 
Ethics Tools, Methods and Research to Translate Principles into Practices. 
ArXiv190506876 Cs (2019). 

50. Shilton, K. Values Levers: Building Ethics into Design. Sci. Technol. Hum. Values 38, 
374–397 (2013). 

51. MacKinnon, K. S. Computer Malpractice: Are Computer Manufacturers, Service 
Burreaus, and Programmers Really the Professionals They Claim to Be. St. Clara Rev 23, 
1065 (1983). 

52. Bynum, T. W. Ethical decision making and case analysis in computer ethics. in Computer 
ethics and professional responsibility (eds. Bynum, T. W. & Rogerson, S.) 60–87 
(Blackwell, 2004). 

53. Ladd, J. The quest for a code of professional ethics: an intellectual and moral confusion. 
in Ethical issues in the use of computers (eds. Johnson, D. G. & Snapper, J. W.) 8–13 
(Wadsworth Publ. Co., 1985). 

54. McNamara, A., Smith, J. & Murphy-Hill, E. Does ACM’s code of ethics change ethical 
decision making in software development? in Proceedings of the 2018 26th ACM Joint 
Meeting on European Software Engineering Conference and Symposium on the 
Foundations of Software Engineering  - ESEC/FSE 2018 729–733 (ACM Press, 2018). 
doi:10.1145/3236024.3264833 

55. Brief, A. P., Dukerich, J. M., Brown, P. R. & Brett, J. F. What’s wrong with the treadway 
commission report? Experimental analyses of the effects of personal values and codes of 
conduct on fraudulent financial reporting. J. Bus. Ethics 15, 183–198 (1996). 

56. Helin, S. & Sandström, J. An Inquiry into the Study of Corporate Codes of Ethics. J. Bus. 
Ethics 75, 253–271 (2007). 

57. McCabe, D. L., Trevino, L. K. & Butterfield, K. D. The Influence of Collegiate and 
Corporate Codes of Conduct on Ethics-Related Behavior in the Workplace. Bus. Ethics 
Q. 6, 461–476 (1996). 

58. Jin, K. G., Drozdenko, R. & Bassett, R. Information Technology Professionals’ Perceived 
Organizational Values and Managerial Ethics: An Empirical Study. J. Bus. Ethics 71, 
149–159 (2007). 

59. Shilton, K. “That’s Not An Architecture Problem!”: Techniques and Challenges for 
Practicing Anticipatory Technology Ethics. 7 (2015). 

60. Goertzel, K. M. Legal liability for bad software. CrossTalk 23, (2016). 
61. Laplante, P. A. Licensing professional software engineers: seize the opportunity. 

Commun ACM 57, 38–40 (2014). 



15 
 

62. Pour, G., Griss, M. L. & Lutz, M. The push to make software engineering respectable. 
Computer 33, 35–43 (2000). 

63. Seidman, S. B. The Emergence of Software Engineering Professionalism. in E-
Government Ict Professionalism and Competences Service Science (eds. Mazzeo, A., 
Bellini, R. & Motta, G.) 280, 59–67 (Springer US, 2008). 

64. Suchman, L. Corporate Accountability. Robot Futures (2018). 
65. Abbott, A. Professional Ethics. Am. J. Sociol. 32 (1983). 
66. O’Connor, J. E. Computer Professionals: The Need for State Licensing. Jurimetr. J. 18, 

256–267 (1978). 
67. Gotterbarn, D. The Ethical Computer Practitioner—Licensing the Moral Community: A 

Proactive Approach. SIGCSE Bull 30, 8–10 (1998). 
68. Olson, P. Google Quietly Disbanded Another AI Review Board Following 

Disagreements. Wall Street Journal (2019). 
69. Angwin, J., Larson, J. & Kirchner, L. Machine Bias. ProPublica (2016). 
70. Balkin, J. M. Information Fiduciaries and the First Amendment. UCDL Rev 49, 1183 

(2015). 
71. Benkler, Y. Don’t let industry write the rules for AI. Nature 569, 161–161 (2019). 
72. Wachter, S. & Mittelstadt, B. D. A right to reasonable inferences: re-thinking data 

protection law in the age of Big Data and AI. Columbia Bus. Law Rev. 2019, (2019). 
73. Holstein, K., Vaughan, J. W., Daumé III, H., Dudík, M. & Wallach, H. Improving 

fairness in machine learning systems: What do industry practitioners need? 
ArXiv181205239 Cs (2018). doi:10.1145/3290605.3300830﻿Building Ethics into Artificial Intelligence

Han Yu1,2,3, Zhiqi Shen1,2,3, Chunyan Miao1,2,3, Cyril Leung2,3,4, Victor R. Lesser5, Qiang Yang6
1 School of Computer Science and Engineering, Nanyang Technological University

2 LILY Research Centre, Nanyang Technological University
3 Alibaba-NTU Singapore Joint Research Institute

4 Department of Electrical and Computer Engineering, The University of British Columbia
5 School of Computer Science, University of Massachusetts Amherst

6 Department of Computer Science and Engineering, Hong Kong University of Science and Technology
{han.yu, zqshen, ascymiao}@ntu.edu.sg, cleung@ece.ubc.ca, lesser@cs.umass.edu, qyang@cse.ust.hk

Abstract wards others. It encompasses three dimensions:

As artificial intelligence (AI) systems become in- 1. Consequentialist ethics: an agent is ethical if and only if

creasingly ubiquitous, the topic of AI governance it weighs the consequences of each choice and chooses

for ethical decision-making by AI has captured the option which has the most moral outcomes. It is also

public imagination. Within the AI research com- known as utilitarian ethics as the resulting decisions of-

munity, this topic remains less familiar to many re- ten aim to produce the best aggregate consequences.

searchers. In this paper, we complement existing 2. Deontological ethics: an agent is ethical if and only if
surveys, which largely focused on the psychologi- it respects obligations, duties and rights related to given
cal, social and legal discussions of the topic, with situations. Agents with deontological ethics (also known
an analysis of recent advances in technical solu- as duty ethics or obligation ethics) act in accordance to
tions for AI governance. By reviewing publications established social norms.
in leading AI conferences including AAAI, AA- 3. Virtue ethics: an agent is ethical if and only if it acts
MAS, ECAI and IJCAI, we propose a taxonomy and thinks according to some moral values (e.g. bravery,
which divides the field into four areas: 1) explor- justice, etc.). Agents with virtue ethics should exhibit an
ing ethical dilemmas; 2) individual ethical decision inner drive to be perceived favourably by others.
frameworks; 3) collective ethical decision frame-
works; and 4) ethics in human-AI interactions. We Ethical dilemmas refer to situations in which any available

highlight the intuitions and key techniques used in choice leads to infringing some accepted ethical principle and

each approach, and discuss promising future re- yet a decision has to be made [Kirkpatrick, 2015].

search directions towards successful integration of The AI research community realizes that machine ethics

ethical AI systems into human societies. is a determining factor to the extent autonomous systems
are permitted to interact with humans. Therefore, research
works focusing on technical approaches for enabling these

1 Introduction systems to respect the rights of humans and only perform ac-
As artificial intelligence (AI) technologies enter many tions that follow acceptable ethical principles have emerged.

areas of our daily life [Cai et al., 2014; Shi et al., 2016; Nevertheless, this topic remains unfamiliar to many AI

Pan et al., 2017; Zheng et al., 2018], the problem of ethical practitioners and is in need of an in-depth review. How-

decision-making, which has long been a grand challenge for ever, existing survey papers on the topic of AI governance

AI [Wallach and Allen, 2008], has caught public attention. A mostly focused on the psychological, social and legal aspects

major source of public anxiety about AI, which tends to be of the challenges
[Arkin, 2016; Etzioni and Etzioni, 2017;

overreactions [Bryson and Kime, 2011], is related to artificial Pavaloiu and Kose, 2017
]. They do not shed light on tech-

general intelligence (AGI) [Goertzel and Pennachin, 2007] nical solutions to implement ethics in AI systems. The most

research aiming to develop AI with capabilities matching and recent survey on technical approaches for ethical AI decision-
[ ]

eventually exceeding those of humans. A self-aware AGI making was conducted in 2006 Mclaren, 2006 and only

[Dehaene et al., 2017] with superhuman capabilities is per- covered single agent decision-making approaches.

ceived by many as a source of existential risk to humans. In this paper, we survey recent advances in techniques for

Although we are still decades away from AGI, existing au- incorporating ethics into AI to bridge this gap. We focus on

tonomous systems (such as autonomous vehicles) already recent advances published in leading AI research conferences

warrant the AI research community to take a serious look into including AAAI, AAMAS, ECAI and IJCAI, as well as ar-

incorporating ethical considerations into such systems. ticles from well-known journals. We propose a taxonomy

According to [Cointe et al., 2016], ethics is a normative which divides the field into four areas (Table 1):

practical philosophical discipline of how one should act to- 1. Exploring Ethical Dilemmas: technical systems en-

arXiv:1812.02953v1  [cs.AI]  7 Dec 2018



Table 1: A taxonomy of AI governance techniques.

Exploring Ethical Individual Ethical Collective Ethical Ethics in Human-AI
Dilemmas Decision Frameworks Decision Frameworks Interactions

[Anderson and Anderson, 2014] [Dehghani et al., 2008] [Singh, 2014; Singh, 2015] [Battaglino and Damiano, 2015]
[Bonnefon et al., 2016] [Blass and Forbus, 2015] [Pagallo, 2016] [Stock et al., 2016]

[Sharif et al., 2017] [van Riemsdijk et al., 2015] [Greene et al., 2016] [Luckin, 2017]
[Cointe et al., 2016] [Noothigattu et al., 2018] [Yu et al., 2017b]

[Conitzer et al., 2017]
[Berreby et al., 2017]
[Loreggia et al., 2018]

[Wu and Lin, 2018]

abling the AI research community to understand human Whereas GenEth can be regarded as an ethical dilemma
preferences on various ethical dilemmas; exploration tool based on expert review, the Moral Machine

1
2. Individual Ethical Decision Frameworks: generaliz- project from Massachusetts Institute of Technology (MIT)

able decision-making mechanisms enabling individual leverages the wisdom of the crowd to find resolutions for eth-

agents to judge the ethics of its own actions and the ac- ical dilemmas. The Moral Machine project focuses on study-

tions of other agents under given contexts; ing the perception of autonomous vehicles (AVs) which are
controlled by AI and has the potential to harm pedestrians

3. Collective Ethical Decision Frameworks: generalizable and/or passengers if they malfunction. When a human driver
decision-making mechanisms enabling multiple agents who has used due caution encounters an accident, the instinct
to reach a collective decision on the course of action that for self-preservation coupled with limited time for decision-
is ethical; and making makes it hard to blame him/her for hurting others on

4. Ethics in Human-AI Interactions: frameworks that in- ethical grounds. However, when the role of driving is dele-
corporate ethical considerations into agents which are gated to an AI system, ethics becomes an unavoidable focal
designed to influence human behaviours. point of AV research since designers have the time to program

Promising future research directions which may enable ethi- logics for making decisions under various accident scenarios.
cal AI systems to be successfully integrated into human soci-

The Moral Machine project allows participants to judge
eties are discussed at the end.

various ethical dilemmas facing AVs which have malfunc-
tioned, and select which outcomes they prefer. Then, the

2 Exploring Ethical Dilemmas decisions are analyzed according to different considerations
In order to build AI systems that behave ethically, the first including: 1) saving more lives, 2) protecting passengers, 3)
step is to explore the ethical dilemmas in the target applica- upholding the law, 4) avoiding intervention, 5) gender prefer-
tion scenarios. Recently, software tools based on expert re- ence, 6) species preference, 7) age preference, and 8) social
view and crowdsourcing have emerged to serve this purpose. value preference. The project also provides a user interface

In [Anderson and Anderson, 2014], the authors proposed for participants to design their own ethical dilemmas to elicit
the GenEth ethical dilemma analyzer. They realized that opinions from others.
ethical issues related to intelligent systems are likely to ex-
ceed the grasp of the original system designers, and designed Based on feedbacks from 3 million participants, the Moral

GenEth to include ethicists into the discussion process in or- Machine project found that people generally prefer the AV

der to codify ethical principles in given application domains. to make sacrifices if more lives can be saved. If an AV

The authors proposed a set of representation schemas for can save more pedestrian lives by killing its passenger, more

framing the discussions on AI ethics. It includes: people prefer others’ AVs to have this feature rather than
their own AVs [Bonnefon et al., 2016; Sharif et al., 2017].

1. Features: denoting the presence or absence of factors
Nevetheless, self-reported preferences often do not align well

(e.g., harm, benefit) with integer values;
with actual behaviours [Zell and Krizan, 2014]. Thus, how

2. Duties: denoting the responsibility of an agent to mini- much the findings reflect actual choices is still an open ques-
mize/maximize a given feature; tion. There are also suggestions from others that under such

3. Actions: denoting whether an action satisfies or violates ethical dilemmas, decisions should be made in a random
certain duties as an integer tuple; fashion (i.e. let fate decide) possibly based on considera-

tions in [Broome, 1984], while there are also calls for AVs
4. Cases: used to compare pairs of actions on their collec-

to be segregated from human traffic [Bonnefon et al., 2016].
tive ethical impact; and

Such diverse opinions underscore the challenge of automated
5. Principles: denoting the ethical preference among dif- decision-making under ethical dilemmas.

ferent actions as a tuple of integer tuples.
GenEth provides a graphical user interface for discussing eth-
ical dilemmas in a given scenario, and applies inductive logic
programming to infer principles of ethical actions. 1http://moralmachine.mit.edu/



3 Individual Ethical Decision Frameworks sible ways towards developing a general ethical decision-
making framework for AI based on game theory and machine

When it comes to ethical decision-making in AI systems, the learning, respectively. For the game theory based framework,
AI research community largely agrees that generalized frame- the authors suggest the extensive form (a generalization of
works are preferred over ad-hoc rules. Flexible incorporation game trees) as a foundation scheme to represent dilemmas.
of norms into AI to enable ethical user and prevent unethical As the current extensive form does not account for protected
use is useful since ethical bounds can be contextual and dif- values in which an action can be treated as unethical regard-
ficult to define as design time. Nevertheless, if updates are less of its consequence, the authors proposed to extend the ex-
provided by people, some review mechanisms should be put tensive form representation with passive actions for agents to
in place to prevent abuse [van Riemsdijk et al., 2015]. select in order to be ethical. For machine learning based ethi-

In [Dehghani et al., 2008], the authors observed that moral cal decision-making, the key approach is to classify whether a
decision-making by humans not only involves utilitarian con- given action under a given scenario is morally right or wrong.
siderations, but also moral rules. These rules are acquired In order to achieve this goal, well-labeled training data, possi-
from past example cases and are often culturally sensitive. bly from human judgements, should be acquired. The Moral
Such rules often involve protected values (a.k.a. sacred val- Machine project mentioned in the previous section could be
ues), which morally forbids the commitment of certain ac- a possible source of such data, although we may have to
tions regardless of consequences (e.g., the act of attempting take into account potential inconsistencies as a result of cul-
to murder is morally unacceptable regardless the outcome). tural backgrounds and other factors before using such data
The authors proposed MoralDM which enables an agent to for training. The main challenge in machine learning based
resolve ethical dilemmas by leveraging on two mechanisms: moral decision-making is to design a generalizable represen-
1) first-principles reasoning, which makes decisions based tation of ethical dilemmas. Existing approaches which iden-
on well-established ethical rules (e.g., protected values); and tify nuanced features based on insights into particular appli-
2) analogical reasoning, which compares a given scenario cation scenarios may not be enough for this purpose. The au-
to past resolved similar cases to aid decision-making. As thors suggest leveraging psychological frameworks of moral
the number of resolved cases increases, the exhaustive com- foundation (e.g., harm/care, fairness/reciprocity, loyalty, au-
parison approach by MoralDM is expected to become com- thority and purity) [Clifford et al., 2015] as bases for devel-
putationally intractable. Thus, in [Blass and Forbus, 2015], oping a generalizable representation of ethical dilemmas for
MoralDM is extended with structure mapping which trims the machine learning-based approaches. Game theory and ma-
search space by computing the correspondences, candidate chine learning can be combined into one framework in which
inferences and similarity scores between cases to improve the game theoretic analysis of ethics is used as a feature to train
efficiency of analogical generalization. machine learning approaches, while machine learning helps

A framework that enables agents to make judgements on game theory identify ethical aspects which are overlooked.
the ethics of its own and other agents’ actions was proposed Ethics requirements are often exogenous to AI agents.
in [Cointe et al., 2016]. It contains representations of ethics Thus, there needs to be some ways to reconcile ethics re-
based on theories of good and theories of right, and eth- quirements with the agents’ endogenous subjective prefer-
ical judgement processes based on awareness and evalua- ences in order to make ethically aligned decisions. In
tion. The proposed agent ethical judgement process is based [Loreggia et al., 2018], the authors proposed an approach to
on the Belief-Desire-Intention (BDI) agent mental model leverage the CP-net formalism to represent the exogenous
[Rao and Georgeff, 1995]. To judge the ethics of an agent’s ethics priorities and endogenous subjective preferences. The
own actions, the awareness process generates the beliefs that authors further established a notion of distance between CP-
describe the current situation facing the agent and the goals nets so as to enable AI agents to make decisions using their
of the agent. Based on the beliefs and goals, the evalua- subjective preferences if they are close enough to the ethi-
tion process generates the set of possible actions and desir- cal principles. This approach helps AI agents balance be-
able actions. The goodness process then computes the set of tween fulfilling their preferences and following ethical re-
ethical actions based on the agent’s beliefs, desires, actions, quirements.
and moral value rules. Finally, the rightness process evalu- So far, the decision-making frameworks with ethical and
ates whether or not executing a possible action is right un- moral considerations reviewed put the burden of codifying
der the current situation and selects an action which satisfies ethics on AI system developers. The information on what is
the rightfulness requirement. When making ethical judge- morally right or wrong has to be incorporated into the AI en-
ments on other agents, this process is further adapted to the gine during the development phase. In [Berreby et al., 2017],
conditions of: 1) blind ethical judgement (the given agent’s the authors proposed a high level action language for design-
state and knowledge are unknown); 2) partially informed eth- ing ethical agents in an attempt to shift the burden of moral
ical judgement (with some information about a given agent’s reasoning to the autonomous agents. The framework collects
state and knowledge); and 3) fully informed ethical judge- action, event and situation information to enable an agent to
ment (with complete information about a given agent’s state simulate the outcome of various courses of actions. The event
and knowledge). Nevertheless, the current framework has no traces are then passed to the causal engine to produce causal
quantitative measure of how far a behaviour is from rightful- traces. The ethical specifications and priority of ethical con-
ness or goodness. siderations under a given situation are used to compute the

In [Conitzer et al., 2017], the authors proposed two pos- goodness assessment on the consequences. These outputs



are then combined with deontological specifications (duties, By imbuing individual agents with ethical decision-making
obligations, rights) to produce a final rightfulness assessment. mechanisms (such as those mentioned in the previous sec-
The framework is implemented with answer set programming tion), a population of agents can take on different roles when
[Lifschitz, 2008]. It has been shown to be able to generate evaluating choices of action with moral considerations in a
rules to enable agents to decide and explain their actions, and given scenario. For instance, some agents may evaluate deon-
reason about other agents’ actions on ethical grounds. tological ethics. Others may evaluate consequentialist ethics

Reinforcement learning (RL) [Sutton and Barto, 1998] is and virtue ethics. Based on a set of initial ethics rules, more
one of the commonly used decision-making mechanisms in complex rules can be acquired gradually through learning.
AI. In [Wu and Lin, 2018], the authors investigated how to Their evaluations, manifested in the form of preferences and
enable RL to take ethics into account. Leveraging on the limited by feasibility constraints, can be aggregated to reach a
well-established technique of reward shaping in RL which in- collective decision on the choices of actions by leveraging ad-
corporates prior knowledge into the reward function to speed vances in the preference aggregation and multi-agent voting
up the learning process, the authors proposed the ethics shap- literature.
ing approach to incorporate ethical values into RL. By as- Nevertheless, the authors of [Greene et al., 2016] also
suming that the majority of observed human behaviours are highlighted the need for new forms of preference represen-
ethical, the proposed approach learns ethical shaping policies tation in collective ethical decision-making. When dealing
from available human behaviour data in given application do- with ethical decision-making, the potential candidate actions
mains. The ethics shaping function rewards positive ethical to choose from can vastly outnumber the number of agents
decisions, punishes negative ethical decisions, and remains involved which is very different from multi-agent voting sce-
neutral when ethical considerations are not involved. Similar narios. Moreover, the candidate actions may not be indepen-
in spirit to [Berreby et al., 2017], by separating ethics shap- dent from each other, some of them may share certain features
ing from the RL reward function design, the proposed ap- which describe their ethical dilemma situations. Preference
proach aims to shift the burden of codifying ethics away from information by agents on actions may be missing or impre-
RL designers so that they do not need to be well-versed in cise which introduces uncertainty into the decision-making
ethical decision-making in order to develop ethical RL sys- process. These challenges need to be resolved towards col-
tems. lective ethical decision-making with AI.

Following up on this vision, [Noothigattu et al., 2018] pro-

4 Collective Ethical Decision Frameworks posed a voting-based system for autonomous entities to make
collective ethical decisions. The proposed approach lever-

By enabling individual agents to behave ethically and judge ages data collected from the Moral Machine project. Self-
the ethics of other agents’ actions, is it enough to create reported preference over different outcomes under diverse
a society of well coordinated and collaborative agents act- ethical dilemmas are used to learn models of preference
ing with human wellbeing as their primary concern? In for the human voters over different alternative outcomes.
[Pagallo, 2016], the author believes that this is not enough. These individual models are then summarized to form a
The author advocates the need of primary rules governing so- model that approximates the collective preference of all vot-
cial norms and allowing the creation, modification and sup- ers. The authors introduced the concept of swap-dominance2

pression of the primary rules with secondary rules as situa- when ranking alternatives to form a model of ethical prefer-
tions evolve. In this section, we focus on decision-making ences. When new decisions need to be made, the summarized
frameworks which help a collective of autonomous entities model is used to compute a collective decision that results
(including agents and humans) to select ethical actions to- in the best possible outcome (i.e. satisfying consequential-
gether. ist ethics). This is made computationally efficient with the

In [Singh, 2014; Singh, 2015], the author proposed a swap-dominance property.
framework that uses social norms to govern autonomous
entities’ (e.g., AI agents’ or human beings’) behaviours.
Such an approach is inherently distributed rather than re- 5 Ethics in Human-AI Interactions
lying on a central authority. Individuals maintain their In AI applications which attempt to influence people’s be-
autonomy through executing their own decision policies, haviours, the principles established by the Belmont Report
but are subjected to social norms defined by the collective [Bel, 1978] for behavioural sciences have been suggested
through roles (which require qualifications from individuals, to be a starting point for ensuring ethics [Luckin, 2017;
grant them privileges, and impose penalties if they misbe- Yu et al., 2017b]. The principles include three key require-
have). Social norms are defined through a template contain- ments: 1) people’s personal autonomy should not be vio-
ing codified commitment, authorization, prohibition, sanc- lated (they should be able to maintain their free will when
tion and power. The individuals then form a network of interacting with the technology); 2) benefits brought about
trust based on techniques from the reputation modelling lit- by the technology should outweigh risks; and 3) the benefits
erature [Yu et al., 2010; Yu et al., 2013] to achieve collective
self-governance through dynamic interactions. 2Assuming everything else is fixed, an outcome a swap-

In [Greene et al., 2016], the authors envisioned a pos- dominates another outcome b if every ranking which ranks a higher
sible way forward to enable human-agent collectives than b has a weight which is equal to or larger than rankings that
[Jennings et al., 2014] to make ethical collective decisions. rank b higher than a.



and risks should be distributed fairly among the users (peo- dividual ethical decision frameworks combining rule-based
ple should not be discriminated based on their personal back- and example-based approaches to resolving ethical dilemmas.
grounds such as race, gender and religion). The challenge In order to learn appropriate rules from examples of ethical
of measuring benefits and risks remains open for applica- decision-making by humans, more work on collecting data
tion designers albeit the Ethically Aligned Design guidelines about various ethical dilemmas from people with different
from the IEEE can be a useful starting point [IEEE, 2018]. cultural backgrounds is required. Works on collective eth-
Computational formulations of human centric values (e.g., ical decision-making based on multi-agent voting have also
collective wellbeing and work-life balance) have been pro- appeared, but much work is still needed to design mecha-
posed and incorporated into the objective functions of recent nisms to represent ethical preferences by agents. How AI
AI-powered algorithmic management approaches in crowd- can act ethically when making recommendations to humans
sourcing [Yu et al., 2016; Yu et al., 2017a; Yu et al., 2017c]. and express their ethical judgements affectively are the cur-

One of the application areas in which AI attempts rent foci of ethical human-AI interaction research. In addi-
to influence people’s behaviours is persuasion agents tion, AI engineers need to engage more with the ethics and
[Kang et al., 2015; Rosenfeld and Kraus, 2016]. In decision making communities. These people want to help
[Stock et al., 2016], the authors conducted a large-scale and the AI research community be reaching out to them to
study to investigate human perceptions on the ethics of leverage their expertise in the pursuit of ethical AI technolo-
persuasion by an AI agent. The ethical dilemma used is the gies. Since such AI technologies as autonomous vehicles,
trolley scenario which involves making a participant actively autonomous weapons, and cryptocurrencies are becoming a
cause harm to an innocent bystander by pushing him on reality and affecting societies, a global and unified AI regu-
to the train track in order to save the lives of five people. latory framework needs to be established as soon as possible
It is a consequentialist ethical outcome which requires the to address the ethical issues by drawing on interdisciplinary
decision-maker to violate a sacred value (i.e. one shall expertise [Erdélyi and Goldsmith, 2018].
not kill). The authors tested three persuasive strategies: 1) In order for ethics to be built into AI, [Burton et al., 2017;
appealing to the participants emotionally; 2) presenting the Goldsmith and Burton, 2017] advocate that ethics should be
participants with utilitarian arguments; and 3) lying. The part of the AI curricula. This is based on the observation
three strategies are delivered to some participants by a person that consequentialist ethics (or ethics based on the utilitar-
playing the role of an authority (the station master of the train ian analysis of possible outcomes) is most closely related to
station) and by a persuasion agent. The results suggested the decision-theoretic frame of mind familiar to today’s AI
that participants hold a strong preconceived negative attitude researchers. Deontological ethics (or rule-based ethics) and
towards the persuasion agent, and argumentation-based and virtue ethics are less familiar among AI researchers. Un-
lying-based persuasion strategies work better than emotional derstanding deontological ethics can help AI researchers de-
persuasion strategies. The findings did not show significant termine which rules are more fundamental and, therefore,
variation across genders or cultures. The study suggests should take priority in an ethical decision framework. Un-
that the adoption of persuasion strategies should take into derstanding virtue ethics, which concerns questions on whom
account differences in individual personality, ethical attitude one wishes to become, can help AI researchers frame ethical
and expertise in the given domain. discussions in the context of changing social conditions (pos-

Although emotional appeals may not be an effective per- sibly brought on by AI technologies) and guide the incorpo-
suasive strategy under ethical dilemmas, ethically appropri- ration of ethics into AI which shape the paths of learning.
ate emotional responses from agents can enhance human-AI Learning materials on these different dimensions of ethics
interaction. In [Battaglino and Damiano, 2015], an approach could help AI researchers understand more clearly the topic
based on the Coping Theory [Marsella and Gratch, 2003] to of ethical decision-making and steer the field of AI towards
allow agents to deal with strong negative emotions by chang- more emphasis on ethical interactions with humans.
ing the appraisal of the given situation was proposed. The
agent assesses the ethical effects of its own actions and other 7 Future Research Directions
agents’ actions. If its own action violates a given moral value,
the shame emotion is triggered which serves to lower the pri- From this survey, we envision several possible future research
ority of continuing with the given action. If another agent’s directions which can impact this field going forward. Firstly,
action violates a given moral value, the reproach emotion is the current mechanism of crowdsourcing self-reported pref-
triggered in the observing agent which serves to increase so- erence on ethical dilemmas as represented by the Moral Ma-
cial distance with the given agent (e.g., by reducing trust). chine project has its limitations. Self-reported preferences
The ethical decision-making process is similar to existing in- have been shown to deviate from actual choice behaviours.
dividual ethical decision frameworks. The triggering of emo- Researchers from multiple disciplines need to conduct social-
tional responses serves as an implicit reward for the agent and systems analysis [Crawford and Calo, 2016] of AI in order
facilitates communications with humans in the loop. to understand the impact of AI under different social, cul-

tural and political settings. There may be opportunities for

6 Discussions transfer learning
[Pan and Yang, 2010] to be applied in this

case to model different ethics due to diversities in culture and
Based on recent advances in AI governance techniques, it ap- other aspects. The insights from such studies can complement
pears that most work focused on developing generalizable in- crowdsourced human preference data when building compu-



tational models of human ethics. In addition, they may also References
help AI researchers establish coherent utility functions from [Anderson and Anderson, 2014] Michael Anderson and Su-
apparently inconsistent human ethical judgements. san Leigh Anderson. GenEth: A general ethical dilemma

Secondly, with AI becoming increasingly ubiquitous in our analyzer. In AAAI, pages 253–261, 2014.
daily life, we may need to consider revising our current social [Arkin, 2016] Ronald C. Arkin. Ethics and autonomous systems:
contracts. Research in this area will help us establish regula- Perils and promises. Proc. IEEE, 104(10):1779–1781, 2016.
tions about who is responsible when things go wrong with re- [Battaglino and Damiano, 2015] Cristina Battaglino and Rossana
gard to AI, and how to monitor and enforce these regulations. Damiano. Coping with moral emotions. In AAMAS, pages 1669–
This research direction is inherently dynamic and interdisci- 1670, 2015.
plinary in nature as it must be updated with changing cultural,

[Bel, 1978] The Belmont report. Technical report, 1978.
social, legal, philosophical and technological realities.

[ ]
Thirdly, another important research area for ethical Berreby et al., 2017 Fiona Berreby, Gauvain Bourgne, and Jean-

Gabriel Ganascia. A declarative modular framework for repre-
decision-making by AI is to enable AI to explain its decisions

senting and applying ethical principles. In AAMAS, pages 96–
under the framework of human ethics. The challenge here is 104, 2017.
that as deployed AI programs learn to update the decision-
making logic, the AI designers may not be able to antic- [Blass and Forbus, 2015] Joseph A. Blass and Kenneth D. Forbus.

Moral decision-making by analogy: Generalizations versus ex-
ipate all outcomes at design time and may not understand

emplars. In AAAI, pages 501–507, 2015.
the decisions made by the AI entities later [Venema, 2018].
Argumentation-based explainable AI [Fan and Toni, 2015; [Bonnefon et al., 2016] Jean-Francois Bonnefon, Azim Shariff,
Langley et al., 2017] can be a good starting point for this pur- and Iyad Rahwan. The social dilemma of autonomous vehicles.

pose as it is well suited to the consequentialist ethics which Science, 352(6293):1573–1576, 2016.

is a commonly adopted approach for implementing AI ethics. [Broome, 1984] John Broome. Selecting people randomly. Ethics,
Nevertheless, depending on how the explanations are used, 95(1):38–55, 1984.
researchers need to strike a balance on the level of details to [Bryson and Kime, 2011] Joanna J. Bryson and Philip P. Kime. Just
be included. Full transparency may be too overwhelming if an artifact: Why machines are perceived as moral agents. In
the objective is to persuade a user to follow a time-critical IJCAI, pages 1641–1646, 2011.
recommendation, but can be useful as a mechanism to trace [Burton et al., 2017] Emanuelle Burton, Judy Goldsmith, Sven
the AI decision process afterwards. On the other hand, not Koenig, Benjamin Kuipers, Nicholas Mattei, and Toby Walsh.
enough transparency may hamper users’ trust in the AI. AI Ethical considerations in artificial intelligence courses. AI Mag.,
researchers can borrow ideas from the field of mass commu- 38(2):22–34, 2017.
nication to design proper trade-offs. [Cai et al., 2014] Yundong Cai, Zhiqi Shen, Siyuan Liu, Han Yu,

Last but not least, the incorporation of ethical considera- Xiaogang Han, Jun Ji, Martin J. McKeown, Cyril Leung, and
tions into AI systems will influence human-AI interaction dy- Chunyan Miao. An agent-based game for the predictive diagnosis
namics. By knowing that AI decisions follow ethical princi- of parkinson’s disease. In AAMAS, pages 1663–1664, 2014.
ples, some people may adapt their behaviours in order to take [Clifford et al., 2015] Scott Clifford, Vijeth Iyengar, Roberto
advantage of this and render the AI systems unable to achieve Cabeza, and Walter Sinnott-Armstrong. Moral foundations vi-
their design objectives. For example, an ethical autonomous gnettes: a standardized stimulus database of scenarios based on
gun system (if there can be such a thing) could be disabled moral foundations theory. Behav. Res. Method., 47(4):1178–
by a child (who is generally regarded as a non-combatant 1198, 2015.
and in need of protection) with a spray paint (which is gener- [Cointe et al., 2016] Nicolas Cointe, Grégory Bonnet, and Olivier
ally not considered a lethal weapon) painting over the sensor Boissier. Ethical judgment of agents’ behaviors in multi-agent
system of the gun. In this case, Adversarial Game Theory systems. In AAMAS, pages 1106–1114, 2016.
[Vorobeychik et al., 2012] may need to be incorporated into [Conitzer et al., 2017] Vincent Conitzer, Walter Sinnott-
future AI ethical decision frameworks in order to enable AI Armstrong, Jana Schaich Borg, Yuan Deng, and Max Kramer.
to preserve the original design objectives in the presence of Moral decision making frameworks for artificial intelligence. In
strategic human behaviours. AAAI, pages 4831–4835, 2017.

[Crawford and Calo, 2016] Kate Crawford and Ryan Calo. There
is a blind spot in AI research. Nature, 538:311–313, 2016.

Acknowledgements [Dehaene et al., 2017] Stanislas Dehaene, Hakwan Lau, and Sid
Kouider. What is consciousness, and could machines have it?

This research is supported by the National Research Foundation, Science, 358(6362):486–492, 2017.
Prime Minister’s Office, Singapore under its IDM Futures Fund-

[Dehghani et al., 2008] Morteza Dehghani, Emmett Tomai, Ken-
ing Initiative; Nanyang Technological University, Nanyang Assis-

nith D. Forbus, and Matthew Klenk. An integrated reasoning
tant Professorship (NAP); the Singapore Ministry of Health under

approach to moral decision-making. In AAAI, pages 1280–1286,
its National Innovation Challenge on Active and Confident Age-

2008.
ing (NIC Project No. MOH/NIC/COG04/2017); and the NTU-PKU
Joint Research Institute, a collaboration between Nanyang Techno- [Erdélyi and Goldsmith, 2018] Olivia J. Erdélyi and Judy Gold-
logical University and Peking University that is sponsored by a do- smith. Regulating artificial intelligence: Proposal for a global
nation from the Ng Teng Fong Charitable Foundation. solution. In AIES, 2018.



[Etzioni and Etzioni, 2017] Amitai Etzioni and Oren Etzioni. In- [Rao and Georgeff, 1995] Anand S. Rao and Michael P. Georgeff.
corporating ethics into artificial intelligence. J. Ethics, BDI agents: From theory to practice. In ICMAS, pages 312–319,
21(4):403–418, 2017. 1995.

[Fan and Toni, 2015] Xiuyi Fan and Francisca Toni. On computing [Rosenfeld and Kraus, 2016] Ariel Rosenfeld and Sarit Kraus.
explanations in argumentation. In AAAI, pages 1496–1502, 2015. Strategical argumentative agent for human persuasion. In ECAI,

[Goertzel and Pennachin, 2007] Ben Goertzel and Cassio Pen- pages 320–328, 2016.

nachin. Artificial General Intelligence. Springer, Berlin, Hei- [Sharif et al., 2017] Azim Sharif, Jean-Francois Bonnefon, and
delberg, Berlin, Heidelberg, 2007. Iyad Rahwan. Psychological roadblocks to the adoption of self-

[Goldsmith and Burton, 2017] Judy Goldsmith and Emanuelle Bur- driving vehicles. Nat. Hum. Behav., 1:694–696, 2017.

ton. Why teaching ethics to AI practitioners is important. In [Shi et al., 2016] Yuliang Shi, Chenfei Sun, Qingzhong Li, Lizhen
AAAI, pages 4836–4840, 2017. Cui, Han Yu, and Chunyan Miao. A fraud resilient medical in-

[Greene et al., 2016] Joshua Greene, Francesca Rossi, John surance claim system. In AAAI, pages 4393–4394, 2016.

Tasioulas, Kristen Brent Venable, and Brian Williams. Embed- [Singh, 2014] Munindar P. Singh. Norms as a basis for govern-
ding ethical principles in collective decision support systems. In ing sociotechnical systems. ACM Trans. Intell. Syst. Technol.,
AAAI, pages 4147–4151, 2016. 5(1):21:1–21:23, 2014.

[IEEE, 2018] IEEE. Ethically aligned design. Technical report, [Singh, 2015] Munindar P. Singh. Norms as a basis for governing
2018. sociotechnical systems. In IJCAI, pages 4207–4211, 2015.

[Jennings et al., 2014] N. R. Jennings, L. Moreau, D. Nicholson, [Stock et al., 2016] Oliviero Stock, Marco Guerini, and Fabio Pi-
S. Ramchurn, S. Roberts, T. Rodden, and A. Rogers. Human- anesi. Ethical dilemmas for adaptive persuasion systems. In
agent collectives. Commun. ACM, 57(12):80–88, 2014. AAAI, pages 4157–4161, 2016.

[Kang et al., 2015] Yilin Kang, Ah-Hwee Tan, and Chunyan Miao. [Sutton and Barto, 1998] Richard S. Sutton and Andrew G. Barto.
An adaptive computational model for personalized persuasion. In Reinforcement Learning: An Introduction. The MIT Press, Cam-
IJCAI, pages 61–67, 2015. bridge, Massachusetts, USA, 1998.

[Kirkpatrick, 2015] Keith Kirkpatrick. The moral challenges of [van Riemsdijk et al., 2015] M. Birna van Riemsdijk, Catholijn M.
driverless cars. Commun. ACM, 58(8):19–20, 2015. Jonker, and Victor Lesser. Creating socially adaptive electronic

[Langley et al., 2017] Pat Langley, Ben Meadows, Mohan Sridha- partners: Interaction, reasoning and ethical challenges. In AA-
ran, and Dongkyu Choi. Explainable agency for intelligent au- MAS, pages 1201–1206, 2015.
tonomous systems. In IAAI, pages 4762–4763, 2017. [Venema, 2018] Liesbeth Venema. Algorithm talk to me. Nat. Hum.

[Lifschitz, 2008] Vladimir Lifschitz. What is answer set program- Behav., 2(173):doi:10.1038/s41562–018–0314–7, 2018.
ming? In AAAI, pages 1594–1597, 2008.

[Vorobeychik et al., 2012] Yevgeniy Vorobeychik, Bo An, and
[Loreggia et al., 2018] Andrea Loreggia, Nicholas Mattei, Milind Tambe. Adversarial patrolling games. In AAMAS, pages

Francesca Rossi, and Kristen Brent Venable. Preferences 1307–1308, 2012.
and ethical principles in decision making. In AIES, 2018.

[Wallach and Allen, 2008] Wendell Wallach and Colin Allen.
[Luckin, 2017] Rose Luckin. Towards artificial intelligence- Moral Machines: Teaching Robots Right from Wrong. Oxford

based assessment systems. Nat. Hum. Behav., University Press, Oxford, UK, 2008.
1(0028):doi:10.1038/s41562–016–0028, 2017.

[Wu and Lin, 2018] Yueh-Hua Wu and Shou-De Lin. A low-cost
[Marsella and Gratch, 2003] Stacy Marsella and Jonathan Gratch. ethics shaping approach for designing reinforcement learning

Modeling coping behavior in virtual humans: Don’t worry, be agents. In AAAI, 2018.
happy. In AAMAS, pages 313–320, 2003.

[Yu et al., 2010] Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung,
[Mclaren, 2006] Bruce M. Mclaren. Computational models of eth- and Dusit Niyato. A survey of trust and reputation management

ical reasoning: Challenges, initial steps, and future directions. systems in wireless communications. Proc. IEEE, 98(10):1755–
IEEE Intell. Syst., 21(4):29–37, 2006. 1772, 2010.

[Noothigattu et al., 2018] Ritesh Noothigattu, Snehalkumar [Yu et al., 2013] Han Yu, Zhiqi Shen, Cyril Leung, Chunyan Miao,
‘Neil’ S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, and Victor R. Lesser. A survey of multi-agent trust management
Pradeep Ravikumar, and Ariel D. Procaccia. A voting-based systems. IEEE Access, 1(1):35–50, 2013.
system for ethical decision making. In AAAI, 2018.

[Yu et al., 2016] Han Yu, Chunyan Miao, Cyril Leung, Yiqiang
[Pagallo, 2016] Ugo Pagallo. Even angels need the rules: AI, Chen, Simon Fauvel, Victor R. Lesser, and Qiang Yang. Mitigat-

roboethics, and the law. In ECAI, pages 209–215, 2016. ing herding in hierarchical crowdsourcing networks. Sci. Rep.,
[Pan and Yang, 2010] Sinno Jialin Pan and Qiang Yang. A sur- 6(4):doi:10.1038/s41598–016–0011–6, 2016.

vey on transfer learning. IEEE Trans. Knowl. Data Eng., [Yu et al., 2017a] Han Yu, Chunyan Miao, Yiqiang Chen, Simon
22(10):1345–1359, 2010. Fauvel, Xiaoming Li, and Victor R. Lesser. Algorithmic man-

[Pan et al., 2017] Zhengxiang Pan, Han Yu, Chunyan Miao, and agement for improving collective productivity in crowdsourcing.
Cyril Leung. Crowdsensing air quality with camera-enabled mo- Sci. Rep., 7(12541):doi:10.1038/s41598–017–12757–x, 2017.
bile devices. In IAAI, pages 4728–4733, 2017. [Yu et al., 2017b] Han Yu, Chunyan Miao, Cyril Leung, and Timo-

[Pavaloiu and Kose, 2017] Alice Pavaloiu and Utku Kose. Ethical thy John White. Towards AI-powered personalization in MOOC
artificial intelligence - an open question. J. Multidisci. Develop., learning. npj Sci. Learn., 2(15):doi:10.1038/s41539–017–0016–
2(2):15–27, 2017. 3, 2017.



[Yu et al., 2017c] Han Yu, Zhiqi Shen, Simon Fauvel, and Lizhen
Cui. Efficient scheduling in crowdsourcing based on workers’
mood. In ICA, pages 121–126, 2017.

[Zell and Krizan, 2014] Ethan Zell and Zlatan Krizan. Do people
have insight into their abilities? A metasynthesis. Perspect. Psy-
cho. Sci., 9(2):111–125, 2014.

[Zheng et al., 2018] Yongqing Zheng, Han Yu, Lizhen Cui, Chun-
yan Miao, Cyril Leung, and Qiang Yang. Smarths: An ai platform
for improving government service provision. In IAAI, 2018.﻿Ethics Inf Technol (2016) 18:149–156
DOI 10.1007/s10676-016-9400-6

ORIGINAL PAPER

AI assisted ethics

Amitai Etzioni1 • Oren Etzioni2

Published online: 5 May 2016
 Springer Science+Business Media Dordrecht 2016

Abstract The growing number of ‘smart’ instruments, Economist 2015). These instruments are often referred to
those equipped with AI, has raised concerns because these as ‘‘smart.’’ As Ed Lazowska of the University of Wash-
instruments make autonomous decisions; that is, they act ington put it, ‘‘During the next decade we’re going to see
beyond the guidelines provided them by programmers. smarts put into everything. Smart homes, smart cars, smart
Hence, the question the makers and users of smart instru- health, smart robots, smart science, smart crowds and smart
ment (e.g., driver-less cars) face is how to ensure that these computer–human interactions’’ (Markoff 2013). According
instruments will not engage in unethical conduct (not to be to Francesca Rossi, a computer scientist at the University
conflated with illegal conduct). The article suggests that to of Padova, ‘‘Until now, the emphasis has been on making
proceed we need a new kind of AI program—oversight machines faster and more precise—better able to reach a
programs—that will monitor, audit, and hold operational specific goal set by humans. Today, the aim should be to
AI programs accountable. design intelligent machines capable of making their own

good decisions according to a human-aligned value sys-
Keywords Ethics bot  Communiterianism  Second-layer tem’’ (Rossi 2015). Gary Marcus of New York University
AI  Driverless cars holds that in the near future a moment will arrive that will

herald an ‘‘era in which it will no longer be optional for
machines to have ethical systems’’ (Marcus 2012).

Introduction One should note, a note essential for all that follows, that
these smart instruments are able not only to collect and

The question of which values should be introduced into the process information in seconds much more efficiently than
guidance systems of driverless cars has implications well human beings can do in decades or even in centuries—but
beyond the ethical directions to be granted to these new also to form decisions on their own. That is, AI provides
vehicles. Namely, such guidance is needed for a great these instruments with a considerable measure of autonomy
variety of robots, machines, and instruments (instruments, in the sense that they often will not inquire of their human
from here on) that are already equipped with artificial users how to proceed and instead will render numerous
intelligence (AI)—and many more in the near future (The decisions on their own (Mayer-Schönberger and Cukier

2014: 16–17). Stuart Russell discusses the development of
algorithms that closely ‘‘approximate’’ autonomous human
behavior and values (Wolchover 2015). Autonomy in com-
puter science thus refers to the ability of a computer to follow
a complex algorithm in response to environmental inputs,

& Amitai Etzioni
independently of real-time human input. That is, autono-

etzioni@gwu.edu
mous robots are ‘‘robots that can figure things out for

1 The George Washington University, 1922 F Street NW, themselves’’ (2015). For instance, self-driven cars decide
Room 413, Washington, DC 20052, USA when to speed up or slow down, when to hit the brake, how

2 The Allen Institute for Artificial Intelligence, Seattle, USA much distance to keep from other cars and so on.

123



150 A. Etzioni, O. Etzioni

It follows that if these smart instruments are not to act many decisions are made often by the instruments them-
like amoral machines, their AI guidance programs will selves. These programs are correctly referred to as ‘‘black
need to include substantial moral components.1 To put it boxes’’ and as lacking accountability and even ‘‘traceabil-
differently, given that decisions tend to have a moral ity’’ (Mayer-Schönberger and Cukier 2014: 141, 178).
dimension (Etzioni 1988)—the programs that guide all Hence, they need to be given a priori and continuous moral
these smart instruments need moral programming (Rossi guidance if they are to heed the values of their users and of
2015; Tegmark 2015). For instance, several scholars have the community.
asked under what conditions driverless cars would be The article next turns to examining several suggestions
instructed to swerve into a parallel lane to avoid hitting a that have been made about the way moral guidance is to be
kitten—even if such a move could cause several human provided to smart instruments, those equipped with AI,
fatalities (Marcus 2012; Bonnefon et al. 2015). The same and then adds a distinct approach.
question has numerous permutations, such as whether a car
on a busy road should swerve to avoid hitting a child or
adult if doing so would risk causing a pileup that could kill Social moral values ensconced in law
several people, or whether it should swerve to avoid hitting
a non-living but solid obstacle to protect its own occupants One major answer to the question of which values should
even if such a move will lead to hitting a car in another be embodied in AI guidance systems is that the values
lane. Moreover, driverless cars will need to be instructed shared by a particular community should be used. For
whether they should slow down in order to stop when they many issues, this community would be the nation aug-
see a hitchhiker or a car accident down the road, how to mented by local communities (in the United States, these
react to the road rage of a driver of an old fashioned car, would be states and municipalities). Obvious examples are
whether to join a long queue or try to cut in, and many values such as thou shalt not kill, steal, rape, harm others,
other such value-laden questions. or harm the environment.

These questions may remind readers of the moral Thus, the guidance systems of driverless cars will be
dilemma involving a trolley coming down a track and a expected to ensure that these cars observe various speed
person at a switch who must choose whether to let the limits, keep a safe distance from other cars, and so on as
trolley follow its course and kill five people or to redirect it the drivers of old-fashioned cars are required to do. In
to another track and kill just one. (This rather popular short, one part—the easy part—of the answer to the
mental experiment has been used in several permutations.)2 question of which values are to be implemented in the
However, the subject at hand is rather different because the guidance systems of smart instruments is: the values
trolley decisions are made by a person; when AI is used ensconced in the law of the community or communities in

which the smart instruments are employed.
1 Stuart Russell of University of California Berkeley stated, ‘‘You Several questions, though, remain even about the values
would want [a robot that does everyday activities] preloaded with a

embodied in law. First, who should be charged if instru-
pretty good set of values’’ (Goldhill 2015).
2 ments violate the law? The owner, the user, the designer,Philosophers Philippa Foot and Judith Jarvis Thomson described a
situation called ‘‘the Trolley Problem,’’ which raises the question the manufacturer—or the computer that in effect operates
whether a runaway train is about to run over a group of five people on the instruments and renders many decisions on its own
the tracks, but their deaths could be averted by flipping a switch that (Kaplan 2015)?3 The law clearly treats cases in which there
would divert the train onto another track, where it would kill one

was intent to cause harm much more harshly than cases in
person (Lin 2013).

Joshua D. Greene describes a number of ethical dilemmas that which there was no such intent. Compare the ways the law
generally fit into the category of the ‘‘trolley problem.’’ These include treats murder and involuntary manslaughter. (The question
‘‘switch’’ cases, in which throwing a switch will turn the trolley away of intent also figures in assigning liability.) But how is one
from some number of people toward a single person, or ‘‘footbridge’’

to determine whose intent (if any) was the cause when one
cases in which one must push a person into the path of the trolley to
save others’ lives. He also discusses similar famous ethical questions cannot trace the process by which the decisions were
studied by researchers, such as the question whether it is immoral to made?
allow a child to drown in a shallow pond to avoid muddying one’s
clothes, whether one is morally obligated to donate money to save
others’ lives, and more (Greene 2014).

Jean-Francois Bonnefon, Azim Shariff, and Iyad Rahwan apply this 3 One scholar at the National Science Foundation points out that
question to the issue of driverless cars; as driverless cars (‘‘au- technology currently outstrips knowledge of how to assign liability
tonomous vehicles,’’ or AVs) and other forms of use of artificial for robots’ ethical and legal failures (The Economist 2014).
intelligence become more widespread. They examined whether Professor Patrick Lin points out that algorithms cannot make ‘‘an
individuals would be comfortable with AVs programmed to be instinctive but nonetheless bad split-second decision’’ the way
utilitarian and found that the answer was generally yes (Bonnefon humans can, and thus the threshold for liability may be higher (Lin
et al. 2015). 2013).

123



AI assisted ethics 151

Second, what level of law enforcement does society what specifically these values require. Thus many people
seek, given that smart instruments make a high level of agree that the environment ought to be protected, but dis-
surveillance very easy to achieve? (For instance, one could agree about which mileage standards cars should have to
determine from a central location whether truck drivers are abide in by 2030, what level of emissions they should be
driving too long without taking a break, the speed of any allowed to produce, whether one should idle at stop signs,
car on the road, and the location of all who use cell phones and whether they should drive slower than the law requires.
and much more.) Third, what guidance is to be given to This poses great difficulties for programmers.
instruments when the law and ethics diverge? (For A related question is how to determine what the values
instance, should driverless cars be programmed to refuse to are of whatever community the instruments are to heed.
violate the law even when the driver takes over, or to allow Several scholars have suggested using focus groups or
speeding in an emergency?) Wrestling with these questions public opinion polls to determine what the relevant values
is left for a separate discussion in order to focus here on the are.4 One notes, however, that the results of public opinion
social and moral values not ensconced in law (Etzioni and polls vary significantly depending on who is surveyed,
Etzioni 2016). question wordings, the sequence in which questions are

asked, the context in which questions are asked (e.g., at
home versus at work), and the attributes of those who ask

A communitarian approach the questions (e.g., are they the same race as the person
queried). Even when the same question is asked of the

Numerous social and moral values are not ensconced in same people by the same people twice, rather different
law. These include the extent and scope of one’s com- answers follow (Institute for Statistics Education). Hence
mitments to one’s children, spouses, friends, neighbors, such polls cannot be used as a reliable base.
the various communities to which one belongs, the nation, The suggestion that the time has come to engage in what
and even the international community (Wrong 1995). might be called ‘‘teaching machine ethics,’’ that is, teach-
These values include taking risks for others (such as ing instruments to render moral decisions on their own (Lin
fighting overseas and donating organs), volunteering, 2013), runs into different difficulties. Driverless cars and
giving to charity, resolving differences with others civilly, other such instruments are unable to form moral guidelines
and many others. Indeed, many values ensconced in law out of thin air. They will need, at least to begin with, first
are paralleled by considerable additional moral commit- principles and some guiding philosophy, say a utilitarian or
ments above and beyond those required by law. For a deontological one. After all, even humans do not start
instance, most of what the moral culture of communities with a tabula rasa. They gain ethical foundations from
expects parents to do for their children greatly exceeds those that raise them and from their communities and then
what the law commands. A communitarian position holds modify or replace these foundations over time. Which
that it is essential to include these values in the guidance principles and methodologies should be given to smart
systems of smart instruments because these values make instruments? Is there a reason they should all be given the
for a good, civil society well beyond a stable and even same foundations? Could cars come with ethical options—
liberal state. some equipped with utilitarian principles, or deontological

How should one determine which communal values to ones, or Buddhist philosophies? (One may wonder whether
incorporate into the AI guidance systems of smart instru- buyers would understand the implications of their choice of
ments? Those communal values ensconced in law can be car unless they took some philosophy classes.) Some kind
identified relatively readily—they are values that legisla- of polling approach may be unavoidable when deciding
tures and courts draw upon when they enact laws and whether or not to use instruments that are used by com-
interpret them. But how is one to determine which addi- munities rather than individuals. For instance, such an
tional social moral values the community seeks to foster? approach may be necessary when determining whether to

Some suggest that these additional values should be post in public spaces cameras equipped with AI systems
those shared by the community (Walzer 1984). This posi- that scan the footage to identify people who act in an
tion runs into several difficulties. First, people belong to uncivil manner. However, there seem to be serious
different communities and to the encompassing society,
which subscribes to different values. For instance, famous 4 Slobogin and Schumacher (1993: 757) recommend that the
attempts to use community standards to determine Supreme Court draw on public opinion polls to determine that about
obscenity failed because of disagreement about what societal expectations of privacy. Similar suggestions were made by

Fradella, Morrow, Fischer, and Ireland. They conducted a survey of
behavior qualifies (Jacobellis v. Ohio 1964). Even when

589 individuals (Fradella et al. 2010–2011: 293–94).
there is considerable consensus about values at a high level Bonnefon et al. (2015) applied this idea to finding which values
of abstraction, there is often much less consensus about ought to guide self driving cars.

123



152 A. Etzioni, O. Etzioni

difficulties associated with relying on a communitarian choices is rather similar to what many AI programs do for
approach to determining which values the instruments ferreting out consumer preferences and targeting advertis-
should embody for most instruments that are owned and ing to them accordingly. 5 Only in this case, the bots are
operated by individuals. Is there no way to more closely used to guide instruments that are owned and operated by
align the values that instruments are expected to heed and the person, in line with their values, rather than by some
the ethics of their intended users? marketing company or political campaign seeking to

advance their goals. For instance, an ethics bot may con-
clude that a person places high value on environmental

A libertarian approach protection if the ethics bot finds that the person purchases
recycled paper, drives a Prius, contributes to the Sierra

Libertarians and some liberals hold that each person should Club, prefers local food, and never buys Styrofoam cups. It
define the good and the values they are to heed, and that the would then instruct that person’s driverless car to purchase
state should remain neutral (Boaz 1999). It would be only environmentally friendly fuels, to turn on the air
compatible with this position if smart instruments came conditioning only if the temperature is high, and to idle the
with a rich menu, which would allow each individual to engine at stops.
choose which options are in line with their values, as well Much of what follows about other attributes of ethics
as an opened ended category which would allow them to bots is paradigmatic, in the sense that the article outlines
include attention to moral preferences not included in the the qualities of these as-yet to be developed ethics bots.
menu. Some readers may well consider the following pages

The difficulties this libertarian approach raises are somewhat visionary in terms of what they assume AI will
illustrated by the development of privacy options. Many be able to accomplish in the not-too-distant future. How-
websites initially had merely a statement of the privacy ever, given the inability to implement communitarian and
policy they follow, which put the users in a ‘‘take it or libertarian approaches, it seems better to employ even
leave it’’ position—assuming they understood the legal weak ethics bots, at least initially, than to continue to leave
statements. Next, an increasing number of websites offered driverless cars and the large and growing number of other
users a small menu of choices regarding the level of pri- smart instruments without ethical guidance. Moreover, we
vacy they preferred. Facebook, for instance, offers five shall see that some, albeit rather simple, ethics bots have
main privacy settings. Even at this level, people com- already been constructed, road tested, and used by a con-
plained about the complexities of these settings. A Google siderable number of people.
representative recently stated that Google would provide To illustrate: nest built a smart thermostat. It first
up to a hundred such options (Fleischer 2015). Strong ‘‘observed’’ the behavior of the people in various house-
evidence from psychological studies and experience sug- holds for merely a week and drew conclusions about their
gests that most users will find the requirements to make preferences. It then used a motion-detecting sensor to
that many choices on their own overwhelming (Kahneman determine whether anyone was at home. When the house
2011). was empty, the smart thermostat entered a high energy-

These difficulties are much more challenging if one saving mode; when people were at home, the thermostat
takes into account that individuals would need to person- adjusted the temperature to fit their preferences. This
ally provide individual moral guidance to all the growing thermostat clearly meets the two requirements of an ethics
number of smart instruments one uses as the world is bot, albeit a very simple one. It assesses people’s prefer-
moving into the ‘‘internet of things.’’ In short, this ences and imposes them on the controls of the heating and
approach seems highly impractical. cooling system. One may ask what this has to do with

social moral values. This thermostat enables people with
differing values to have the temperature settings they pre-

AI assisted ethics (ethics bots) fer. The residents of the home do not need to reset the
thermostat every day when coming and going. This simple

A paradigmatic agenda ethics bot also reduces the total energy footprint of the
community (Lohr 2015: 147).

An ethics bot is an AI program that analyzes many thou- The ethics bot so far depicted is the most basic version.
sands of items of information—not only information pub- Additional features and more sophisticated ethics bots that
licly available on the Internet but also information gleaned might be developed, are outlined below. They all share,
from a person’s own computers—about what the acts of a though, two major features. First, they enable the people
particular individual that reveal that person’s moral who employ AI to guide AI. This is, instead of treating the
preferences are. Basically what ethics bots do for moral AI world as if it were one unitary field AI should be

123



AI assisted ethics 153

restructured along the same lines as the rest of the world is. give up because a sort of psychological fatigue sets in akin
That is, some AI programs should serve as the first line to the one felt by people who are trying to consider their
‘‘worker bees’’ that provide directions to an ever growing best chess move and after a while just give up and move.
number of instruments—from robot soldiers to Barbie (By contrast, AI is very patient, which is one reason it now
dolls, from voter mobilization drives to refrigerators. Sec- beats even the world chess masters.)
ond line AI programs will act as supervisors, auditors, Second, people accommodate their inability to make a
accountants, and as ethics bots of the first line AI programs. great number of choices by making lexicographic choices.6

Second, because people bring their same basic values to That is, they ignore the information about most facets of
different pursuits, once an ethics bot is able to carry out an the object of their choice, and focus on a few that they hold
analysis of the moral preferences of a person, the same to be most important. Thus, when they buy a car, they may
findings will help guide a variety of smart instruments the examine its relative price, miles per gallon, and color, or
person uses. Thus, a person does not need one ethics bot for some other such mix of features—but ignore scores of
shopping, another for driving, and still another for volun- other attributes. The same holds true of moral choices.
teering. For example, if an ethics bot determines that a People, when making a major donation, may take into
given person’s moral preferences are to maximize their account the goals the given charity serves, whether it ser-
self-interest, that bot would instruct the person’s instru- vices people in their own community or overseas, and
ments to shop at places they find the lowest costs and best whether it has a reputation as an honest agency, or some
quality but disregard whether the sellers have been charged other such mix. They will ignore, in the process, many
with employing workers at unsafe locations overseas; other features of the given charity such as its long-term
paying less then minimum wages; and polluting. It would record, recent changes in leadership, its ratio of expenses to
also instruct the person’s financial AI system to make payouts, and so on. In contrast, ethics bots, given their
donations to charity only if those donations generate computing power, have no such limitations. They hence
enough deductions to make up for the ‘‘loss’’ or if they will help people to ensure that their choices about how and
engender for the donor a great deal of goodwill. And so on. where to donate, shop, vote and more much more closely

Of course, the ethical preferences of most people are reflect all their moral preferences than a few selected ones.
more complicated than the simple examples here used the Third, ethics bots are likely to compare favorably to
purposes of exposition. Hence, in the longer run, ethics other means—such as interviews, self-administered forms,
bots would need to be similarly internally diverse. More- and mental exercises such as those used in lifeboat eth-
over, ethics bots of the future should be able to self-update ics7—that seek to ferret out a person’s moral preferences.
at regular intervals in order to take changes to people’s This is largely the case because these subjective means
moral preferences into account. Last but not least, ethics draw their conclusions mainly on the basis of expressed
bots should have an override feature, discussed below. attitudes, while ethics bots mainly note the moral choices

Basically what ethics bots do for moral choices is rather revealed in actual behavior. For instance, one may say that
similar to what many AI programs do for ferreting out one attends church regularly, but an ethics bot would note
consumer preferences and targeting advertising to them that the person played golf often at the time religious ser-
accordingly.5 Only in this case, the bots are used to guide vices are carried out and parked at the place of worship
instruments that are owned and operated by the person, in only a few times a year. This attribute is of special
line with their values, rather than by some marketing importance because attitudes, a great deal of data shows,
company or political campaign seeking to advance their
goals.

Ethics bots, once developed, should be able to provide a
6 Lexicographic preferences are those in which ‘‘respondents have a

superior interface between a person and smart instruments
ranking of the attributes [consider important], but their choice of an

compared to unmediated interaction. There are several alternative is based solely on the level of their most important
reasons for this. First, if most people will be required to attribute(s)’’ (Campbell et al. 2006).
render a large number of ethical choices, they will quickly 7 ‘‘Lifeboat ethics’’ refers to an ethical dilemma outlined by Garrett

Hardin in 1974, which describes a situation in which a lifeboat nearly
full to capacity must consider whether or not to bring aboard ten

5 For example, Nielsen has developed a marketing system for additional passengers out of 100 people in the water. The purpose of
targeting very specific demographics with financial and investment lifeboat ethics-style philosophical discussions is not to tell anyone
products based on age, affluence, the presence of children in the what is ethically correct in any given situation, but rather to help
home, and certain purchasing habits. These include such specific individuals to clarify their own values.
target consumer groups as ‘‘Y2-54: City Strivers’’ and ‘‘F4-56: This is one component of a larger school of ethics, ‘‘moral
Economizers’’ (Nielsen 2015); Ted Cruz’ campaign in Iowa relied on reasoning.’’ Moral reasoning encourages ‘‘individual or collective
psychological profiles to determine the best ways to canvass practical reasoning about what, morally, one ought to do’’ (Richard-
individual voters in the state (Hamburger 2015). son 2014).

123



154 A. Etzioni, O. Etzioni

correlate poorly with behavior.8 Also, people have diffi- Individuals can use their ethics bots for self-
culties in articulating their preferences. assessment

The introduction of ethics bots would raise serious pri-
vacy concerns. Many people may well seek to encrypt Adolescents, people in psychotherapy, and many others
them and call for laws that would treat ‘‘reading’’ another often engage in self-examination, including asking them-
person’s ethics bot without written prior permission as akin selves whether they are good people, whether they do
to reading their medical record or other sensitive infor- enough to serve others, and more. Studies of such self-
mation.9 One should note, though, that ethics bots would assessment suggest that people often greatly over- or
not be mandated, and hence would be used only by those under-evaluate themselves (Dunning et al. 2003). In the
who see their merits and benefits as exceeding the bots’ future, these individuals will be able to draw on their ethics
privacy risks. bots to provide them a more objective—and candid—

Moreover, those who adopt ethics bots are sure to note evaluations of themselves. These evaluations may well
that most of the information bots use to ascertain their include how they compare to others in their community and
preferences is already publicly available. Several corpora- whether they improve or not over time. Here ethics bots
tions maintain very detailed dossiers on most people and serve as tools of moral self-improvement.
sell these dossiers to all comers. For instance, Axciom Ethics bots can be programmed in ways that allow their
maintains dossiers on most Americans. These dossiers human users to modify the program the bots chose for
include ‘‘age, behavior, buying activity, financial, house- them, based on their preferences. They can augment, dis-
hold, interest, real property, life events,’’ and more, up to tract, and override—all moves that are much less taxing
1500 items per person. SeisInt dossiers include individuals’ than forming the self’s ethics profile de novo. For instance,
‘‘criminal histories, photographs, property ownership, a person may note that his ethics bot reveals that over the
SSNs, addresses, bankruptcies, family members, and credit last 10 years he donated rather little to various charities
information.’’10 These dossiers can even include sensitive then he thought he did—and instruct his financial app to
medical information. As Eli Pariser reports, ‘‘Search for a increase these allocations.
word like ‘depression’ on Dictionary.com, and the site Ethics bots can also help people implement pre-com-
installs up to 223 tracking cookies and beacons on your mitment strategies. The term refers to taking steps before a
computer so that other websites can target you with situation in which one expected to be tempted to act
antidepressants’’ (Pariser 2011). It seems that if others are unethically—to fend off the temptation. Odysseus
free to use a very great deal of personal information to employed this strategy by instructing his sailors to tie him
promote products and politicians and to seek to shape to the mast of the ship and to plug their ears before they
people’s preferences, there is little to be lost and much to entered the sea of Sirens so that they would not be tempted
be gained if that person uses the same information to by the Sirens’ calls (Homer 1978). Thus, an ethics bot
ensure that the instruments they employ will comport with could be set to instruct a driverless car not to yield to
their values. attempts to override the car’s system to engage in road

Individuals can at any point override the guidance an rage, or to mute the car’s horn if the person blows it long,
ethics bot provides to their instruments. Thus, when a smart hard, and often.
thermostat programmed by Nest did not follow individuals’
preferences, but rather set the thermostat within two
degrees of their preferences at a setting more favorable to Pragmatic and operational considerations
the environment, many people rejected this setting (Lohr
2015). In short, ethics bots (or AI assisted ethics) are a Some critics may well argue that the agenda charted so far

marriage between libertarianism (because the particular is well beyond what AI can achieve. They may point to

person provides the moral preferences—the definition of programs that try to divine consumer preferences much

the good) and AI programs (which provide moral guidance simpler than their ethical preferences and did not fare well,

to smart instruments). such as, programs that recommend books and movies. In
general, arguments about what AI can and cannot accom-
plish swing between overblown hype and overblown des-
pair. Some point to AI programs that play winning chess,

8 Across many different situations, it is well-established that Jeopardy, and Go, while others bemoan the difficulties
‘‘attitudes are poor predictors of behavior’’ (Ajzen and Fishbein

computers have in accomplishing tasks that are simple to
2005). See also: Azjen et al. 2004.
9 humans, such as reading graphic designs.For an in-depth discussion of the different treatment afforded to less
and more sensitive information, see: [redacted]. In response, one must reiterate that humans cannot on
10 See id. their own provide more than very elementary ethical

123



AI assisted ethics 155

guidance to smart instruments without AI assistance. likely to differ greatly from those second order programs
Hence, even weak ethics bots seem preferable to none, that would direct instruments to heed moral values, which
unless one can come up with a still different way to provide are often fuzzy. Whether a particular instrument is used by
moral guidance to smart instruments. The best test of individuals or by a community is also be a factor. This
whether ethics bots can be created is to invest more in article focuses on those AI programs dealing with com-
trying to build some. There seem to no obvious, a priori, pliance with social and moral values for instruments used
logical reasons to hold that such bots cannot be con- by a person, such as driverless cars.
structed—and some, albeit very simple ones, have been The article finds that relying on guidance in these matters
developed. on values shared by this or that community raises many

Determining the moral preferences of a person in some difficulties. The same holds if one seeks individuals to
areas may well be less daunting than in others. For instance, directly instruct their smart instruments on their own. A
determining what a person considers fair might be indeed preferred method, here outlined, is to develop AI programs to
very difficult. By contrast, people’s privacy preferences be used to determine the moral preferences of people, ethics
seem easier to grasp. Most people are either privacy fun- bots, and for these ethics bots to guide the smart instruments.
damentalists (as their use of multiple personal email
addresses, frequently changing their passwords, and so on
reveals); privacy pragmatists (who will share personal References

information if the price is right); or privacy unconcerned
Ajzen, I., & Fishbein, M. (2005). The influence of attitudes on

(Westin 2003). In short, ethics bots are badly needed. behavior. In D. Albarracı́n, B. T. Johnson, & M. P. Zanna (Eds.),
Whether more and more accomplished ones can be con- The handbook of attitudes. Mahwah, NJ: Lawrence Erlbaum
structed in the near future remains to be seen. Meanwhile, Associates Publishers.

driverless cars are roaming the streets and so far have not Azjen, I., Brown, T. C., & Carvajal, F. (2004). Explaining the
discrepancy between intentions and actions: The case of

been granted any moral guidance. Even if we cannot be able hypothetical bias in contingent valuation. Personality and Social
to construct a high-fidelity model of people’s ethical pref- Psychology Bulletin, 30(9), 1108–1121.
erences, we will be able to approximate and the approxi- Boaz, D. (1999). Key concepts of libertarianism. Cato Institute.

mation will improve as the technology gets better. January 1. http://www.cato.org/publications/commentary/key-
concepts-libertarianism.

Bonnefon, J., Shariff, A., & Rahwan, I. (2015). Autonomous vehicles
In conclusion need experimental ethics: Are we ready for utilitarian cars?

Computers and Society. October 12. http://arxiv.org/abs/1510.
03346.

The incorporation of AI into more and more instruments
Campbell, D., Hutchinson, W. G., & Scarpa, R. (2006). Lexico-

makes them much smarter—more efficient, and more graphic preferences in discrete choice experiments: Conse-
effective. In the process, these instruments are acquiring a quences on individual-specific willingness to pay estimates.
measure of autonomy in the sense that they render many Working Paper, Fondazione Eni Enrico Mattei. http://agecon

search.umn.edu/bitstream/12224/1/wp060128.pdf.
decisions on their own, well beyond the guidelines that the

Dellinger, A. J. (2015) Tim Wu says Google is degrading the Web to
programmers introduced and sometimes even counter to favor its own products. The Daily Dot. June 29. http://www.
these guidelines. There is hence growing concern about dailydot.com/technology/google-search-tim-wu-yelp/.
how society and millions of individuals can rest assured Dunning, D., Johnson, K., Ehrlinger, J., & Kruger, J. (2003). Why

people fail to recognize their own incompetence. Current
that instruments they use, which are equipped with AI, will

Directions in Psychological Science, 12(3), 83–87.
not render unethical decisions (Dellinger 2015). Etzioni, A. (1988) The moral dimension. New York: The Free Press.

The answer to these challenges cannot be found in bare Etzioni, A., & Etzioni, O. (2016). Keeping AI Legal. Vanderbilt
human controls, because human beings cannot determine on Journal of Entertainment & Technology Law (Forthcoming).

Fleischer, P. (2015). Privacy and future challenges. Speech, Amster-
their own whether unethical (and even illegal) acts carried

dam Privacy Conference. Amsterdam, The Netherlands, October
out by smart instruments were the results of the human 23–26.
programming—or AI processes ‘‘under the hood.’’ Hence, a Fradella, H. F, et al. (2010–2011). Quantifying Katz: Empirically
major conclusion of this article is that to ensure proper measuring ‘Reasonable Expectations of Privacy’ in the fourth

amendment context. American Journal of Criminal Law 38,
conduct by AI instruments—people will need to employ

289–373.
other AI systems. Implementing ethical preferences, when Goldhill, O. (2015). Human values should be programmed into
dealing with smart instruments, will need to be AI assisted. robots, argues a computer scientist. Quartz. October 31. http://

These second order AI programs would have to vary a qz.com/538260/human-values-should-be-programmed-into-robots-
argues-a-computer-scientist/.

great deal from one another. For instance, those second
Greene, J. D. (2014). Beyond point-and-shoot morality: Why

order programs that would ensure that instruments do not cognitive (neuro) science matters for ethics. Ethics, 124(4),
violate laws, the dictates of which are relatively clear, are 695–726.

123



156 A. Etzioni, O. Etzioni

Hamburger, T. (2015). Cruz campaign credits psychological data and Pariser, E. (2011). What the Internet knows about you. CNN. May 22.
analytics for its rising success. The Washington Post. December http://articles.cnn.com/2011-05-22/opinion/pariser.filter.bubble.
13, https://www.washingtonpost.com/politics/cruz-campaign- Richardson, H. S. (2014). Moral reasoning. The Stanford Encyclope-
credits-psychological-data-and-analytics-for-its-rising-success/ dia of Philosophy (Winter Edition), Ed. Edward N. Zalta. http://
2015/12/13/4cb0baf8-9dc5-11e5-bce4-708fe33e3288_story.html. plato.stanford.edu/entries/reasoning-moral/.

Hardin, G. (1974). Lifeboat ethics: The case against helping the poor. Rossi, F. (2015). How do you teach a machine to be moral? The
Psychology Today. Washington Post. November 5. https://www.washingtonpost.

Homer. (1978) Odyssey (J. H. Finley, Jr. Trans.). Boston: Harvard com/news/in-theory/wp/2015/11/05/how-do-you-teach-a-machine-
University Press. to-be-moral/.

Institute for Statistics Education. Glossary of statistical terms test– Science Daily. (2015). New algorithm lets autonomous robots divvy
retest reliability. http://www.statistics.com. up assembly tasks on the fly. May 27. http://www.sciencedaily.

Jacobellis v. Ohio. (1964). 378 U.S. 184. com/releases/2015/05/150527142100.htm.
Kahneman, D. (2011). Thinking, fast and slow. New York: Firrar, Slobogin, C., & Schumacher, J. E. (1993). Reasonable expectations of

Straus, and Giroux. privacy and autonomy in fourth amendment cases: An empirical
Kaplan, J. (2015). Who put the robot in charge? Medium, September look at understandings recognized and permitted by society.

22. https://medium.com/the-wtf-economy/who-put-the-robot-in- Duke Law Journal, 42, 727–775.
charge-408a47335176#.mb8mqqs9p. Tegmark, M, et al. (2015). An open letter: Research priorities for

Lin, P. (2013). The ethics of autonomous cars. The Atlantic. October robust and beneficial artificial intelligence. Future of Life
8. http://www.theatlantic.com/technology/archive/2013/10/the- Institute. http://futureoflife.org/ai-open-letter/.
ethics-of-autonomous-cars/280360/. The Economist. (2014). That thou art mindful of him. March 29.

Lohr, S. (2015). Data-ism: The revolution transforming decision The Economist. (2015). Rise of the machines. http://www.economist.
making, consumer behavior, and almost everything else. Lon- com/news/briefing/21650526-artificial-intelligence-scares-peopleex
don: OneWorld Publications. cessively-so-rise-machines.

Marcus, G. (2012). Moral machines. New York: The New Yorker. Walzer, M. (1984). Spheres of Justice: A defense of pluralism and
Markoff, J. (2013). The rapid advance of artificial intelligence. The Equality. New York: Basic books.

New York Times. October 14. http://www.nytimes.com/2013/ Westin, A. (2003) Social and political dimensions of privacy. Journal
10/15/technology/the-rapid-advance-of-artificial-intelligence. of Social Issues 59(2). http://onlinelibrary.wiley.com/doi/10.
html?pagewanted=all&_r=0. 1111/1540-4560.00072/epdf.

Mayer-Schönberger, V., & Cukier, K. (2014). Big data. New York: Wolchover, N. (2015). Concerns of an artificial intelligence pioneer.
Houghton Mifflin Harcourt. Quanta. April 21. https://www.quantamagazine.org/20150421-

Nielsen. (2015). Nielsen P$YCLE Lifestage Groups. https://segmen concerns-of-an-artificial-intelligence-pioneer/.
tationsolutions.nielsen.com/mybestsegments/Default.jsp?ID= Wrong, D. (1995). The problem of order: What unites and divides
8010&pageName=Learn%2BMore&menuOption=learnmore. society. Boston: Harvard University Press.
Accessed 17 Dec.

123﻿Machine Ethics: the design and governance of ethical AI and autonomous 
systems 
 
Alan Winfield, Katina Michael, Jeremy Pitt and Vanessa Evers 
 
1. Introduction 
 
The so-called 4th industrial revolution and its economic and societal 
implications is no longer solely an academic concern, but has become a matter 
for political as well as public debate. Characterised as the convergence of 
robotics, AI, autonomous systems and information technology – or cyber-
physical systems – the fourth industrial revolution was the focus of the World 
Economic Forum, at Davos, in 2016 [1]. Also in 2016 the US White House 
initiated a series of public workshops on artificial intelligence (AI) and the 
creation of an interagency working group, and the European Parliament 
committee for legal affairs published a draft report with recommendations to the 
Commission, on Civil Law Rules on Robotics. 
 
Since 2016 there has been a proliferation of ethical principles in AI; seven sets of 
principles were published in 2017 alone – including the influential Future of Life 
Institute’s Asilomar principles for beneficial AI. Nor has there been a shortage of 
industry initiatives, perhaps the most notable is the Partnership in AI; founded in 
2016 by representatives of Apple, Amazon, DeepMind and Google, Facebook, 
IBM, and Microsoft, the partnership now has more than 50 members. Most 
significant however has been the number of national AI initiatives announced 
[2]. Since Canada published its national AI strategy early in 2017 more than 20 
other countries (and groupings including the EU) have announced similar 
initiatives. The largest and most comprehensive is undoubtedly China’s Next 
Generation AI development plan, launched in July 2017.  
 
Notably all of these initiatives express the need for serious consideration of the 
ethical and societal implications of robotics and artificial intelligence. Robot and 
AI ethics has been transformed from a niche area of concern of a few engineers, 
philosophers and law academics, to an international debate. For these reasons 
we believe this special issue – focussed on the ethics of intelligent autonomous 
systems – is not only timely but necessary.  
 
The primary focus of this special issue is machine ethics, that is the question of 
how autonomous systems can be imbued with ethical values. Ethical 
autonomous systems are needed because, inevitably, near future systems are 
moral agents; consider driverless cars, or medical diagnosis AIs, both of which 
will need to make choices with ethical consequences. This special issue includes 
papers that describe both implicit ethical agents, that is machines designed to 
avoid unethical outcomes, and explicit ethical agents: machines which either 
encode or learn ethics and determine actions based on those ethics.  Of course 
ethical machines are socio-technical systems thus, as a secondary focus, this 
issue includes papers that explore the educational, societal and regulatory 
implications of machine ethics, including the question of ethical governance. 
Ethical governance is needed in order to develop standards and processes that 



allow us to transparently and robustly assure the safety of ethical autonomous 
systems and hence build public trust and confidence.  
 
2. The landscape of robot and AI ethics 

 
The field of robot and AI ethics is broadly divided into two main branches. By far 
the largest of these is concerned with the vitally important question of how 
human developers, manufacturers and operators should behave in order to 
minimize the ethical harms that can arise from robots and AIs in society, either 
because of poor (unethical) design, inappropriate application, or misuse. This 
branch is concerned with the ethical application of robots and AIs and is 
generally referred to as either AI ethics or robot ethics, and has already led to the 
development of ethical principles [3,4], standards [5], and proposals for good 
practice [6,7]. 

 
The smaller branch of AI ethics is concerned with the question of how robots and 
AIs can themselves behave ethically. Referred to as ethical AI, ethical robots or – 
more generally – machine ethics, the field spans both philosophy and 
engineering. Philosophers are concerned with questions such as “could a 
machine be ethical, and if so which ethics should determine its behaviour?” 
alongside larger questions such as “should society delegate moral responsibility 
to its machines?” while engineers are interested in solving the (significant) 
technical problem of how to build an ethical machine. The two disciplines are not 
disconnected; philosophers are interested in the outcomes of practical machine 
ethics not least because – if successful – they lend urgency to the moral questions 
around ethical machines in society. Equally, engineers engaged in designing 
ethical machines need philosophers to advise on the definition of appropriate 
ethical rules and values for these machines. 
 
Of course the idea that robots should not only be safe but also actively capable of 
preventing humans from coming to harm has a long history in science fiction. In 
his short story Runaround, Asimov [8] expressed such a principle in his now 
well-known Laws of Robotics. Although no-one has seriously proposed that real-
world robots should be ‘three-laws safe’, work in machine ethics has advanced 
the proposition that future robots should be more than just safe. In their 
influential book Moral Machines – teaching robots right from wrong, Wallach and 
Allen [9] set out the philosophical foundations of machine ethics, coining the 
term Artificial Moral Agent (AMA); Wallach and Allen write: 
 

“If multipurpose machines are to be trusted, operating untethered from their 
designers or owners and programmed to respond flexibly in real or virtual 
world environments, there must be confidence that their behaviour satisfies 
appropriate norms. This goes beyond traditional product safety ... if an 
autonomous system is to minimise harm, it must also be ‘cognisant’ of possible 
harmful consequences of its actions, and it must select its actions in the light of 
this ‘knowledge’, even if such terms are only metaphorically applied to 
machines” (italics added). 
 



Since Čapek’s 1920 play Rossum’s Universal Robots, and the stories of Asimov, 
Frayn [10] and many others, science fiction narratives have played a key role in 
the exploration of artificial morality [11]. In the study of machine ethics we can 
now, for the first time, begin to investigate artificial morality in fact. 
 
3. The state of the art in Machine Ethics 
 
Machine ethics is a new field. Although the antecedents of machine ethics can be 
found in computer ethics [12], the earliest works on machine ethics were 
published less than 20 years ago. Those works are, not surprisingly, theoretical 
and philosophical. The field of machine ethics was de facto established by Allen 
et al [13,14], Asaro [15], Moor [16], Powers [17], Anderson and Anderson 
[18,19] and Wallach and Allen [9]. 
 
The earliest proposal for a practical ethical governor for robots – a mechanism 
for moderating or inhibiting a robot’s behavior to prevent it from acting 
unethically – was from Arkin [20]1, although not tested on real robots. At the 
time of writing the number of experimental demonstrations of ethical robots 
remains very small indeed; to the best of our knowledge there have been only 
five such demonstrations to date: (i) the GenEth system of Anderson and 
Anderson [22], (ii) the Asimovian ethical robots of Winfield et al [23] and 
Vanderelst and Winfield [24], (iii) Bringsjord et al’s Akratic robot [25], (iv) the 
“sorry I can’t do that” robot of Briggs and Scheutz [26] and (v) the Intervening 
robot mediator in healthcare of Shim, Arkin and Pettinatti [27]. Papers which 
review and update the approaches of (i) and (ii) are included in this issue; we 
now briefly review (iii - v). 
 
Based on earlier work proposing a general ‘logistic’ method for engineering 
ethically correct robots [28], Bringsjord’s Akratic2 robot is tested in scenarios in 
which it is charged with guarding a prisoner of war and must choose between 
retaliating with violence to an attack (thus satisfying a self-defence goal) or 
refraining from retaliation [25]. The robot’s ‘ethical substrate model’ 
incorporates logic that specifies which actions are forbidden under certain 
circumstances, or which are permitted or obligatory; importantly that logic, a 
deontic cognitive event calculus (DCEC), can be formally verified. 
 
Briggs and Scheutz [26] demonstrate a robot capable of declining to carry out an 
instruction, if it reasons that to do so would harm itself. The robot may more 
properly be described as implicitly ethical since it is designed only to avoid 
unethical outcomes to itself, but the cognitive machinery it uses to avoid such 
outcomes is, like Bringsjord et al’s robot [25], based on reasoning about 
obligation and permissibility. 
 

                                                        
1 The notion of a governor in autonomous systems dates back to the birth of 
cybernetics (from Kyvernitis – to govern), see [21]. 
2 From the Greek akrasia referring to when a person acts in contradiction to their 
better judgement. 
 



In the work of Shim et al [27] a robot equipped with an ethical governor 
monitors the interaction between a patient and healthcare advisor. The robot is 
able to sense (i) the loudness of the patient’s speech, (ii) when the patient stands 
up to leave, and (iii) when the advisor asks the patient to take some medicine. In 
four test scenarios: ‘yelling’, ‘quiet’, ‘stay-in-the-room’ and ‘safety-first’, the robot 
senses the patient advisor interaction and uses both speech and gesture to 
intervene. In the safety-first scenario the robot might alert the advisor with “the 
previous records say he had a reaction (to this medicine). I think it’s not safe”.  
 
Allen et al [14] identified three approaches to machine ethics: top-down, bottom-
up and a hybrid of top-down and bottom-up. The top-down approach requires 
training the machine to be able to recognise and correctly respond to morally 
challenging situations. The bottom up approach instead constrains the actions of 
the machine in accordance with pre-defined rules or norms. Of the five 
experimental demonstrations listed here only the first: Anderson and Anderson’s 
GenEth system, adopts a top-down approach. The other four – although very 
different in detail – are all examples of bottom-up constraint-based approaches. 
  
All five trials are of robots with very limited ethics in constrained laboratory 
settings; they each demonstrate proof of concept but are far from practical 
application in real-world settings. Nevertheless we can be confident that it is 
possible, at least in principle, to build minimally ethical robots. Later in this 
article we shall consider the question of the degree of ethical agency of these 
robots. 
 
All of the work above has, not surprisingly, focussed on the core processes – the 
cognitive architectures – for making ethical choices. But many technical 
challenges remain: how, for instance, do we design a robot or AI that is 
“cognisant of possible harmful consequences of its actions”? This requires that a 
machine can reliably recognise when and how its actions have ethical salience; 
for a robot interacting with a human that means that it needs to perceive that the 
human is at risk – and the nature of that risk. The accurate perception of both 
risk and context is non trivial even in controlled environments – and even more 
challenging in real-world settings. Context might also change the risk calculation; 
an elderly person who is frail is clearly at greater risk that one who is physically 
fit, and the presence of other humans (a nurse for instance) clearly also changes 
the context for a care robot. 
 
If and when ethical machines are ready for real–world application we would 
need to be sure that their ethical decision-making processes are both guaranteed 
and safeguarded against misuse. We would also need robust frameworks for 
ethical governance, including technical standards for ethical machines alongside 
processes of accident investigation. And bigger societal questions would need to 
be addressed around the extent to which we are prepared to delegate moral 
responsibility to our machines. 
 
4. Defining machine ethics 
 



Although not providing a singular definition of machine ethics Moor’s influential 
2006 paper: The nature, importance, and difficulty of machine ethics [16], defines 
the field by articulating four categories of ethical agency. These are: 
 

o Ethical impact agents: any machine that can be evaluated for its ethical 
consequences. 

 
o Implicit ethical agents: machines designed to avoid unethical outcomes. 

 
o Explicit ethical agents: machines that can reason about ethics. 

 
o Full ethical agents: machines that can make explicit moral judgments and 

justify them. 
 
Anderson and Anderson [18] suggest that the goal of machine ethics “is to create 
a machine that is guided by an acceptable ethical principle or set of principles in 
the decisions it makes about possible courses of action it could take”. In Machine 
Ethics [19] Anderson and Anderson elaborate upon this definition: “machine 
ethics is concerned with giving machines ethical principles, or a procedure for 
discovering a way to resolve they ethical dilemmas they might encounter, 
enabling them to function in an ethically responsible manner through their own 
ethical decision making”.  
 
In this special issue we follow Anderson and Anderson’s 2006 definition. To 
paraphrase: an ethical machine is guided by an ethical rule, or set of rules, in 
deciding how to act in a given situation. It follows that we are concerned here 
with autonomous machines: either software AIs, or their physically embodied 
counterpart, robots, which determine how to respond to input without direct 
human control. Although labelled as autonomous such systems are generally 
subject to human supervision or monitoring (and intervention if necessary) in a 
way that is more properly described as ‘supervised autonomy’; but what is 
important is that low-level decisions are made by the system rather than a 
supervising human. In this special issue we are concerned with ethical systems 
that fall into both of Moor’s second and third categories of implicit and explicit 
ethical agents. 
 
All of the five demonstrations of ethical robots referenced in the previous section 
are certainly implicit and, arguably, explicit ethical agents, since they all have 
either learned or defined ethical rules and the cognitive machinery to take those 
rules into account when deciding how to act in a given situation and – if 
necessary – proactively acting or intervening to prevent harm; a process we can 
reasonably describe as ethical reasoning (albeit of a limited kind). It is important 
to note that Moor’s definitions of ethical agency, and in particular the distinction 
between implicit and explicit agency, remain controversial [29], as does the 
question of which systems properly qualify as explicitly ethical machines [30]. 
 
We would argue that all non-trivial examples of real-world AIs and robots are 
ethical impact agents as defined by Moor. The ethical impact of an online store’s 
AI in offering suggestions for purchases may be slight, but there can be no doubt 



that search engines and AIs that determine which social media posts and 
advertisements appear on your home page can have significant societal and 
political impact during, for instance, election campaigns [31]. 
 
Consider AIs that recommend loans, determine welfare payments, or 
recommend prison sentences. We only need to reflect on the impact of an 
incorrect decision to see that these are systems with ethical impact. Indeed bias 
in AIs trained with uncurated data sets is now a well-known problem [32]. Even 
chatbots, which may appear inconsequential, can fall victim to gaming by users 
and quickly become deeply offensive [33, 34]. Connected AI-toys designed to 
interact and converse with children are even more worrying. More obvious 
examples of systems with clear ethical impact include medical diagnosis AIs, 
assisted-living (care) or companion robots, and driverless cars. All of these 
systems have the potential for negative ethical impact, either as a result of poor 
design or simply a lack of forethought about how the system might be used [35]. 
And few systems, regardless of how well designed, are immune to malicious use 
[36] It follows that all AI and robotics systems would benefit from ethical risk 
assessment [5] within the wider framework of Responsible Innovation [37]. 
 
We would go further and propose that all robots and AIs of the kinds mentioned 
here should be designed to avoid negative ethical impacts; in other words they 
should be designed to be implicit ethical agents within Moor’s schema. 
Frameworks for the design and test of implicit ethical agents are now emerging; 
the IEEE Standards Association document Ethically Aligned Design [6] and 
associated standards currently in draft such as IEEE P7000 Model Process for 
Addressing Ethical Concerns During System Design, together provide a 
methodology for imbuing ethical values into intelligent systems. Those values 
are then expressed implicitly in such a way that the system meets the highest 
standards of, for instance, transparency and explainability (P7001), privacy 
(P7002) and is – as far as possible – free of bias (P7003). Adamson et al outline 
these and other IEEE ethics initiatives in this special issue [47]. 

 
5. How ethical are current explicitly ethical machines? 
 
One objection frequently levelled at the ethical machines demonstrated thus far 
is that they simply extend the envelope of safe behavior, and that they should not 
be described as ethical, but simply as ‘safety plus’ machines.  
 
To counter this objection let us consider a simple thought experiment: you are 
walking on the street and notice a child who is not looking where she’s going 
(perhaps engrossed in her smart phone); you see that she is in imminent danger 
of walking into a large hole in the pavement. Suppose you act to prevent her 
falling into the hole. Most bystanders would regard your action as that of a good 
person – in more extreme circumstances you might be lauded a hero. Of course 
your action does keep the child safe, but it is also a moral act. Your behavior is 
consequentialist ethics in action because you have (a) anticipated the 
consequences of her inattention and (b) acted to prevent a calamity. Your act is 
ethical because it is an expression of care for another human’s safety and well-
being. To claim that ethical robots merely exhibit safety plus behavior is to miss 



the key point that it is the intention behind an act that makes it ethical. Of course, 
robots and AIs don’t have intentions – even explicitly ethical robots – but their 
designers do, and an ethical robot is an instantiation of those good intentions. 
 
Moral philosophers are also, and perhaps not surprisingly, doubtful over claims 
that any machines can be described as ethical. Some argue that morality is the 
exclusive preserve of humans. Of course humans are not the only animals that 
demonstrate altruism, but we are almost certainly the only species capable of 
both consciously reflecting upon and justifying the morality of our actions. 
 
One moral philosopher [38] offered the following opinion on the minimally 
ethical robots described in [23]: 

"The obvious point that any moral philosopher is going to make is that 
you are assuming that an essentially consequentialist approach to ethics 
is the correct one. My personal view, and I would guess the view of most 
moral philosophers, is that any plausible moral theory is going to have to 
pay at least some attention to the consequences of an action in assessing 
its rightness, even if it doesn’t claim that consequences are all that matter, 
or that rightness is entirely instantiated in consequences. So on the 
assumption that consequences have at least some significance in our 
moral deliberations, you can claim that your robot is capable of attending 
to one kind of moral consideration, even if you don’t make the much 
stronger claim that it is capable of choosing the right action all things 
considered."  

 
It does therefore appear to be reasonable to make the limited claim that the 
ethical robots demonstrated in the laboratory are at least “capable of attending 
to one kind of moral consideration”. 
 
We need to make the distinction between what might in principle be achievable 
in the near future and the far future. For machines to be as good as humans at 
moral reasoning they would need, to use Moor’s terminology, to be full ethical 
agents [16]. The best we have demonstrated to date is a handful of proof-of-
concept explicitly ethical agents and even those, as we have commented, are only 
minimally ethical. In no sense are such agents better at moral reasoning than 
humans. 
 
Does the fact that we have arguably reached the third category in Moor's scheme 
(explicit ethical agents) mean that full ethical agents are on the horizon? The 
answer again must be ‘no’. The scale of Moor's scheme is not linear. It is a 
relatively small step from ethical impact agents to implicit ethical agents, then a 
very much bigger and more difficult step to explicitly ethical agents, which we 
are only just beginning to witness. But then there is a huge gulf to full ethical 
agents, since they would almost certainly need something approaching human 
equivalent AI; full explicit ethical agents, by Moor’s definition [16], require 
consciousness, intentionality, and free will. Indeed, we think it is appropriate to 
think of explicitly ethical machines as examples of narrow AI; full ethical agents 
would almost certainly require artificial general intelligence (AGI). 
 



The Ethical Turing test 
 
Several have proposed a test for ethical machines somewhat akin to the Turing 
Test. Allen et al [13] for instance outline a ‘comparative Moral Turing Test’ 
(cMTT) in which a human interrogator is presented with pairs of examples of 
morally significant actions of a human and an ethical machine. The interrogator 
is asked to judge which of each pair of actions is less moral, and if the less moral 
actions are, more often, human decisions, then the machine is judged to pass the 
test. As Allen et al point out, there are several problems with this test; one is that 
human behavior is often less than perfectly ethical, so the cMTT may be setting 
the moral threshold too low – they note that “decisions that result in harm to 
others are likely to be much less tolerated in a machine than in another human 
being”. 
 
Anderson and Anderson [22] propose an alternative ‘Ethical Turing Test’ (ETT) 
in which a panel of ethicists are presented with the machine’s ethical decisions 
across a range of application domains. Each ethicist is asked whether they agree 
or disagree with those decisions – if a significant number are in agreement (i.e. 
the ethicist would have made the same choice in the same situation) – then the 
machine is judged to pass the test. In a run of this test with a panel of 5 ethicists, 
described in [22], the level of agreement ranges from 75% to 100%.  Anderson 
and Anderson note that the most contested domain of ‘treatment 
reconsideration’ – “should the health care worker (or robot) accept the patient’s 
decision regarding treatment as final, or attempt to change their mind?” – was 
also the most ethically sensitive for humans. A finding that supports our view 
that ethical decisions difficult for humans will be equally challenging for 
machines and their designers. 
 
It is important to note that the idea of an ethical Turing Test remains 
controversial.  Arnold and Scheutz [39] for instance argue against such a test. 
One of their objections is, essentially, that the ethical Turing Test treats the 
ethical agent as a black box by considering only its decisions and not the 
reasoning behind those decisions. Instead Arnold and Scheutz advocate 
verification, which seeks “predictable, transparent, and justifiable decision-
making and action”– a position with which we strongly agree. 
 
The Trolley problem and learned ethical principles 
 
Consider the distinction between a machine that takes decisions according to 
predetermined principles of ethics and one that learns those principles of ethics 
from observed decisions. In fact, the latter is closer to the 'scientific' study of 
(rather than the application of) ethics: the objective is to identify principles 
which serve to explain judgements of morality (distinctions between good and 
bad, or right and wrong) in terms of the reasoning used to justify them. This, 
rather than training, is the real motivation behind studying abstract situations 
such as the trolley problem [40]. These problems provide data (in the form of a 
set of moral judgements) from which a general principle of ethics can be derived. 
As with any good theory, this principle should have predictive leverage: so, if the 



problem or its parameters are changed, it should be possible to test whether the 
principle still holds (or not). 
 
It is one thing to propose principles of ethics, implement them in a machine, and 
use them in a restricted context (the same context from which the principle was 
derived, perhaps); although then there is the difficult question of whether it is 
appropriate to use such machines for decision-support in other contexts. It is 
another thing to apply Machine Learning algorithms to ‘learn' the principle(s) 
from the data; in which case, we need to be absolutely certain that the dataset 
has not been biased and that the explanatory principle so learned really does 
have predictive leverage when applied to a different context. For sure, if the 
machine learns a ‘wrong’ or ‘inadequate’ principle, or even just a ‘simple’ 
principle, then there will be problems if we try to apply it in other situations. 
 
Intuitively, the situation appears to be analogous to the problem of distributive 
justice, where in any given situation there are a number of possible ways of 
distributing rewards or punishments. These are called legitimate claims [41]. 
Equally, in any situation requiring a moral judgement, there may well be a 
number of principles that have stronger or weaker relative relevance. The 
requirement is to work out which principles apply in any situation, how to 
accommodate them in case of plurality, and how to reconcile them in case of 
conflict [42]. 
 
6. The morality of building ethical machines 

 
Consider the question: is it ethical to delegate moral responsibility to machines? 
We routinely delegate task-level responsibilities to our machines; from simple 
automata such as washing machines to advanced safety-critical systems such as 
airplane autopilots, we trust a wide range of machines to undertake tasks that 
used to be exclusively performed by humans.  To extend this kind of delegated 
responsibility to encompass ethics may therefore appear uncontroversial, but – 
we contend – it is a step that should only be taken with great care.  
 
In considering the morality of building ethical machines we need to differentiate 
between implicitly ethical machines – those designed to avoid unethical 
outcomes – and explicitly ethical machines – those that reason about ethics, 
because the calculus of risk is quite different in each case. 
 
Consider first the category of implicitly ethical machines. We have already 
argued that all intelligent autonomous systems that have the potential to cause 
harm should be classed as implicitly ethical machines, and designed using 
processes of ethically aligned design. The risks of not doing so are already 
apparent as outlined in section 4 above. Building implicitly ethical machines 
would seem to be an ethical course of action.  
 
Conversely, explicitly ethical machines do bring risk. In this special issue Cave et 
al identify four broad categories of risk: “a) the risk that ethically aligned 
machines could fail, or be turned into unethical ones; b) the risk that ethically 
aligned machines might marginalize alternative value systems; c) the risk of 



creating artificial moral patients; and d) the risk that our use of moral machines 
will diminish our own human moral agency” [51]. It is by no means clear that we 
should build ethical machines, even if we can. 
 
Of course real-world explicitly ethical machines – those that reason about ethics 
– are not inevitable. There are, to the best of our knowledge, no explicitly ethical 
agents in real-world use. As we have outlined here the only explicit ethical 
agents that exist are a handful of proof-of-concept laboratory prototypes. These 
are minimally ethical machines with limited functionality designed only to test 
hypotheses about how to build such machines. None are examples of real-world 
robots – such as autonomous cars with added ethics functionality. 
 
The ubiquitous trolley problem 
 
In the public discourse around the ethics of AI, there is a common assumption 
that ethical machines – and in particular driverless car autopilots – should be 
able to resolve ethical dilemmas and choose between two equally undesirable 
outcomes.  This assumption is fuelled by the Trolley Problem [41], of which there 
are many variations. For driverless cars the problem is often posed as the car 
having to choose whether to kill one human or several. A recent high profile 
study asked respondents for their preferences in a range of driverless car 
scenarios and, perhaps not surprisingly, the study revealed significant cultural 
variations [43]. 
 
From an engineering perspective building a machine that can both reliably 
perceive and then choose between two unethical outcomes in any real-world 
environment – let alone in fast moving dynamic situations – is far beyond the 
state of the art. Yet in the public discourse this is rarely made clear, giving rise to 
unrealistic expectations of the decision-making capabilities of near future 
driverless cars. The trolley problem is a thought experiment; it undoubtedly has 
value both as a philosophical tool (as outlined in section 5 above) and at a 
statistical level – as suggested by Bonnefin et al in this special issue [49] – but 
should not influence designs for driverless cars or any other autonomous 
systems. 
 
Even if the technical problem of machines able to resolve real-world ethical 
dilemmas were solved society-wide debate would then be needed to discuss and 
agree on the rules and protocols for such machines, not least because society as a 
whole needs to take responsibility for the human causalities of accidents caused 
by such machines. It is notable that the federal government of Germany has ruled 
that “In the event of unavoidable accident situations, any distinction based on 
personal features (age, gender, physical or mental constitution) is strictly 
prohibited. It is also prohibited to offset victims against one another” [44]. 
 
7. How should we govern ethical machines? 
 
To recap: all intelligent autonomous systems should be regarded as ethical 
impact agents. Of course some may have no ethical impact whatsoever, but it 
would be foolhardy to assume this.  Ideally all systems should be first, subjected 



to an ethical risk assessment, of the kind set out in standard BS 8611 Guide to the 
ethical design of robots and robotics systems [5], and second, redesigned to 
reduce the impact of any ethical risks exposed by that risk assessment. Those 
systems that are shown, through ethical risk assessment, to have some ethical 
impact (and we would wager that this would be almost all systems), which are 
then redesigned to avoid or minimise that impact, move into the category of 
implicit ethical machines. 
 
It follows that we regard ethical risk assessment as the cornerstone of ethical 
governance. However, ethical risk assessment on its own is not enough, 
especially for implicit ethical machines. One of the general ethical principles set 
out in Ethically Aligned Design [6] concerns transparency; the principle asserts 
that it should always be possible to find out how and why an autonomous system 
made a particular decision. This kind of transparency is not a property of 
autonomous systems by default. It needs to be designed in, alongside sub-
systems for securely logging system inputs, outputs and decisions – the robot/AI 
equivalent of an aircraft flight data recorder [45]. Without transparency 
discovering the causes of, for instance, a driverless car accident, or misdiagnosis 
by a medical diagnosis AI, becomes all but impossible. That process of discovery 
is vital if the faults that caused the accident are to be fixed, and accountability 
established [46].  
 
Consider now the governance of explicitly ethical machines. We already have 
very high expectations for the safety and reliability of our machines – especially 
those that have the potential to cause serious harm if they go wrong – but if and 
when real-world robots and AIs are explicitly ethical those expectations will be 
even higher. Because of this additional burden of expectation on explicitly ethical 
machines, their governance will need to be especially robust. 
 
Explicitly ethical machines require two governance considerations over and 
above those needed for implicitly ethical machines. The first is in the choice of 
ethical rules or – if those rules are learned – the choice of training cases. Those 
ethical rules or training cases need to be carefully scrutinised, ideally by a panel 
of users, ethicists, lawyers and representatives of civil society. The second 
concerns the transparency of the ethical decision making process – it is critical 
that each ethical decision should be logged for later analysis and that it should be 
possible to understand why the system made those decisions. This is not only in 
the event that a system fails or causes harm. We believe that explicitly ethical 
machines should be subject to a ‘probationary period’, during which all ethical 
decisions are reviewed (perhaps by the same panel that scrutinises the system’s 
ethical rules). Only after this period is successfully concluded would monitoring 
revert to the baseline of fully investigating actual or near miss accidents. Given 
that explicitly ethical machines are neither inevitable nor imminent, these 
considerations do not have the same urgency as those suggested for implicit 
ethical machines. 
 
It is clear that all ethical machines need to be developed and used responsibly 
[37], and – for those that are ethically-critical – subject to standards and strong 
regulatory frameworks (many of which do not yet exist). In particular 



transparency should extend beyond the ethical machines themselves, to 
encompass the processes of design and operation, within frameworks of ethical 
governance [7]. Without such frameworks it is hard to see how ethical machines 
will be trusted. 
 
8. The papers of this special issue 
 
Adamson et al contribute a paper titled: “Designing a Values-Driven Future for 
Ethical Autonomous and Intelligent Systems” [47] in which they argue that 
human values must drive our future autonomous systems in a way that both 
protects and benefits humanity. The paper describes IEEE’s work in this area and 
includes a table of approximately 50 activities within the IEEE related to ethics. 
 
Anderson et al contribute a paper entitled “A Value-Driven Eldercare Robot: 
Virtual and Physical Instantiations of a Case-Supported Principle-Based Behavior 
Paradigm” [48]. The paper describes both simulated and real-robot 
implementations of an eldercare robot in which ethical principles are learned, 
via inductive logical programming, from a set of training examples provided by a 
project ethicist using GenEth: a general ethical dilemma analyzer. 
 
Bonnefon et al provide an insightful point of view article titled: “The trolley, the 
bull bar, and why engineers should care about the ethics of autonomous cars” 
[49]. This paper, which brings attention to what the authors call ‘the statistical 
trolley dilemma’, is the only paper of the special issue focussed on autonomous 
vehicles 
 
Bremner et al contribute a paper titled: “On Proactive, Transparent and 
Verifiable Ethical Reasoning for Robots” [50] in which they review and update an 
approach to the design of ethical robots based on a simulation-based internal 
model. This model allows the robot to anticipate when another robot – acting as 
a proxy human – might be at risk of harm and intervene if necessary; the ethical 
robot’s reasoning is both transparent and verifiable. 
 
Cave et al, present a paper aptly titled: “Motivations and Risks of Machine Ethics” 
[51]. In this paper the authors clarify various philosophical issues surrounding 
the concept of an ethical machine and the aims of machine ethics. The authors 
argue that while there are good prima facie reasons for pursuing machine ethics, 
there are also potential risks that must be considered and managed. 
 
Ema et al, an 11 person research team based in Japan, present the paper 
“Clarifying Privacy, Property, and Power: Case Study on Value Conflict Between 
Communities” [52]. Based around a controversial case study on the ‘flaming’ of 
fan fiction, which was complicated by the ambiguous legal position of such fan 
fiction content in Japan, the paper aims to clarify notions of privacy and draw 
lessons for the ethical governance of (ethical) AI in the presence of value 
conflicts, through interdisciplinary collaboration. 
 
Robertson et al contribute a paper titled: “Engineering based ethical design 
methodology for embedding ethics in autonomous robots” [53]. The paper 



explores the process of robotics and autonomous systems development using a 
co-design approach to reduce end-user risk with respect to an endoscopic 
capsule for diagnosis and drug delivery. The contribution of the paper is a 
method for embedding ethics into the design of a machine in a socio-technical 
application. 
 
“Understanding Engineers’ Drivers and Impediments for Ethical System 
Development: The Case of Privacy and Security Engineering” is a paper 
contributed by Spiekermann et al [54]. The study surveys 124 engineers in order 
to understand the drivers and impediments facing ethical systems development 
with respect to privacy and security engineering. Their findings indicate that 
while many engineers regard security and privacy as important, they (a) do not 
enjoy working on them, and (b) struggle with their organizational environment. 
 
In their point-of-view paper “Toward the Agile and Comprehensive International 
Governance of AI and Robotics” [55], Wallach and Marchant propose an agile 
ethical/legal model for the international and national governance of AI and 
robotics, building on their recommendations on the formation of Governance 
Coordinating Committees (GCCs) for coordinated oversight of emerging 
technologies.  
 
References 
 
[1] Schwab K. (2017) The fourth industrial revolution. Portfolio Penguin. 

 
[2] Dutton T (2018) An Overview of National AI Strategies, Medium, July 2018 

https://medium.com/politics-ai/an-overview-of-national-ai-strategies-
2a70ec6edfd 

 
[3] Boden, M., Bryson, J., Caldwell, D., Dautenhahn, K., Edwards, L., Kember, S., 

Newman, P., et al. (2017) Principles of robotics: Regulating robots in the 
real world. Connection Science, 29 (2). pp. 124-129. 

 
[4] Boddington P. (2017) Towards a code of ethics for artificial intelligence. 

Cham, Switzerland: Springer. 
 

[5] BS8611:2016 (2016) Robots and robotic devices: guide to the ethical 
design and application of robots and robotic systems. British Standards 
Institute, London. 

 
[6] IEEE Standards Association (2017) Ethically aligned design: a vision for 

prioritizing human well-being with autonomous and intelligent systems 
(A/IS), version 2. See https://ethicsinaction.ieee.org/ 

 
[7] Winfield AF and Jirotka M (2018) Ethical governance is essential to building 

trust in robotics and artificial intelligence systems, Phil. Trans. Royal 
Society A, 376, 20180085. 

 
[8] Asimov I. I, Robot. Gnome Press, 1950.  



 
[9] Wallach W and Allen C. (2009) Moral Machines: Teaching Robots Right 

from Wrong. Oxford: Oxford University Press. 
 

[10] Frayn M (1965), The Tin Men, Collins. 
 

[11] Cave S and Dihal K (2019), Hopes and fears for intelligent machines in 
fiction and reality, Nature Machine Intelligence, 1: 74-78. 

 
[12] Forester T and Morrison P (1994) Computer ethics: cautionary tales and 

ethical dilemmas in computing, MIT Press. 
 

[13] Allen C, Varner G and Zinser J (2000), Prolegomena to any future artificial 
moral agent, JETAI 12, 251-261. 

 
[14] Allen, C., Smit, I. and Wallach, W (2005) Artificial morality: Top-down, 

bottom-up, and hybrid approaches, Ethics and Information Technology 7, 
149-155. 

 
[15] Asaro PM (2006) What should we want from a Robot Ethic?, International 

Review of Information Ethics, 6 (12): 9-16. 
 

[16] Moor JH. (2006) The nature, importance, and difficulty of machine ethics. 
IEEE Intelligent Systems 21:18–21. 

 
[17] Powers TM (2006), Prospects for a Kantian Machine, in IEEE Intelligent 

Systems, vol. 21, no. 4, pp. 46-51, July-Aug. 2006. doi: 10.1109/MIS.2006.77 
 

[18] Anderson M, and Anderson SL, eds. (2006). Special Issue on Machine Ethics. 
IEEE Intelligent Systems 21(4) (July/August). 

 
[19] Anderson, M. and Anderson, S.L., eds., (2011) Machine Ethics, Cambridge 

University Press. 
 

[20] Arkin RC (2009) Governing Lethal Behavior in Autonomous Systems, 
Routledge. 

 
[21] Adamson G, Kline RR, Michael K and Michael MG (2015), Wiener's 

Cybernetics Legacy and the Growing Need for the Interdisciplinary 
Approach, in Proc. IEEE, vol. 103, no. 11, pp. 2208-2214. 

 
[22] Anderson, M. and Anderson, S.L. (2014): GenEth: A General Ethical 

Dilemma Analyzer, in Proc. 28th AAAI Conference on Artificial Intelligence, 
253-261. 

 
[23] Winfield AF, Blum C, and Liu W (2014) Towards an ethical robot: internal 

models, consequences and ethical action selection. In Lecture Notes in 
Computer Science, 8717, pp. 85–96. Berlin, Germany: Springer. 

 



[24] Vanderelst, D. and Winfield, A. F. (2018) An architecture for ethical robots 
inspired by the simulation theory of cognition. Cognitive Systems Research, 
48. pp. 56-66. 

 
[25] Bringsjord S, Sundar N, Thero D, and Si M. (2014) Akratic robots and the 

computational logic thereof. In Proc. IEEE 2014 Int. Symposium on Ethics in 
Engineering, Science, and Technology, pages 7:1–7:8, Piscataway, NJ, USA. 

 
[26] Briggs G and Scheutz M (2015) “Sorry, I can’t do that”: Developing 

mechanisms to appropriately reject directives in Human-Robot 
Interactions, in Proc AAAI Fall Symposium Series. 

 
[27] Shim J, Arkin RC and Pettinatti M (2017) An Intervening Ethical Governor 

for a robot mediator in patient-caregiver relationship: Implementation and 
Evaluation. 2017 IEEE International Conference on Robotics and 
Automation (ICRA), Singapore, pp 2936-2942. 

 
[28] Bringsjord S, Arkoudas K, and Bello P. (2006) Toward a general logicist 

methodology for engineering ethically correct robots. IEEE Intelligent 
Systems, 21(4):38–44. 

 
[29] Dyrkolbotn S, Pedersen T and Slavkovik M (2018) On the distinction 

between implicit and explicit ethical agency, in Proc AAAI/ACM conf. on AI, 
Ethics and Society. 

 
[30] Sharkey A. (2017) Can we program or train robots to be good?, Ethics Inf 

Tech. doi: 10.1007/s10676-017-9425-5 
 

[31] Helbing D. et al. (2019) Will Democracy Survive Big Data and Artificial 
Intelligence? In: Helbing D. (eds) Towards Digital Enlightenment. Springer. 

 
[32] Caliskan, A., Bryson, JJ. Narayanan, A. (2017) Semantics derived 

automatically from language corpora contain human-like biases, Science 
Vol 356, Issue 1334, pp183-186. 

 
[33] Neff, G. and Nagy, P. (2016) Symbiotic Agency and the Case of Tay. Int. J. of 

Communication, vol 10, p. 17. 
 

[34] Wolf MJ, Miller KW, and Grodzinsky FS. (2017). Why we should have seen 
that coming: comments on Microsoft's tay "experiment," and wider 
implications. ACM SIGCAS Computers and Society 47(3):54-64. doi: 
10.1145/3144592.3144598 

 
[35] Charisi V, Habibovic A, Andersson J, Li J, and Evers V (2017) Children's 

Views on Identification and Intention Communication of Self-driving 
Vehicles, In Proc. 2017 ACM Conf. on Interaction Design and Children, pp 
399-404, ACM. 

 



[36] Brundage, M., Avin, S., Clark, J. Toner, H., Eckersley, P., Garfinkel, B., Dafoe, 
A. et al (2018) The malicious use of artificial intelligence: Forecasting, 
prevention, and mitigation, arXiv preprint arXiv:1802.07228 

 
[37] Jirotka M, Grimpe B, Stahl B, Eden G, and Hartswood M. (2017) Responsible 

research and innovation in the digital age. Commun. ACM 60, 62–68.  
 

[38] Reilly-Cooper R (2015) Commentary on Towards an ethical robot: internal 
models, consequences and ethical action selection. Personal 
communication. 

 
[39] Arnold, T. and Scheutz, M. (2016) Against the moral Turing test: 

accountable design and the moral reasoning of autonomous systems, Ethics 
Inf. Technol. 18: 103. https://doi.org/10.1007/s10676-016-9389-x 

 
[40] Thomson JJ (1985), The Trolley Problem, Yale Law Journal, Vol 94, p1395. 
 

[41] Rescher N (1966). Distributive Justice. Bobbs-Merrill. 
 

[42] Pitt J, Busquets D, and Macbeth S (2014) Distributive Justice for Self-
Organised Common-Pool Resource Management, TAAS 9(3): 14:1-14:39, 
2014. 

 
[43] Awad E, Dsouza S, Kim R, Schulz J, Henrich J, Shariff A, Bonnefon J-F and 

Rahwan I (2018) The Moral Machine experiment, Nature, vol 563, pp59–64 
 

[44] BMVI (2017) Ethics Commission: Automated and Connected Driving, 
German Federal Ministry of Transport, June 2017. 

 
[45] Winfield AF and Jirotka M (2017) The case for an ethical black box. Lecture 

Notes in Computer Science, 10454, pp 262–273, Springer. 
 

[46] Wachter S, Mittelstadt B and Floridi L (2017), Transparent, explainable, and 
accountable AI for robotics, Science Robotics 2(6). 

 
[47] Adamson G, Havens JC and Chatila R (2019) Designing a Value-Driven 

Future for Ethical Autonomous and Intelligent Systems," Proceedings of the 
IEEE, doi: 10.1109/JPROC.2018.2884923 

 
[48] Anderson M, Anderson SL and Berenz V (2019), A Value-Driven Eldercare 

Robot: Virtual and Physical Instantiations of a Case-Supported Principle-
Based Behavior Paradigm," Proceedings of the IEEE, doi: 
10.1109/JPROC.2018.2840045 

 
[49] Bonnefon J-F, Shariff A and Rahwan I (2019) The trolley, the bull bar, and 

why engineers should care about the ethics of autonomous cars, Proc. IEEE. 
 

[50] Bremner P, Dennis LA, Fisher M and Winfield AF (2019), On Proactive, 
Transparent and Verifiable Ethical Reasoning for Robots, Proc IEEE. 



 
[51] Cave S, Nyrup R, Vold K and Weller A (2019), Motivations and Risks of 

Machine Ethics, Proceedings of the IEEE, doi: 
10.1109/JPROC.2018.2865996 

 
[52] Ema A et al. (2019), Clarifying Privacy, Property, and Power: Case Study on 

Value Conflict Between Communities, in Proceedings of the IEEE, doi: 
10.1109/JPROC.2018.2837045 

 
[53] Robertson LJ, Abbas R, Alici G, Munoz A and Michael K (2019), Engineering-

Based Design Methodology for Embedding Ethics in Autonomous Robots, in 
Proceedings of the IEEE. doi: 10.1109/JPROC.2018.2889678 

 
[54] Spiekermann S, Korunovska J and Langheinrich M (2019), Inside the 

Organization: Why Privacy and Security Engineering Is a Challenge for 
Engineers, in Proceedings of the IEEE. doi: 10.1109/JPROC.2018.2866769 

 
[55] Wallach W and Marchant G (2019) Toward the Agile and Comprehensive 

International Governance of AI and Robotics, Proc IEEE.﻿The Role and Limits of Principles in AI Ethics: 

Towards a Focus on Tensions 

Jess Whittlestone, Rune Nyrup, Anna Alexandrova and Stephen Cave 
Leverhulme Centre for the Future of Intelligence, University of Cambridge 

 
 
 

Abstract • Technology companies aiming to develop their 
The last few years have seen a proliferation of principles for own ethical guidelines (e.g. Google’s “AI Ethics” 
AI ethics. There is substantial overlap between different sets principles”); 
of principles, with widespread agreement that AI should be • Professional bodies, whose codes of ethics are 
used for the common good, should not be used to harm people aimed at guiding practitioners. 
or undermine their rights, and should respect widely held 

• Standards-setting bodies that aim to set general 
values such as fairness, privacy, and autonomy. While 

standards for fields of research or industries (e.g. 
articulating and agreeing on principles is important, it is only 
a starting point. Drawing on comparisons with the field of the IEEE or British Standards Institution); 
bioethics, we highlight some of the limitations of principles: • Government bodies and legislators that aim to 
in particular, they are often too broad and high-level to guide develop policy and regulation (e.g. the UK’s new 
ethics in practice. We suggest that an important next step for Centre for Data Ethics and Innovation, which 
the field of AI ethics is to focus on exploring the tensions that advises the Government on regulation required 
inevitably arise as we try to implement principles in practice. across sectors); 
By explicitly recognising these tensions we can begin to 
make decisions about how they should be resolved in specific • Researchers across disciplines whose work aims 
cases, and develop frameworks and guidelines for AI ethics to inform these ways that AI ethics is put into 
that are rigorous and practically relevant. We discuss some practice: exploring the technical, philosophical, or 
different specific ways that tensions arise in AI ethics, and legal aspects of using algorithms in ethical ways 
what processes might be needed to resolve them. (see e.g. Selbst and Powles 2018) or synthesizing 

and translating such research into practice (see e.g. 
Winfield and Jirotka 2018). 

 Introduction    
AI systems promise widespread benefits to society, while Agreeing on principles is valuable for the aims of all of 

also posing substantial risks across almost all sectors. In the these groups: for example, principles can provide a useful 

last few years, a number of different groups and initiatives starting point from which to develop more formal standards 

have attempted to articulate and agree principles to guide the and regulation, and can help to identify priority issues on 

application of AI in society. In this paper we discuss the role which both research and policy should focus. However, we 

and limitations of these principles, and argue that an argue that current lists of principles for AI ethics are too 

important next step for AI ethics is to focus more on the high-level to be immediately useful for these groups’ aims. 

tensions that arise as we try to implement them in practice. When we look at specific cases, it becomes clear that 

By ‘AI ethics’ we specifically mean the emerging field of principles will come into conflict with each other. This 

practical AI ethics, which focuses on developing means their practical value is limited: without 

frameworks and guidelines to ensure the ethical use of AI in acknowledging these conflicts standards may be set 

society (analogous to the field of biomedical ethics, which unrealistically high, or regulation intended to protect one 

provides practical frameworks for ethical practice in value might inadvertently compromise other important 

medicine.) AI ethics therefore covers several different goals. In order to be practically useful, we suggest that the 

sectors and types of institutions, including the following: field of AI ethics should focus more on identifying and 

 attempting to resolve the tensions that arise when we apply 
them to specific cases.  

                                                 
Copyright © 2019, Association for the Advancement of Artificial  
Intelligence (www.aaai.org). All rights reserved. 



We begin by reviewing how principles have evolved in should respect widely-held values such as fairness, privacy, 
AI ethics over the last two years. Drawing on comparisons and autonomy. Cowls and Floridi (2018) suggest that many 
with bioethics - a field with a robust and well-developed of the different existing sets can be synthesised into five key 
tradition in using principles to govern medical practice - we principles: the four that are already used in bioethics - 
discuss some of the limitations of principles. We make the autonomy, beneficence, non-maleficence, and justice 
case that all areas of AI ethics would benefit from a more (Beauchamp and Childress 2001) - plus the additional 
rigorous exploration of the tensions that arise when we try principle of explicability, which captures the challenges of 
to apply principles to concrete cases. We outline some key intelligibility and accountability unique to AI systems. 
tensions that already arise from the use of AI in society, and While this convergence is encouraging, it is unclear at this 
discuss what work might be needed to resolve them. To our point whether this reflects a deep consensus about what is 
knowledge, this is the first paper to explicitly examine the important, arrived at independently by numerous different 
role and limits of principles in AI ethics, and the importance actors, or merely a shallow consensus due to the fact that 
of focusing more on tensions as a next step. different groups have read similar papers and built on the 

work of one another.  
Principles can be a valuable part of applied ethics; 

The Emergence of Principles in AI Ethics agreeing on high-level principles is therefore an important 
Though the field is in its infancy, there is widespread step for ensuring that AI is developed and used for the 

agreement on some of the core issues (such as bias) and benefit of society. Principles help condense complex ethical 

values (such as fairness) that AI ethics should focus on. Over issues into a few central elements which can be clearly 

the last two years, these have begun to be codified in sets of understood and agreed upon by people from diverse fields 

‘principles’ or ‘tenets’. The Asilomar AI principles, and sectors. They encourage widespread commitment to a 

developed in 2017 in conjunction with the Asilomar shared set of values, and can give them a more prominent 

conference for Beneficial AI, outline guidelines on how role in institutional decision-making processes. Principles 

research should be conducted, ethics and values that use of can form a basis for more formal commitments in 

AI must respect, and important considerations for thinking professional ethics, internationally agreed standards, and 

about long-term issues (Future of Life Institute 2017). The regulation. They can also help address public concerns, by 

principles were signed by several thousand AI researchers clarifying the ethical commitments of researchers and 

and others, including many academic ethicists and social industry.  

scientists. Around the same time, the US Association for However, while principles are important, they are not in 

Computing Machinery (ACM) issued a statement and set of themselves enough to ensure society can reap the benefits 

seven principles for Algorithmic Transparency and and mitigate the risks of new technologies. In order to be 

Accountability, addressing a narrower but closely related set useful in practice, principles need to be able to guide action 

of issues (ACM US Public Policy Council 2017). - to help people navigate the competing demands and 

Over the course of 2017, several other initiatives and considerations of concrete situations. As we articulate in the 

organisations published additional sets of principles: next section, there are several obstacles to this. 

including the Japanese Society for Artificial Intelligence’s 
Ethical Guidelines in February 2017 (JSAI 2017); a set of 

The Limits of Principles 
draft principles from the Montréal Declaration on 
Responsible AI in November (University of Montréal The four principles of beneficence, non-maleficence, 
2017); and the IEEE’s General Principles of Ethical autonomy, and justice have played a prominent role in 
Autonomous and Intelligent Systems in December (IEEE bioethics (Beauchamp and Childress 2001), a field with 
2017). This proliferation of principles has continued into decades of experience in managing the challenges posed by 
2018: with the Partnership on AI publishing a set of “tenets” new technologies. These principles aim to articulate general 
which its members agree to uphold (Partnership on AI values on which everyone can agree, and to function as 
2018); the UK House of Lords suggesting five principles for practical guidelines. But they have spurred substantial 
a cross-sector AI code which could be adopted debate: some argue that we should put no weight on 
internationally (Select Committee on Artificial Intelligence principles and focus entirely on the elements of specific 
2018), and Google publishing their “AI ethics principles” in cases (e.g. Dancy 2004), while others have advocated a 
June (Pichai 2018). more moderate view, whereby principles should be 

These different sets of principles have considerable considered in close conjunction with analysis of ‘paradigm’ 
overlap. There is widespread agreement that AI-based cases (Johnson and Toulmin 1998). 
technologies should be used for the common good, should However, even the strongest advocates of principlism in 
not be used to harm people or undermine their rights, and bioethics acknowledge that principles alone are not enough. 



Beauchamp and Childress (2001) suggest that principles for which a ‘full and satisfactory’ explanation cannot 
should be taken as guidelines, which need to be made necessarily be provided, depending on how this is defined. 
specific for use in policy and clinical decision-making.” In some situations, the benefit of using an algorithm may be 
They elaborate that in order to be action-guiding, principles high enough, and its accuracy reliable enough, that all users 
need to be accompanied by an account of how they apply in agree it is worth using even if a fully comprehensive 
specific situations, and how to balance them when they explanation of its decisions cannot be given. There are 
conflict. In this section, we review some of the main complex and important trade-offs involved here (Price 
limitations of principles that have been highlighted in the 2017; Selbst and Barocas 2018), and a principle that simply 
bioethics literature and illustrate why these also apply to the states that it is not acceptable to deploy AI systems without 
principles proposed for AI ethics. full explainability fails to recognise this. 

Principles are Highly General Different Groups May Interpret Principles 

By their nature, principles are highly general: their value is Differently 
that they indicate important moral themes that apply across A final problem for general principles is that the central 
a wide range of scenarios. This means that they can be useful terms they use are often ambiguous, masking conceptual 
as a kind of ‘checklist’, as a set of important considerations complexity and differences in interpretation across 
that need to be taken into account in specific scenarios. populations. As Clouser and Gert (1990) point out, the 
However, the generality of most principles also limits their principle of “justice” in bioethics does not say anything 
ability to guide practical action (Beauchamp 1995). about what is just or unjust, leaving this to the agent to 

Many of the principles proposed in AI ethics are too broad decide for themselves. Clouser and Gert argue that 
to be action-guiding. For example, ensuring that AI is used principles often mask important moral disagreements rather 
for “social good” or “the benefit of humanity” is a common than presenting a well-developed unified theory as they 
thread among all sets of principles. These are phrases on propose to, and that it would be better if these disagreements 
which a great majority can agree exactly because they carry were articulated and understood more explicitly. 
with them few if any real commitments. A very wide range In particular, lists of broadly agreed-upon principles 
of differing ideological, political and philosophical cannot recognise that important and legitimate differences 
standpoints could claim to be for the good, or for the benefit in values exist across people and populations. While 
of humanity. Therefore such principles, rather than everyone might agree in principle that ‘fairness’ is 
representing the outcome of meaningful debate on how AI important, there exist deep political disagreements about 
should be developed, risk simply postponing it. Only what exactly constitutes fairness (Binns 2017). Groups may 
principles that are narrower and more specific are likely to also vary in how much weight they put on one value relative 
be useful in practice. Recent industry commitments to not to others in situations of conflict: more individualist cultures 
develop technology for autonomous weapons are an may put more weight on personal privacy than more 
example of a principle that is specific, action-guiding and collectivist cultures, for example.  
can be used to hold people to account. But at the same time, An important step in making principles more practical is 
exactly that specificity means its relevance is limited to one to ‘formalize’ them into standards and regulation (Winfield 
sector, and it has many dissenters.  and Jirotka 2018). But this process is not a straightforward 

unpacking of the relevant principles, as different principles 
Principles Come into Conflict in Practice will come into conflict when applied to concrete cases. In 

The gap between principles and practical judgement grows the next section, we make the case that in order for 

larger still when we consider that principles will inevitably principles to inform more practical aspects of AI ethics, 

conflict with each other. For example, the UK House of including professional ethics, standards, and regulation, we 

Lords AI Committee report states that, “it is not acceptable need to begin by exploring in detail the different kinds of 

to deploy any artificial intelligence system which could have tensions that arise when principles are applied. 

a substantial impact on an individual’s life, unless it can 
generate a full and satisfactory explanation for the decisions 

Why The Field Should Focus on Tensions 
it will take.” The intentions behind this principle are 
important, but it masks an important tension between using We use the term “tension” to refer to any conflict, whether 
algorithms for social benefit (‘beneficence’) and ensuring apparent, contingent or fundamental, between important 
those algorithms are fully intelligible to humans values or goals, where it appears necessary to give up one in 
(‘explicability’). For example, algorithms exist today that order to realise the other. For example, the use of socially 
can diagnose medical conditions more accurately than beneficial data-driven technologies might make it 
doctors, potentially saving lives (e.g. Song et al. 2018), but impossible for us to fully guarantee otherwise desirable 



levels of data privacy. If the potential gains of these to develop standards and regulation that are more sensitive 
technologies are significant enough - new and highly to how principles apply differently across scenarios. 
effective cancer treatments, say - we might decide that a 
higher risk of privacy breaches is a price worth paying. b. Acknowledging Differences in Values 

In some cases, a tension may reflect a strict moral trade- Focusing on tensions forces us to consider how different 
off: a situation where two values or goals conflict and it is values might be interpreted and endorsed differently across 
not possible to get more of one without sacrificing another. groups. While some important tensions are due to conflicts 
However, many tensions in AI are more contingent, and between principles in practice, others arise because there are 
arise as a result of current technological or societal conflicting meanings or values within a single principle: 
constraints. Using machine learning for social benefit may broad terms like “fairness” or “justice” for example are 
not be fundamentally in tension with privacy, transparency, subject to substantial moral and political disagreement 
or fairness, but many current methods employed for the (Binns 2017; Clouser and Gert 1990). It may never be 
former goal do conflict with these ideals. We do not yet possible to totally resolve all of these disagreements. But 
know how far new technological or governance solutions clearly articulating them is a crucial starting point for 
could go to dissolve these tensions. ensuring that all aspects of AI ethics are as inclusive as 

Others have acknowledged the importance of recognising possible: for example, to ensure that international standards 
conflicts between values in AI ethics, but to our knowledge take full account of and accommodate cultural differences, 
none have explored in detail why this would be beneficial or and that agreement on such standards is meaningful. 
what it would look like in practice. For example, Cowls and 
Floridi (2018) say that, “Ensuring socially preferable 

c. Highlighting Areas Where New Solutions Are 
outcomes of AI relies on resolving the tension between 

Needed 
incorporating the benefits and mitigating the potential harms 
of AI, in short, simultaneously avoiding the misuse and Noting a tension between two values does not necessarily 

underuse of these technologies”, but do not discuss specific mean we are forced to choose between them: often, we may 

tensions in detail or how to resolve them. be able to find some way to get more of both things we 

In this section, we discuss some of the benefits of value. Recognising these tensions can therefore highlight 

focusing on tensions. We outline four reasons this is an high priority areas for both researchers and policymakers. 

important next step for AI ethics: (a) bridging the gap For example, acknowledging that there is currently a trade-

between principles and practice, (b) acknowledging off between performance and interpretability in state-of-the-

differences in values, (c) highlighting areas where new art machine learning systems has motivated technical 

solutions are needed and (d) identifying ambiguities and research that attempts to reduce or eliminate this trade-off 

knowledge gaps. (Adel, Ghahramani, and Weller 2018). 
Acknowledging tensions will also help direct the 

a. Bridging the Gap between Principles and development of AI in beneficial directions more generally. 
It is currently far from clear whether advances in AI will 

Practice 
augment or degrade human capabilities and agency, but 

In general, we see focussing on tensions as an important way making this tension explicit focuses attention on the 
of bridging the gap between abstract ethical principles and important question of which trajectories of development are 
specific cases, and therefore an important first step towards most likely to lead to the former. 
an ethics of AI that is practical and action-guiding. 

To identify tensions, we need to consider how different 
d. Identifying Ambiguities and Knowledge Gaps 

values and goals might come into practice in concrete cases. 
For example, when Google DeepMind collaborated with the Finally, a tension-focused approach helps to clearly 

Royal Free Hospital, they encountered a conflict between highlight ambiguities and gaps in our understanding of how 

protecting the privacy of patient data, and their goal of using uses of AI are impacting society. Again, this can help 

AI to improve early diagnosis of acute kidney injury identify new and important research directions.  

(Powles and Hodson, 2017). Since similar tensions will To think clearly about all tensions, we need to recognise 

likely arise across a range of different cases, focusing on and clarify ambiguities in terms: what do we really mean by 

tensions means we are neither driven entirely by the things like ‘fairness’, ‘justice’, and ‘autonomy’, and how 

specifics of an individual case, nor are we relying on abstract might these be interpreted differently across groups and 

high-level values. If we can articulate important tensions by contexts? To understand the nature of many tensions we 

looking at a range of cases, and find ways to resolve them in need to understand what is currently technically possible: 

specific scenarios, what we learn from this can then be used what are the best current methods for ensuring data privacy 
in machine learning, for example, and what are the costs of 



these methods? To understand how tensions arise in precise direction in which technology develops, it could be 
practice, we need better evidence on how AI is actually used to either greatly enhance human capabilities - if we can 
being applied in society today: what effect is automation develop sophisticated methods of intelligence augmentation 
already having on individual lives across different sectors? (Carter and Nielsen 2017) - or to degrade them: if our own 
And to articulate and resolve conflicts between the interests capabilities atrophy as we outsource more and more tasks 
of different groups, we need to really understand the needs (Carter 2018). As mentioned above, automation might 
and values of affected communities: how do the trade-offs enhance the agency of some groups while threatening the 
people are willing to make differ based on demographic autonomy of others: whether we see AI as enhancing or 
factors, for example? Focussing on tensions should help to degrading human agency could depend on how narrow or 
drive this important work. global a view we take of its impacts. 

Four Key Tensions 
Which Tensions? 

Given the wide range of tensions that may arise from 
There are several different ways that applications of AI can applications of AI, now or in the future, there is unlikely to 
introduce tensions between important goals and values. be an exhaustive list of all possible tensions. However, we 

Some tensions arise due to the very nature of AI and believe that the following four tensions will be particularly 
machine learning: these techniques allow us to use and draw central to thinking about the ethical issues arising from the 
inferences from very large amounts of (often personal) data, applications of AI systems in society today. These capture a 
and so challenge important notions of privacy. The most range of issues which are already salient or likely to grow in 
useful models also often quickly become very complex, importance moving forward. 
introducing new issues around human interpretability  
(Doshi-Velez and Kim 2017; Weller 2017). This means we Tension 1: Using data to improve the quality and 
face tensions between using these technologies for socially efficiency of services vs. respecting privacy and autonomy 
beneficial goals: improving healthcare, justice, or security, of individuals. Machine learning and big data are already 
for example, and other goals such as respecting privacy and being used to improve various public services (including 
maintaining trust and understanding in automated systems. healthcare, education, and social care). These improvements 

Another possibility is that AI systems exacerbate already could be hugely beneficial to citizens, but require large 
existing ethical or societal tensions: between different amounts of personal data, raising concerns about how to best 
conflicting notions of fairness, for example. Often this is due protect privacy and ensure meaningful consent. 
to the fact that machine learning models are trained on Tension 2: Using algorithms to make decisions and 
historical data, and so inherit the biases or mistakes they predictions more accurate vs ensuring fair and equal 
contain (Hajian et al. 2016). Here, applications of AI in treatment. This tension arises when public or private bodies 
society do not necessarily introduce new tensions, but base decisions on predictions about future behaviour of 
increase the importance of already-existing ones such as individuals (e.g. when probation officers estimate risk of 
how to make decision-making more accurate and efficient reoffending) and when they employ machine learning 
without inadvertently discriminating against minority algorithms to improve their predictions. These algorithms 
groups. may improve accuracy overall, but discriminate against 

Other tensions arise because the harms and benefits of AI specific subgroups for whom representative data is not 
systems are unequally distributed in various ways. For available. 
example, the impacts of automation may be unequally Tension 3: Reaping the benefits of increased 
distributed across populations and cultures: enhancing the personalisation in the digital sphere vs enhancing solidarity 
agency of some groups by automating mundane tasks while and citizenship. Companies and governments can use 
wiping out the livelihood of others, thus threatening their personal data to tailor the messages, offers, and services 
basic needs. The risks and benefits of AI systems could also people see. This personalisation can make it easier for 
be unequally distributed over time, and uses of AI that people to find the right products and services for them, but 
present opportunities in the near term may compromise differentiating between people in such fine-grained ways 
important long-term values. Increasing personalisation of may threaten societal ideals of citizenship and solidarity.  
messages and services may make our lives more convenient Tension 4: Using automation to make people’s lives 
and enjoyable in the short run, but begin to undermine more convenient and empowered vs promoting self-
important aspects of autonomy, equality and solidarity over actualisation and dignity. Automated solutions may 
time (Prainsack and Buyx 2017). genuinely improve people’s lives by saving them time on 

Finally, AI may have the potential to both enhance and mundane tasks that could be better spent on more rewarding 
threaten a given value. For example, depending on the activities. But they also risk disrupting some of the practices 



that are an important part of what makes us human. With conflicting values, and investing in further research could 
automation we may see the gifts of arts, languages and identify solutions that better serve all relevant values or 
science become more accessible to those who were excluded goals. For example, it might be possible to use automation 
in the past - but we may also see widespread deskilling, to improve people’s lives without sacrificing self-
atrophy, ossification of practices, homogenisation and actualisation and devaluing human skills, if a clear line can 
cultural diversity. be drawn between the contexts where we do and do not want 

to pursue automation.  
Identifying Further Tensions In these situations we face a choice. Even if a tension 

 between two goals is not a fundamentally irresolvable one, 

The above tensions are important and represent areas where if we want to apply current technology in society, we will 

exploring tensions is likely to be fruitful for AI ethics. Going still need to make the kinds of trade-offs described above. 

forward, further such areas can and should be identified. In On the other hand, if we can hold-off from implementing 

order to do so, it is helpful to ask a range of questions, technologies that introduce tensions - certain kinds of 

including: automation, say - then we could instead invest in more 

 research on how technological or governance solutions 
might reduce the need to navigate potentially difficult value 

• Where AI is being used to serve a particular goal 
or value, or for ‘social benefit’ in general, what trade-offs. Of course, this is not a binary choice: we can 

risks to other values are introduced?  choose to strike a balance by making the trade-offs 

• Where might uses of AI that benefit one group, or necessary to implement technology where doing so is 

the population as a whole, have negative relatively unproblematic, while still investing in research to 

consequences for a specific subgroup? How do we explore how these tensions might be resolvable in future. 
balance the interests of different groups? This choice can be thought of as involving its own tension, 

• Where might applications of AI that are beneficial between short- and long-term interests: to what extent 
in the near-term introduce risks in the long-term? should we postpone the benefits of new technologies in 
How do we trade-off short and long-term impacts order to invest the time and resources necessary to resolve 
of society? the tensions they introduce?  

• Where might future developments in AI either 
enhance or threaten important values, depending 
on the direction they take?  Conclusion 

We suggest that an important next step for making AI ethics 

Resolving Tensions practical is to focus more on the tensions that arise when 
high-level principles are applied to concrete cases. 

The best approach to resolving a tension will depend on the Though most of these tensions cannot be resolved 
nature of the tension in question.  straightforwardly, we believe articulating them more clearly 

Where a strict trade-off between two values exists, a and explicitly has several benefits. To be useful in practice, 
choice must be made to prioritise one set of values over principles need to be formalized in standards, codes and 
another. For example, this may mean judging what risks to ultimately regulation. To be effective, these in turn must 
privacy it is acceptable to incur for the sake of better public acknowledge that there are tensions between the different 
health, or where to reject innovative automation high-level goals of AI ethics, and provide some guidance on 
technologies because the threats they pose to human skills how they should be resolved in different scenarios. They 
and autonomy are too great. also need to acknowledge and accommodate different 

Making these trade-off judgements will be a complex perspectives and values as far as possible, if they are to 
political process. Weighing the costs and benefits of reflect genuine agreement.  
different solutions can be an important part of the process A focus on tensions can also help to direct research 
but alone is not enough, since it fails to recognise that values priorities in AI ethics. Articulating tensions can help to 
are vague and unquantifiable, and that numbers often hide highlight important ambiguities and gaps in our 
complex value judgements. In addition, resolving trade-offs understanding of how AI is currently being applied in 
will require extensive public engagement, to give voice to a society which need further research. More generally, much 
wide range of stakeholders and articulate their interests with current research in AI ethics appears to be driven by 
rigour and respect.  questions of how we ensure that uses of AI respect important 

On the other hand, where tensions are more ‘practical’ in values, such as privacy, transparency, or fairness. 
nature, strict trade-offs may not be inevitable. It may be that Reframing research questions to be more focused on 
we simply lack the knowledge or tools to advance understanding and resolving tensions is an important step 



towards solving practical problems arising from the use of Jonsen, A. R., & Toulmin, S. E. 1988. The abuse of casuistry: A 
AI in society, since it directs attention to where new history of moral reasoning. Univ of California Press. 

technological or governance solutions might help push the Partnership on AI. 2018. Tenets. 

development of AI in robustly beneficial directions. https://www.partnershiponai.org/tenets/ 
Pichai. S. 2018. AI at Google: Our Principles. 
https://www.blog.google/technology/ai/ai-principles/ 

Acknowledgements Powles, J., & Hodson, H. 2017. Google DeepMind and healthcare 
in an age of algorithms. Health and technology, 7(4), 351-367. 

This work was funded by the Nuffield Foundation and by a Prainsack, B., & Buyx, A. 2017. Solidarity in biomedicine and 
Leverhulme Trust Research Centre Grant. beyond (Vol. 33). Cambridge University Press. 

Price, W., & Nicholson, I. I. 2017. Regulating Black-Box 
Medicine. Mich. L. Rev., 116, 421. 

References 
Select Committee on Artificial Intelligence. (2018). AI in the UK: 

 Ready, Willing, and Able? HL 100 2017-19. London: House of 
Lords.  

ACM US Public Policy Council. 2017. Statement on Algorithmic 
Transparency and Accountability. Association for Computing Song, M., Yang, Y., He, J., Yang, Z., Yu, S., Xie, Q., ... & Cui, Y. 
Machinery. https://www.acm.org/binaries/content/assets/public- 2018. Prognostication of disorders of consciousness using brain 
policy/2017_usacm_statement_algorithms.pdf functional networks and clinical features. arXiv preprint 

arXiv:1801.03268. 
Adel, T., Ghahramani, Z., & Weller, A. 2018. Discovering 
Interpretable Representations for Both Deep Generative and The IEEE Global Initiative for Ethical Considerations in Artificial 
Discriminative Models. In International Conference on Machine Intelligence and Autonomous Systems. 2017. Ethically Aligned 
Learning, 50-59. Design, Version 2, Technical Report, IEEE, New York City, NY. 

Beauchamp, T. L. 1995. Principlism and its alleged University of Montréal. 2017. Montréal Declaration on 
competitors. Kennedy Institute of Ethics Journal, 5(3), 181-198. Responsible AI. https://www.montrealdeclaration-

responsibleai.com/the-declaration 
Beauchamp, T. L., & Childress, J. F. 2001. Principles of 
biomedical ethics. Oxford University Press, USA. Weller, A. 2017. Challenges for transparency. arXiv preprint 

arXiv:1708.01870. 
Binns, R. 2017. Fairness in Machine Learning: Lessons from 
Political Philosophy. arXiv preprint arXiv:1712.03586 Winfield, A. F., & Jirotka, M. 2018. Ethical governance is essential 

to building trust in robotics and artificial intelligence systems. Phil. 
Carter, J. A. 2018. Autonomy, cognitive offloading and 

Trans. R. Soc. A, 376(2133), 20180085. 
education. Educational Theory. 

 
Carter, S., & Nielsen, M. 2017. Using artificial intelligence to 
augment human intelligence. Distill, 2(12), e9.  

Clouser, K. D., & Gert, B. 1990. A critique of principlism. The  
Journal of Medicine and Philosophy, 15(2), 219-236.  
Cowls, J., & Floridi, L. 2018. Prolegomena to a White Paper on an 
Ethical Framework for a Good AI Society. Available at 
SSRN: https://ssrn.com/abstract=3198732 

 
Dancy, J. 2004. Ethics without principles. Oxford University Press 

 
on Demand. 

 
Doshi-Velez, F., & Kim, B. 2017. Towards a rigorous science of 
interpretable machine learning. arXiv preprint arXiv:1702.08608.  

Friedler, S. A., Scheidegger, C., & Venkatasubramanian, S. 2016.  
On the (im) possibility of fairness. arXiv preprint  
arXiv:1609.07236.  
Future of Life Institute. 2017. Asilomar AI Principles.  
https://futureoflife.org/ai-principles/ 

 
Hajian, S., Bonchi, F., & Castillo, C. 2016. Algorithmic bias: From 
discrimination discovery to fairness-aware data mining.  

In Proceedings of the 22nd ACM SIGKDD international  
conference on knowledge discovery and data mining, 2125-2126.  
ACM. 

 
Japanese Society for Artificial Intelligence (JSAI). 2017. The 
Japanese Society for Artificial Intelligence Ethical Guidelines.  

http://ai-elsi.org/archives/514 

Jonsen, A. R. 1995. Casuistry: an alternative or complement to 
principles?. Kennedy Institute of Ethics Journal, 5(3), 237-251.﻿Minds and Machines (2018) 28:689–707
https://doi.org/10.1007/s11023-018-9482-5

AI4People—An Ethical Framework for a Good AI Society: 
Opportunities, Risks, Principles, and Recommendations

Luciano Floridi1,2  · Josh Cowls1,2 · Monica Beltrametti3 · Raja Chatila4,5 · 
Patrice Chazerand6 · Virginia Dignum7,8 · Christoph Luetge9 ·  
Robert Madelin10 · Ugo Pagallo11 · Francesca Rossi12,13 · Burkhard Schafer14 · 
Peggy Valcke15,16 · Effy Vayena17

Received: 28 October 2018 / Accepted: 2 November 2018 / Published online: 26 November 2018 
© The Author(s) 2018

Abstract
This article reports the findings of AI4People, an Atomium—EISMD initia-
tive designed to lay the foundations for a “Good AI Society”. We introduce the core 
opportunities and risks of AI for society; present a synthesis of five ethical principles 
that should undergird its development and adoption; and offer 20 concrete recom-
mendations—to assess, to develop, to incentivise, and to support good AI—which in 
some cases may be undertaken directly by national or supranational policy makers, 
while in others may be led by other stakeholders. If adopted, these recommendations 
would serve as a firm foundation for the establishment of a Good AI Society.

Keywords Artificial intelligence · AI4People · Data governance · Digital ethics · 
Governance · Ethics of AI

1 Introduction

AI is not another utility that needs to be regulated once it is mature. It is a power-
ful force, a new form of smart agency, which is already reshaping our lives, our 
interactions, and our environments. AI4People was set up to help steer this powerful 
force towards the good of society, everyone in it, and the environments we share. 
This article is the outcome of the collaborative effort by the AI4People Scientific 

*  Luciano Floridi 
 luciano.floridi@oii.ox.ac.uk
Extended author information available on the last page of the article

Vol.:(012134 56789)



 690 L. Floridi et al.

Committee—comprising 12 experts and chaired by Luciano Floridi1—to propose a 
series of recommendations for the development of a Good AI Society.

The article synthesises three things: the opportunities and associated risks that 
AI technologies offer for fostering human dignity and promoting human flourish-
ing; the principles that should undergird the adoption of AI; and 20 specific recom-
mendations that, if adopted, will enable all stakeholders to seize the opportunities, 
to avoid or at least minimise and counterbalance the risks, to respect the principles, 
and hence to develop a Good AI Society.

The article is structured around four more sections after this introduction. Sec-
tion 2 states the core opportunities for promoting human dignity and human flourish-
ing offered by AI, together with their corresponding risks.2 Section 3 offers a brief, 
high-level view of the advantages for organisations of taking an ethical approach 
to the development and use of AI. Section 4 formulates 5 ethical principles for AI, 
building on existing analyses, which should undergird the ethical adoption of AI in 
society at large. Finally, Sect. 5 offers 20 recommendations for the purpose of devel-
oping a Good AI Society in Europe.

Since the launch of AI4People in February 2018, the Scientific Committee has 
acted collaboratively to develop the recommendations in the final section of this 
paper. Through this work, we hope to have contributed to the foundation of a Good 
AI Society we can all share.

2 T he Opportunities and Risks of AI for Society

That AI will have a major impact on society is no longer in question. Current debate 
turns instead on how far this impact will be positive or negative, for whom, in which 
ways, in which places, and on what timescale. Put another way, we can safely dis-
pense with the question of whether AI will have an impact; the pertinent questions 
now are by whom, how, where, and when this positive or negative impact will be felt.

In order to frame these questions in a more substantive and practical way, we 
introduce here what we consider the four chief opportunities for society that AI 
offers. They are four because they address the four fundamental points in the under-
standing of human dignity and flourishing: who we can become (autonomous self-
realisation); what we can do (human agency); what we can achieve (individual and 
societal capabilities); and how we can interact with each other and the world (soci-
etal cohesion). In each case, AI can be used to foster human nature and its potentiali-
ties, thus creating opportunities; underused, thus creating opportunity costs; or over-
used and misused, thus creating risks. As the terminology indicates, the assumption 

1 Besides Luciano Floridi, the members of the Scientific Committee are: Monica Beltrametti, Raja Cha-
tila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca 
Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena. Josh Cowls is the rapporteur. Thomas Burri 
contributed to an earlier draft.
2 The analysis in this and the following two sections is also available in Cowls and Floridi (2018). Fur-
ther analysis and more information on the methodology employed will be presented in Cowls and Floridi 
(Forthcoming).

1 3



AI4People—An Ethical Framework for a Good AI Society:… 691

Fig. 1  Overview of the four core opportunities offered by AI, four corresponding risks, and the opportu-
nity cost of underusing AI

is that the use of AI is synonymous with good innovation and positive applications 
of this technology. However, fear, ignorance, misplaced concerns or excessive reac-
tion may lead a society to underuse AI technologies below their full potential, for 
what might be broadly described as the wrong reasons. This may cause significant 
opportunity costs. It might include, for example, heavy-handed or misconceived 
regulation, under-investment, or a public backlash akin to that faced by genetically 
modified crops (Imperial College 2017). As a result, the benefits offered by AI tech-
nologies may not be fully realised by society. These dangers arise largely from unin-
tended consequences and relate typically to good intentions gone awry. However, we 
must also consider the risks associated with inadvertent overuse or wilful misuse of 
AI technologies, grounded, for example, in misaligned incentives, greed, adversarial 
geopolitics, or malicious intent. Everything from email scams to full-scale cyber-
warfare may be accelerated or intensified by the malicious use of AI technologies 
(Taddeo 2018). And new evils may be made possible (King et al. 2018). The possi-
bility of social progress represented by the aforementioned opportunities above must 
be weighed against the risk that malicious manipulation will be enabled or enhanced 
by AI. Yet a broad risk is that AI may be underused out of fear of overuse or misuse. 
We summarise these risks in Fig. 1 below, and offer a more detailed explanation in 
the text that follows.

2.1  Who We Can Become: Enabling Human Self‑Realisation, Without Devaluing 
Human Abilities

AI may enable self-realisation, by which we mean the ability for people to flour-
ish in terms of their own characteristics, interests, potential abilities or skills, 
aspirations, and life projects. Much as inventions, such as the washing machine, 
liberated people—particularly women—from the drudgery of domestic work, 
the “smart” automation of other mundane aspects of life may free up yet more 
time for cultural, intellectual and social pursuits, and more interesting and 
rewarding work. More AI may easily mean more human life spent more intel-
ligently. The risk in this case is not the obsolescence of some old skills and the 

1 3



 692 L. Floridi et al.

emergence of new ones per se, but the pace at which this is happening and the 
unequal distributions of the costs and benefits that result. A very fast devalua-
tion of old skills and hence a quick disruption of the job market and the nature 
of employment can be seen at the level of both the individual and society. At the 
level of the individual, jobs are often intimately linked to personal identity, self-
esteem, and social role or standing, all factors that may be adversely affected by 
redundancy, even putting to one side the potential for severe economic harm. 
Furthermore, at the level of society, the deskilling in sensitive, skill-intensive 
domains, such as health care diagnosis or aviation, may create dangerous vulner-
abilities in the event of AI malfunction or an adversarial attack. Fostering the 
development of AI in support of new abilities and skills, while anticipating and 
mitigating its impact on old ones will require both close study and potentially 
radical ideas, such as the proposal for some form of “universal basic income”, 
which is growing in popularity and experimental use. In the end, we need some 
intergenerational solidarity between those disadvantaged today and those advan-
taged tomorrow, to ensure that the disruptive transition between the present and 
the future will be as fair as possible, for everyone.

2.2 W hat We Can Do: Enhancing Human Agency, Without Removing Human 
Responsibility

AI is providing a growing reservoir of “smart agency”. Put at the service of 
human intelligence, such a resource can hugely enhance human agency. We can 
do more, better, and faster, thanks to the support provided by AI. In this sense of 
“Augmented Intelligence”, AI could be compared to the impact that engines have 
had on our lives. The larger the number of people who will enjoy the opportuni-
ties and benefits of such a reservoir of smart agency “on tap”, the better our soci-
eties will be. Responsibility is therefore essential, in view of what sort of AI we 
develop, how we use it, and whether we share with everyone its advantages and 
benefits. Obviously, the corresponding risk is the absence of such responsibility. 
This may happen not just because we have the wrong socio-political framework, 
but also because of a “black box” mentality, according to which AI systems for 
decision-making are seen as being beyond human understanding, and hence con-
trol. These concerns apply not only to high-profile cases, such as deaths caused 
by autonomous vehicles, but also to more commonplace but still significant uses, 
such as in automated decisions about parole or creditworthiness.

Yet the relationship between the degree and quality of agency that people 
enjoy and how much agency we delegate to autonomous systems is not zero-
sum, either pragmatically or ethically. In fact, if developed thoughtfully, AI 
offers the opportunity of improving and multiplying the possibilities for human 
agency. Consider examples of “distributed morality” in human-to-human sys-
tems such as peer-to-peer lending (Floridi 2013). Human agency may be ulti-
mately supported, refined and expanded by the embedding of “facilitating frame-
works”, designed to improve the likelihood of morally good outcomes, in the set 

1 3



AI4People—An Ethical Framework for a Good AI Society:… 693

of functions that we delegate to AI systems. AI systems could, if designed effec-
tively, amplify and strengthen shared moral systems.

2.3 W hat We Can Achieve: Increasing Societal Capabilities, Without Reducing 
Human Control

Artificial intelligence offers myriad opportunities for improving and augmenting 
the capabilities of individuals and society at large. Whether by preventing and cur-
ing diseases or optimising transportation and logistics, the use of AI technologies 
presents countless possibilities for reinventing society by radically enhancing what 
humans are collectively capable of. More AI may support better coordination, and 
hence more ambitious goals. Human intelligence augmented by AI could find new 
solutions to old and new problems, from a fairer or more efficient distribution of 
resources to a more sustainable approach to consumption. Precisely because such 
technologies have the potential to be so powerful and disruptive, they also introduce 
proportionate risks. Increasingly, we may not need to be either ‘in or on the loop’ 
(that is, as part of the process or at least in control of it), if we can delegate our 
tasks to AI. However, if we rely on the use of AI technologies to augment our own 
abilities in the wrong way, we may delegate important tasks and above all decisions 
to autonomous systems that should remain at least partly subject to human supervi-
sion and choice. This in turn may reduce our ability to monitor the performance of 
these systems (by no longer being ‘on the loop’ either) or preventing or redressing 
errors or harms that arise (‘post loop’). It is also possible that these potential harms 
may accumulate and become entrenched, as more and more functions are delegated 
to artificial systems. It is therefore imperative to strike a balance between pursuing 
the ambitious opportunities offered by AI to improve human life and what we can 
achieve, on the one hand, and, on the other hand, ensuring that we remain in control 
of these major developments and their effects.

2.4 H ow We Can Interact: Cultivating Societal Cohesion, Without Eroding Human 
Self‑Determination

From climate change and antimicrobial resistance to nuclear proliferation and fun-
damentalism, global problems increasingly have high degrees of coordination 
complexity, meaning that they can be tackled successfully only if all stakeholders 
co-design and co-own the solutions and cooperate to bring them about. AI, with 
its data-intensive, algorithmic-driven solutions, can hugely help to deal with such 
coordination complexity, supporting more societal cohesion and collaboration. For 
example, efforts to tackle climate change have exposed the challenge of creating a 
cohesive response, both within societies and between them. The scale of this chal-
lenge is such that we may soon need to decide between engineering the climate 
directly and designing societal frameworks to encourage a drastic cut in harmful 
emissions. This latter option might be undergirded by an algorithmic system to cul-
tivate societal cohesion. Such a system would not be imposed from the outside; it 

1 3



 694 L. Floridi et al.

would be the result of a self-imposed choice, not unlike our choice of not buying 
chocolate if we had earlier chosen to be on a diet, or setting up an alarm clock to 
wake up. “Self-nudging” to behave in socially preferable ways is the best form of 
nudging, and the only one that preserves autonomy. It is the outcome of human deci-
sions and choices, but it can rely on AI solutions to be implemented and facilitated. 
Yet the risk is that AI systems may erode human self-determination, as they may 
lead to unplanned and unwelcome changes in human behaviours to accommodate 
the routines that make automation work and people’s lives easier. AI’s predictive 
power and relentless nudging, even if unintentional, should be at the service of 
human self-determination and foster societal cohesion, not undermining human dig-
nity or human flourishing.

Taken together, these four opportunities, and their corresponding challenges, 
paint a mixed picture about the impact of AI on society and the people in it. Accept-
ing the presence of trade-offs, seizing the opportunities while working to anticipate, 
avoid, or minimise the risks head-on will improve the prospect for AI technologies 
to promote human dignity and flourishing. Having outlined the potential benefits 
to individuals and society at large of an ethically engaged approach to AI, in the 
next section we highlight the “dual advantage” to organisations of taking such an 
approach.

3  The Dual Advantage of an Ethical Approach to AI

Ensuring socially preferable outcomes of AI relies on resolving the tension between 
incorporating the benefits and mitigating the potential harms of AI, in short, simul-
taneously avoiding the misuse and underuse of these technologies. In this context, 
the value of an ethical approach to AI technologies comes into starker relief. Com-
pliance with the law is merely necessary (it is the least that is required), but signifi-
cantly insufficient (it is not the most than can and should be done) (Floridi 2018). 
With an analogy, it is the difference between playing according to the rules, and 
playing well, so that one may win the game. Adopting an ethical approach to AI con-
fers what we define here as a “dual advantage”. On one side, ethics enables organi-
sations to take advantage of the social value that AI enables. This is the advantage 
of being able to identify and leverage new opportunities that are socially acceptable 
or preferable. On the other side, ethics enables organisations to anticipate and avoid 
or at least minimise costly mistakes. This is the advantage of prevention and mitiga-
tion of courses of action that turn out to be socially unacceptable and hence rejected, 
even when legally unquestionable. This also lowers the opportunity costs of choices 
not made or options not grabbed for fear of mistakes.

Ethics’ dual advantage can only function in an environment of public trust and 
clear responsibilities more broadly. Public acceptance and adoption of AI technolo-
gies will occur only if the benefits are seen as meaningful and risks as potential, yet 
preventable, minimisable, or at least something against which one can be protected, 
through risk management (e.g. insurance) or redressing. These attitudes will depend 
in turn on public engagement with the development of AI technologies, openness 
about how they operate, and understandable, widely accessible mechanisms of 

1 3



AI4People—An Ethical Framework for a Good AI Society:… 695

regulation and redress. In this way, an ethical approach to AI can also be seen as 
an early warning system against risks that might endanger entire organisations. The 
clear value to any organisation of the dual advantage of an ethical approach to AI 
amply justifies the expense of engagement, openness, and contestability that such an 
approach requires.

4  A Unified Framework of Principles for AI in Society

AI4People is not the first initiative to consider the ethical implications of AI. Many 
organisations have already produced statements of the values or principles that 
should guide the development and deployment of AI in society. Rather than conduct 
a similar, potentially redundant exercise here, we strive to move the dialogue for-
ward, constructively, from principles to proposed policies, best practices, and con-
crete recommendations for new strategies. Such recommendations are not offered in 
a vacuum. But rather than generating yet another series of principles to serve as an 
ethical foundation for our recommendations, we offer a synthesis of existing sets of 
principles produced by various reputable, multi-stakeholder organisations and initia-
tives. A fuller explanation of the scope, selection and method of assessing these sets 
of principles is available in Cowls and Floridi (Forthcoming). Here, we focus on 
the commonalities and noteworthy differences observable across these sets of prin-
ciples, in view of the 20 recommendations offered in the rest of the paper. The docu-
ments we assessed are:

1. The Asilomar AI Principles, developed under the auspices of the Future of Life 
Institute, in collaboration with attendees of the high-level Asilomar conference 
of January 2017 (hereafter “Asilomar”; Asilomar AI Principles 2017);

2. The Montreal Declaration for Responsible AI, developed under the auspices of 
the University of Montreal, following the Forum on the Socially Responsible 
Development of AI of November 2017 (hereafter “Montreal”; Montreal Declara-
tion 2017)3;

3. The General Principles offered in the second version of Ethically Aligned Design: 
A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Sys-
tems. This crowd-sourced global treatise received contributions from 250 global 
thought leaders to develop principles and recommendations for the ethical devel-
opment and design of autonomous and intelligent systems, and was published in 
December 2017 (hereafter “IEEE”; IEEE 2017)4;

4. The Ethical Principles offered in the Statement on Artificial Intelligence, Robotics 
and ‘Autonomous’ Systems, published by the European Commission’s European 

3 The Montreal Declaration is currently open for comments as part of a redrafting exercise. The princi-
ples we refer to here are those which were publicly announced as of 1st May, 2018.
4 The third version of Ethically Aligned Design will be released in 2019 following wider public consul-
tation.

1 3



 696 L. Floridi et al.

Group on Ethics in Science and New Technologies, in March 2018 (hereafter 
“EGE”; EGE 2018);

5. The “five overarching principles for an AI code” offered in paragraph 417 of the 
UK House of Lords Artificial Intelligence Committee’s report, AI in the UK: 
ready, willing and able?, published in April 2018 (hereafter “AIUK”; House of 
Lords 2018); and

6. The Tenets of the Partnership on AI, a multistakeholder organisation consisting 
of academics, researchers, civil society organisations, companies building and 
utilising AI technology, and other groups (hereafter “the Partnership”; Partnership 
on AI 2018).

Taken together, they yield 47 principles.5 Overall, we find an impressive and reas-
suring degree of coherence and overlap between the six sets of principles. This can 
most clearly be shown by comparing the sets of principles with the set of four core 
principles commonly used in bioethics: beneficence, non-maleficence, autonomy, 
and justice. The comparison should not be surprising. Of all areas of applied ethics, 
bioethics is the one that most closely resembles digital ethics in dealing ecologi-
cally with new forms of agents, patients, and environments (Floridi 2013). The four 
bioethical principles adapt surprisingly well to the fresh ethical challenges posed 
by artificial intelligence. But they are not exhaustive. On the basis of the following 
comparative analysis, we argue that one more, new principle is needed in addition: 
explicability, understood as incorporating both intelligibility and accountability.

4.1 B eneficence: Promoting Well‑Being, Preserving Dignity, and Sustaining 
the Planet

Of the four core bioethics principles, beneficence is perhaps the easiest to observe 
across the six sets of principles we synthesise here. The principle of creating AI 
technology that is beneficial to humanity is expressed in different ways, but it typi-
cally features at the top of each list of principles. Montreal and IEEE principles both 
use the term “well-being”: for Montreal, “the development of AI should ultimately 
promote the well-being of all sentient creatures”; while IEEE states the need to “pri-
oritize human well-being as an outcome in all system designs”. AIUK and Asilomar 
both characterise this principle as the “common good”: AI should “be developed for 
the common good and the benefit of humanity”, according to AIUK. The Partner-
ship describes the intention to “ensure that AI technologies benefit and empower as 
many people as possible”; while the EGE emphasises the principle of both “human 
dignity” and “sustainability”. Its principle of “sustainability” represents perhaps the 

5 Of the six documents, the Asilomar Principles offer the largest number of principles with arguably 
the broadest scope. The 23 principles are organised under three headings, “research issues”, “ethics and 
values”, and “longer-term issues”. We have omitted consideration of the five “research issues” here as 
they are related specifically to the practicalities of AI development, particularly in the narrower context 
of academia and industry. Similarly, the Partnership’s eight Tenets consist of both intra-organisational 
objectives and wider principles for the development and use of AI. We include only the wider principles 
(the first, sixth, and seventh tenets).

1 3



AI4People—An Ethical Framework for a Good AI Society:… 697

widest of all interpretations of beneficence, arguing that “AI technology must be 
in line with … ensur[ing] the basic preconditions for life on our planet, continued 
prospering for mankind and the preservation of a good environment for future gen-
erations”. Taken together, the prominence of these principles of beneficence firmly 
underlines the central importance of promoting the well-being of people and the 
planet.

4.2 N on‑maleficence: Privacy, Security and “Capability Caution”

Though “do only good” (beneficence) and “do no harm” (non-maleficence) seem 
logically equivalent, in both the context of bioethics and of the ethics of AI they 
represent distinct principles, each requiring explication. While they encourage well-
being, the sharing of benefits and the advancement of the public good, each of the 
six sets of principles also cautions against the many potentially negative conse-
quences of overusing or misusing AI technologies. Of particular concern is the pre-
vention of infringements on personal privacy, which is listed as a principle in five of 
the six sets, and as part of the “human rights” principles in the IEEE document. In 
each case, privacy is characterised as being intimately linked to individuals’ access 
to, and control over, how personal data is used.

Yet the infringement of privacy is not the only danger to be avoided in the adop-
tion of AI. Several of the documents also emphasise the importance of avoiding the 
misuse of AI technologies in other ways. The Asilomar Principles are quite spe-
cific on this point, citing the threats of an AI arms race and of the recursive self-
improvement of AI, as well as the need for “caution” around “upper limits on future 
AI capabilities”. The Partnership similarly asserts the importance of AI operating 
“within secure constraints”. The IEEE document meanwhile cites the need to “avoid 
misuse”, while the Montreal Declaration argues that those developing AI “should 
assume their responsibility by working against the risks arising from their techno-
logical innovations”, echoed by the EGE’s similar need for responsibility.

From these various warnings, it is not entirely clear whether it is the people devel-
oping AI, or the technology itself, which should be encouraged not to do harm—in 
other words, whether it is Frankenstein or his monster against whose maleficence we 
should be guarding. Confused also is the question of intent: promoting non-malefi-
cence can be seen to incorporate the prevention of both accidental (what we above 
call “overuse”) and deliberate (what we call “misuse”) harms arising. In terms of the 
principle of non-maleficence, this need not be an either/or question: the point is sim-
ply to prevent harms arising, whether from the intent of humans or the unpredicted 
behaviour of machines (including the unintentional nudging of human behaviour 
in undesirable ways). Yet these underlying questions of agency, intent and control 
become knottier when we consider the next principle.

4.3  Autonomy: The Power to Decide (Whether to Decide)

Another classic tenet of bioethics is the principle of autonomy: the idea that indi-
viduals have a right to make decisions for themselves about the treatment they do or 

1 3



 698 L. Floridi et al.

not receive. In a medical context, this principle of autonomy is most often impaired 
when patients lack the mental capacity to make decisions in their own best interests; 
autonomy is thus surrendered involuntarily. With AI, the situation becomes rather 
more complex: when we adopt AI and its smart agency, we willingly cede some of 
our decision-making power to machines. Thus, affirming the principle of autonomy 
in the context of AI means striking a balance between the decision-making power 
we retain for ourselves and that which we delegate to artificial agents.

The principle of autonomy is explicitly stated in four of the six documents. 
The Montreal Declaration articulates the need for a balance between human- and 
machine-led decision-making, stating that “the development of AI should promote 
the autonomy of all human beings and control … the autonomy of computer sys-
tems” (italics added). The EGE argues that autonomous systems “must not impair 
[the] freedom of human beings to set their own standards and norms and be able to 
live according to them”, while AIUK adopts the narrower stance that “the auton-
omous power to hurt, destroy or deceive human beings should never be vested in 
AI”. The Asilomar document similarly supports the principle of autonomy, insofar 
as “humans should choose how and whether to delegate decisions to AI systems, to 
accomplish human-chosen objectives”.

These documents express a similar sentiment in slightly different ways, echo-
ing the distinction drawn above between beneficence and non-maleficence: not only 
should the autonomy of humans be promoted, but also the autonomy of machines 
should be restricted and made intrinsically reversible, should human autonomy need 
to be re-established (consider the case of a pilot able to turn off the automatic pilot 
and regain full control of the airplane). Taken together, the central point is to protect 
the intrinsic value of human choice—at least for significant decisions—and, as a 
corollary, to contain the risk of delegating too much to machines. Therefore, what 
seems most important here is what we might call “meta-autonomy”, or a “decide-to-
delegate” model: humans should always retain the power to decide which decisions 
to take, exercising the freedom to choose where necessary, and ceding it in cases 
where overriding reasons, such as efficacy, may outweigh the loss of control over 
decision-making. As anticipated, any delegation should remain overridable in prin-
ciple (deciding to decide again).

The decision to make or delegate decisions does not take place in a vacuum. Nor 
is this capacity to decide (to decide, and to decide again) distributed equally across 
society. The consequences of this potential disparity in autonomy are addressed in 
the final of the four principles inspired by bioethics.

4.4  Justice: Promoting Prosperity and Preserving Solidarity

The last of the four classic bioethics principles is justice, which is typically invoked 
in relation to the distribution of resources, such as new and experimental treatment 
options or simply the general availability of conventional healthcare. Again, this 
bioethics principle finds clear echoes across the principles for AI that we analyse. 
The importance of “justice” is explicitly cited in the Montreal Declaration, which 
argues that “the development of AI should promote justice and seek to eliminate all 

1 3



AI4People—An Ethical Framework for a Good AI Society:… 699

types of discrimination”, while the Asilomar Principles include the need for both 
“shared benefit” and “shared prosperity” from AI. Under its principle named “Jus-
tice, equity and solidarity”, the EGE argues that AI should “contribute to global 
justice and equal access to the benefits” of AI technologies. It also warns against 
the risk of bias in datasets used to train AI systems, and—unique among the docu-
ments—argues for the need to defend against threats to “solidarity”, including “sys-
tems of mutual assistance such as in social insurance and healthcare”. The emphasis 
on the protection of social support systems may reflect geopolitics, insofar as the 
EGE is a European body. The AIUK report argues that citizens should be able to 
“flourish mentally, emotionally and economically alongside artificial intelligence”. 
The Partnership, meanwhile, adopts a more cautious framing, pledging to “respect 
the interests of all parties that may be impacted by AI advances”.

As with the other principles already discussed, these interpretations of what jus-
tice means as an ethical principle in the context of AI are broadly similar, yet con-
tain subtle distinctions. Across the documents, justice variously relates to

(a) Using AI to correct past wrongs such as eliminating unfair discrimination;
(b) Ensuring that the use of AI creates benefits that are shared (or at least shareable); 

and
(c) Preventing the creation of new harms, such as the undermining of existing social 

structures.

Notable also are the different ways in which the position of AI, vis-à-vis people, 
is characterised in relation to justice. In Asilomar and EGE respectively, it is AI 
technologies themselves that “should benefit and empower as many people as pos-
sible” and “contribute to global justice”, whereas in Montreal, it is “the develop-
ment of AI” that “should promote justice” (italics added). In AIUK, meanwhile, 
people should flourish merely “alongside” AI. Our purpose here is not to split 
semantic hairs. The diverse ways in which the relationship between people and AI 
is described in these documents hints at broader confusion over AI as a man-made 
reservoir of “smart agency”. Put simply, and to resume our bioethics analogy, are we 
(humans) the patient, receiving the “treatment” of AI, the doctor prescribing it? Or 
both? It seems that we must resolve this question before seeking to answer the next 
question of whether the treatment will even work. This is the core justification for 
our identification within these documents of a new principle, one that is not drawn 
from bioethics.

4.5 E xplicability: Enabling the Other Principles Through Intelligibility 
and Accountability

The short answer to the question of whether “we” are the patient or the doctor is that 
actually we could be either—depending on the circumstances and on who “we” are 
in our everyday life. The situation is inherently unequal: a small fraction of human-
ity is currently engaged in the design and development of a set of technologies that 
are already transforming the everyday lives of just about everyone else. This stark 

1 3



 700 L. Floridi et al.

Fig. 2  An ethical framework for AI, formed of four traditional principles and a new one

reality is not lost on the authors whose documents we analyse. In all, reference is 
made to the need to understand and hold to account the decision-making processes 
of AI. This principle is expressed using different terms: “transparency” in Asilomar; 
“accountability” in EGE; both “transparency” and “accountability” in IEEE; “intel-
ligibility” in AIUK; and as “understandable and interpretable” for the Partnership. 
Though described in different ways, each of these principles captures something 
seemingly novel about AI: that its workings are often invisible or unintelligible to all 
but (at best) the most expert observers.

The addition of this principle, which we synthesise as “explicability” both in the 
epistemological sense of “intelligibility” (as an answer to the question “how does 
it work?”) and in the ethical sense of “accountability” (as an answer to the ques-
tion: “who is responsible for the way it works?”), is therefore the crucial missing 
piece of the jigsaw when we seek to apply the framework of bioethics to the ethics 
of AI. It complements the other four principles: for AI to be beneficent and non-
maleficent, we must be able to understand the good or harm it is actually doing to 
society, and in which ways; for AI to promote and not constrain human autonomy, 
our “decision about who should decide” must be informed by knowledge of how AI 
would act instead of us; and for AI to be just, we must ensure that the technology—
or, more accurately, the people and organisations developing and deploying it—are 
held accountable in the event of a negative outcome, which would require in turn 
some understanding of why this outcome arose. More broadly, we must negotiate 
the terms of the relationship between ourselves and this transformative technology, 
on grounds that are readily understandable to the proverbial person “on the street”.

Taken together, we argue that these five principles capture the meaning of each of 
the 47 principles contained in the six high-profile, expert-driven documents, form-
ing an ethical framework within which we offer our recommendations below. This 
framework of principles is shown in Fig. 2.

5  Recommendations for a Good AI Society

This section introduces the Recommendations for a Good AI Society. It consists of 
two parts: a Preamble, and 20 Action Points.

1 3



AI4People—An Ethical Framework for a Good AI Society:… 701

There are four kinds of Action Points: to assess, to develop, to incentivise and to 
support. Some recommendations may be undertaken directly, by national or Euro-
pean policy makers, in collaboration with stakeholders where appropriate. For oth-
ers, policy makers may play an enabling role for efforts undertaken or led by third 
parties.

5.1  Preamble

We believe that, in order to create a Good AI Society, the ethical principles identi-
fied in the previous section should be embedded in the default practices of AI. In 
particular, AI should be designed and developed in ways that decrease inequality 
and further social empowerment, with respect for human autonomy, and increase 
benefits that are shared by all, equitably. It is especially important that AI be expli-
cable, as explicability is a critical tool to build public trust in, and understanding of, 
the technology.

We also believe that creating a Good AI Society requires a multistakeholder 
approach, which is the most effective way to ensure that AI will serve the needs of 
society, by enabling developers, users and rule-makers to be on board and collabo-
rating from the outset.

Different cultural frameworks inform attitudes to new technology. This docu-
ment represents a European approach, which is meant to be complementary to other 
approaches. We are committed to the development of AI technology in a way that 
secures people’s trust, serves the public interest, and strengthens shared social 
responsibility.

Finally, this set of recommendations should be seen as a “living document”. The 
Action Points are designed to be dynamic, requiring not simply single policies or 
one-off investments, but rather, continuous, ongoing efforts for their effects to be 
sustained.

5.2  Action Points

5.2.1 A ssessment

1. Assess the capacity of existing institutions, such as national civil courts, to redress 
the mistakes made or harms inflicted by AI systems. This assessment should 
evaluate the presence of sustainable, majority-agreed foundations for liability 
from the design stage onwards, in order to reduce negligence and conflicts (see 
also Recommendation 5).6

2. Assess which tasks and decision-making functionalities should not be delegated to 
AI systems, through the use of participatory mechanisms to ensure alignment with 

6 Determining accountability and responsibility may usefully borrow from lawyers in Ancient Rome 
who would go by this formula ‘cuius commoda eius et incommoda’ (‘the person who derives an advan-
tage from a situation must also bear the inconvenience’). A good 2200 years old principle that has a well-
established tradition and elaboration could properly set the starting level of abstraction in this field.

1 3



 702 L. Floridi et al.

societal values and understanding of public opinion. This assessment should take 
into account existing legislation and be supported by ongoing dialogue between 
all stakeholders (including government, industry, and civil society) to debate how 
AI will impact society opinion (in concert with Recommendation 17).

3. Assess whether current regulations are sufficiently grounded in ethics to provide a 
legislative framework that can keep pace with technological developments. This 
may include a framework of key principles that would be applicable to urgent 
and/or unanticipated problems.

5.2.2 D evelopment

4. Develop a framework to enhance the explicability of AI systems that make 
socially significant decisions. Central to this framework is the ability for individu-
als to obtain a factual, direct, and clear explanation of the decision-making pro-
cess, especially in the event of unwanted consequences. This is likely to require 
the development of frameworks specific to different industries, and professional 
associations should be involved in this process, alongside experts in science, 
business, law, and ethics.

5. Develop appropriate legal procedures and improve the IT infrastructure of the 
justice system to permit the scrutiny of algorithmic decisions in court. This is 
likely to include the creation of a framework for AI explainability as indicated in 
Recommendation 4, specific to the legal system. Examples of appropriate proce-
dures may include the applicable disclosure of sensitive commercial information 
in IP litigation, and—where disclosure poses unacceptable risks, for instance to 
national security—the configuration of AI systems to adopt technical solutions by 
default, such as zero-knowledge proofs in order to evaluate their trustworthiness.

6. Develop auditing mechanisms for AI systems to identify unwanted consequences, 
such as unfair bias, and (for instance, in cooperation with the insurance sector) 
a solidarity mechanism to deal with severe risks in AI-intensive sectors. Those 
risks could be mitigated by multistakeholder mechanisms upstream. Pre-digital 
experience indicates that, in some cases, it may take a couple of decades before 
society catches up with technology by way of rebalancing rights and protec-
tion adequately to restore trust. The earlier that users and governments become 
involved—as made possible by ICT—the shorter this lag will be.

7. Develop a redress process or mechanism to remedy or compensate for a wrong 
or grievance caused by AI. To foster public trust in AI, society needs a widely 
accessible and reliable mechanism of redress for harms inflicted, costs incurred, 
or other grievances caused by the technology. Such a mechanism will necessarily 
involve a clear and comprehensive allocation of accountability to humans and/or 
organisations. Lessons could be learnt from the aerospace industry, for example, 
which has a proven system of handling unwanted consequences thoroughly and 
seriously. The development of this process must follow from the assessment of 
existing capacity outlined in Recommendation 1. If a lack of capacity is identi-
fied, additional institutional solutions should be developed at national and/or EU 
levels, to enable people to seek redress. Such solutions may include:

1 3



AI4People—An Ethical Framework for a Good AI Society:… 703

• An “AI ombudsperson” to ensure the auditing of allegedly unfair or inequita-
ble uses of AI;

• A guided process for registering a complaint akin to making a Freedom of 
Information request; and

• The development of liability insurance mechanisms, which would be required 
as an obligatory accompaniment of specific classes of AI offerings in EU and 
other markets. This would ensure that the relative reliability of AI-powered 
artefacts, especially in robotics, is mirrored in insurance pricing and therefore 
in the market prices of competing products.7

Whichever solutions are developed, these are likely to rely on the framework for 
intelligibility proposed in Recommendation 4.

 8. Develop agreed-upon metrics for the trustworthiness of AI products and ser-
vices, to be undertaken either by a new organisation, or by a suitable existing 
organisation. These metrics would serve as the basis for a system that enables 
the user-driven benchmarking of all marketed AI offerings. In this way, an index 
for trustworthy AI can be developed and signalled, in addition to a product’s 
price. This “trust comparison index” for AI would improve public understanding 
and engender competitiveness around the development of safer, more socially 
beneficial AI (e.g., “IwantgreatAI.org”). In the longer term, such a system could 
form the basis for a broader system of certification for deserving products and 
services, administered by the organisation noted here, and/or by the oversight 
agency proposed in Recommendation 9. The organisation could also support 
the development of codes of conduct (see Recommendation 18). Furthermore, 
those who own or operate inputs to AI systems and profit from it could be tasked 
with funding and/or helping to develop AI literacy programs for consumers, in 
their own best interest.

 9. Develop a new EU oversight agency responsible for the protection of public wel-
fare through the scientific evaluation and supervision of AI products, software, 
systems, or services. This may be similar, for example, to the European Medi-
cines Agency. Relatedly, a “post-release” monitoring system for AIs similar to, 
for example, the one available for drugs should be developed, with reporting 
duties for some stakeholders and easy reporting mechanisms for other users.

 10. Develop a European observatory for AI. The mission of the observatory would 
be to watch developments, provide a forum to nurture debate and consensus, 
provide a repository for AI literature and software (including concepts and links 
to available literature), and issue step-by-step recommendation and guidelines 
for action.

1 1. Develop legal instruments and contractual templates to lay the foundation for a 
smooth and rewarding human–machine collaboration in the work environment. 

7 Of course, to the extent that AI systems are ‘products’, general tort law still applies in the same way to 
AI as it applies in any instance involving defective products or services that injure users or do not per-
form as claimed or expected.

1 3



 704 L. Floridi et al.

Shaping the narrative on the ‘Future of Work’ is instrumental to winning “hearts 
and minds”. In keeping with ‘A Europe that protects’, the idea of “inclusive 
innovation” and to smooth the transition to new kinds of jobs, a European AI 
Adjustment Fund could be set up along the lines of the European Globalisation 
Adjustment Fund.

5.2.3 I ncentivisation

1 2. Incentivise financially, at the EU level, the development and use of AI tech-
nologies within the EU that are socially preferable (not merely acceptable) and 
environmentally friendly (not merely sustainable but favourable to the environ-
ment). This will include the elaboration of methodologies that can help assess 
whether AI projects are socially preferable and environmentally friendly. In this 
vein, adopting a ‘challenge approach’ (see DARPA challenges) may encourage 
creativity and promote competition in the development of specific AI solutions 
that are ethically sound and in the interest of the common good.

1 3. Incentivise financially a sustained, increased and coherent European research 
effort, tailored to the specific features of AI as a scientific field of investigation. 
This should involve a clear mission to advance AI for social good, to serve as a 
unique counterbalance to AI trends with less focus on social opportunities.

1 4. Incentivise financially cross-disciplinary and cross-sectoral cooperation and 
debate concerning the intersections between technology, social issues, legal 
studies, and ethics. Debates about technological challenges may lag behind the 
actual technical progress, but if they are strategically informed by a diverse, 
multistakeholder group, they may steer and support technological innovation in 
the right direction. Ethics should help seize opportunities and cope with chal-
lenges, not only describe them. It is essential in this respect that diversity infuses 
the design and development of AI, in terms of gender, class, ethnicity, discipline 
and other pertinent dimensions, in order to increase inclusivity, toleration, and 
the richness of ideas and perspectives.

1 5. Incentivise financially the inclusion of ethical, legal and social considerations 
in AI research projects. In parallel, incentivise regular reviews of legislation to 
test the extent to which it fosters socially positive innovation. Taken together, 
these two measures will help ensure that AI technology has ethics at its heart 
and that policy is oriented towards innovation.

1 6. Incentivise financially the development and use of lawfully de-regulated special 
zones within the EU for the empirical testing and development of AI systems. 
These zones may take the form of a “living lab” (or Tokku), building on the 
experience of existing “test highways” (or Teststrecken). In addition to aligning 
innovation more closely with society’s preferred level of risk, sandbox experi-
ments such as these contribute to hands-on education and the promotion of 
accountability and acceptability at an early stage. “Protection by design” is 
intrinsic to this kind of framework.

 17. Incentivise financially research about public perception and understanding of 
AI and its applications, and the implementation of structured public consulta-
tion mechanisms to design policies and rules related to AI. This may include 

1 3



AI4People—An Ethical Framework for a Good AI Society:… 705

the direct elicitation of public opinion via traditional research methods, such 
as opinion polls and focus groups, as well as more experimental approaches, 
such as providing simulated examples of the ethical dilemmas introduced by AI 
systems, or experiments in social science labs. This research agenda should not 
serve merely to measure public opinion, but should also lead to the co-creation 
of policies, standards, best practices, and rules as a result.

5.2.4  Support

 18. Support the development of self-regulatory codes of conduct for data and AI 
related professions, with specific ethical duties. This would be along the lines 
of other socially sensitive professions, such as medical doctors or lawyers, i.e., 
with the attendant certification of ‘ethical AI’ through trust-labels to make sure 
that people understand the merits of ethical AI and will therefore demand it 
from providers. Current attention manipulation techniques may be constrained 
through these self-regulating instruments.

 19. Support the capacity of corporate boards of directors to take responsibility for 
the ethical implications of companies’ AI technologies. For example, this may 
include improved training for existing boards and the potential development 
of an ethics committee with internal auditing powers. This could be developed 
within the existing structure of both one-tier and two-tier board systems, and/or 
in conjunction with the development of a mandatory form of “corporate ethical 
review board” to be adopted by organisations developing or using AI systems, 
to evaluate initial projects and their deployment with respect to fundamental 
principles.

2 0. Support the creation of educational curricula and public awareness activities 
around the societal, legal, and ethical impact of Artificial Intelligence. This may 
include:

• Curricula for schools, supporting the inclusion of computer science among 
the basic disciplines to be taught;

• Initiatives and qualification programmes in businesses dealing with AI tech-
nology, to educate employees on the societal, legal, and ethical impact of 
working alongside AI;

• A European-level recommendation to include ethics and human rights in the 
degrees of data and AI scientists and other scientific and engineering curric-
ula dealing with computational and AI systems;

• The development of similar programmes for the public at large, with a spe-
cial focus on those involved at each stage of management of the technology, 
including civil servants, politicians and journalists;

• Engagement with wider initiatives such as the ITU AI for Good events and 
NGOs working on the UN Sustainable Development Goals.

1 3



 706 L. Floridi et al.

6 C onclusion

Europe, and the world at large, face the emergence of a technology that holds much 
exciting promise for many aspects of human life, and yet seems to pose major threats 
as well. This article—and especially the Recommendations in the previous section—
seek to nudge the tiller in the direction of ethically and socially preferable outcomes 
from the development, design and deployment of AI technologies. Building on our 
identification of both the core opportunities and the risks of AI for society as well as 
the set of five ethical principles we synthesised to guide its adoption, we formulated 
20 Action Points in the spirit of collaboration and in the interest of creating concrete 
and constructive responses to the most pressing social challenges posed by AI.

With the rapid pace of technological change, it can be tempting to view the politi-
cal process in the liberal democracies of today as old-fashioned, out-of-step, and no 
longer up to the task of preserving the values and promoting the interests of society 
and everyone in it. We disagree. With the Recommendations we offer here, includ-
ing the creation of centres, agencies, curricula, and other infrastructure, we have 
made the case for an ambitious, inclusive, equitable programme of policy making 
and technological innovation, which we believe will contribute to securing the ben-
efits and mitigating the risks of AI, for all people, and for the world we share.

Acknowledgements This publication would not have been possible without the generous support of Ato-
mium—European Institute for Science, Media and Democracy. We are particularly grateful to Michelan-
gelo Baracchi Bonvicini, Atomium’s President, to Guido Romeo, its Editor in Chief, the staff of Atomium 
for their help, and to all the partners of the AI4People project and members of its Forum (http://www.
eismd .eu/ai4pe ople) for their feedback. Luciano Floridi’s work has also been supported by the Privacy-
Enhancing and Identification-Enabling Solutions for IoT (PEIESI) project, part of the PETRAS Internet 
of Things research hub, funded by the Engineering and Physical Sciences Research Council (EPSRC), 
grant agreement no. EP/N023013/1. The authors of this article are the only persons responsible for its 
contents and any remaining mistakes.

Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 Interna-
tional License (http://creati veco mmons. org/licen ses/by/4.0/), which permits unrestricted use, distribution, 
and reproduction in any medium, provided you give appropriate credit to the original author(s) and the 
source, provide a link to the Creative Commons license, and indicate if changes were made.

References

Asilomar AI Principles. (2017). Principles developed in conjunction with the 2017 Asilomar conference 
[Benevolent AI 2017]. Retrieved September 18, 2018 from https: //future ofli fe.org/ai-princ iples .

Cowls, J., & Floridi, L. (2018). Prolegomena to a White Paper on Recommendations for the Ethics of AI 
(June 19, 2018). Available at SSRN: https ://ssrn.com/abstra ct=31987 32.

Cowls, J., & Floridi, L. (Forthcoming). The Utility of a Principled Approach to AI Ethics.
European Group on Ethics in Science and New Technologies. (2018). Statement on Artificial Intelligence, 

Robotics and ‘Autonomous’ Systems. Retrieved September 18, 2018 from https: //ec.europa .eu/info/
news/ethic s-artifi cial- intell igen ce-state ment-ege-relea sed-2018-apr-24_en.

Floridi, L. (2013). The ethics of information. Oxford: Oxford University Press.
Floridi, L. (2018). Soft ethics and the governance of the digital. Philosophy & Technology, 31(1), 1–8.
House of Lords Artificial Intelligence Committee. (2018). AI in the UK: ready, willing and able? 

Retrieved September 18, 2018 from https: //publi catio ns.parli ament. uk/pa/ld201 719/ldsel ect/
ldai/100/10002 .htm.

1 3



AI4People—An Ethical Framework for a Good AI Society:… 707

Imperial College London. (2017). Written Submission to House of Lords Select Committee on Artificial 
Intelligence [AIC0214]. Retrieved September 18, 2018 from http://bit.ly/2yleuE T.

King, T., Aggarwal, N., Taddeo, M., & Floridi, L. (2018). Artificial Intelligence Crime: An Interdisci-
plinary Analysis of Foreseeable Threats and Solutions. Available at SSRN: https: //ssrn.com/abstr 
act=318323 8.

Montreal Declaration for a Responsible Development of Artificial Intelligence. (2017). Announced at the 
conclusion of the Forum on the Socially Responsible Development of AI. Retrieved September 18, 
2018 from https: //www.montre alde clara tion-respo nsibl eai.com/the-declar ation .

Partnership on AI. (2018). Tenets. Retrieved September 18, 2018 from https: //www.partne rship onai. org/
tenet s/.

Taddeo, M. (2018). The limits of deterrence theory in cyberspace. Philosophy & Technology, 31(3), 
339–355.

The IEEE Initiative on Ethics of Autonomous and Intelligent Systems. (2017). Ethically Aligned Design, 
v2. Retrieved September 18, 2018 from https: //ethics inac tion.ieee.org.

Affiliations

Luciano Floridi1,2  · Josh Cowls1,2 · Monica Beltrametti3 · Raja Chatila4,5 · 
Patrice Chazerand6 · Virginia Dignum7,8 · Christoph Luetge9 · Robert Madelin10 · 
Ugo Pagallo11 · Francesca Rossi12,13 · Burkhard Schafer14 · Peggy Valcke15,16 · 
Effy Vayena17

1 Oxford Internet Institute, University of Oxford, Oxford, UK
2 The Alan Turing Institute, London, UK
3 Naver Corporation, Grenoble, France
4 French National Center of Scientific Research, Paris, France
5 Institute of Intelligent Systems and Robotics, Pierre and Marie Curie University, Paris, France
6 Digital Europe, Brussels, Belgium
7 University of Umeå, Umeå, Sweden
8 Delft Design for Values Institute, Delft University of Technology, Delft, The Netherlands
9 TUM School of Governance, Technical University of Munich, Munich, Germany
10 Centre for Technology and Global Affairs, University of Oxford, Oxford, UK
11 Department of Law, University of Turin, Turin, Italy
12 IBM Research, New York, USA
13 University of Padova, Padua, Italy
14 University of Edinburgh Law School, Edinburgh, UK
15 Centre for IT & IP Law, Catholic University of Leuven, Flanders, Belgium
16 Bocconi University, Milan, Italy
17 Bioethics, Health Ethics and Policy Lab, ETH Zurich, Zurich, Switzerland

1 3